# W&B Comprehensive Sweep Configuration
# Combines sweep_longer_epochs.yaml + sweep_longer_epochs_2.yaml + production settings
# Ultra-comprehensive hyperparameter search across model sizes from tiny (32-dim) to large (512-dim)
# Includes advanced training features: LR schedulers, warmup, optimizer tuning

project: gcode-fingerprinting
entity: seacuello-university-of-rhode-island
name: comprehensive_sweep_20251129

program: scripts/train_multihead.py
method: bayes
metric:
  name: val/composite_acc  # CHANGED: Use composite_acc (product of all tasks)
  goal: maximize

# Run budget: Aim for extensive exploration
run_cap: 200

# Environment setup
command:
  - ${env}
  - PYTHONPATH=src
  - python
  - ${program}
  - --use-wandb
  - ${args}

# Early termination: Less aggressive - allow runs to train longer
# With 200 max epochs and patience=15, give runs at least 50-75 epochs
early_terminate:
  type: hyperband
  min_iter: 50  # CHANGED: from 25 to 50 - allow more training time
  s: 2          # CHANGED: from 3 to 2 - less aggressive pruning

parameters:
  # ============================================================================
  # DATA & TRAINING SETUP (Fixed)
  # ============================================================================

  data-dir:
    value: outputs/processed_hybrid

  vocab-path:
    value: data/vocabulary_1digit_hybrid.json

  class-weights-path:
    value: outputs/class_weights_hybrid.json

  output-dir:
    value: outputs/comprehensive_sweep_20251129

  oversample-factor:
    value: 1

  # ============================================================================
  # TRAINING DURATION & EARLY STOPPING
  # ============================================================================

  max-epochs:
    value: 200

  patience:
    value: 15

  # ============================================================================
  # MODEL ARCHITECTURE: Focused on proven ranges
  # Based on experiment comparison: Best = hidden_dim:256, num_heads:8, num_layers:5
  # Focus search around 192-384 dim sweet spot
  # ============================================================================

  hidden_dim:
    values: [192, 256, 320, 384]  # CHANGED: Removed tiny (32-128) and huge (512)

  # num_heads must divide hidden_dim evenly
  # Valid combinations:
  #   192: 2, 3, 4, 6, 8
  #   256: 2, 4, 8
  #   320: 2, 4, 8
  #   384: 2, 3, 4, 6, 8
  num_heads:
    values: [4, 6, 8]  # CHANGED: Focus on 4-8 heads (best performers)

  # Layers: Focus on 4-6 layers
  num_layers:
    values: [4, 5, 6]  # CHANGED: Removed very shallow (2-3) and very deep (7)

  # ============================================================================
  # OPTIMIZER & LEARNING RATE
  # ============================================================================

  # Learning rate: Narrowed to proven range (best: 5.4e-05)
  learning_rate:
    distribution: log_uniform_values
    min: 0.00004   # CHANGED: 4e-05 to 8e-05 (narrower, better range)
    max: 0.00008

  # Batch size: Focus on 24-40 (best: 32 effective)
  batch_size:
    values: [16, 24, 32]  # CHANGED: Removed 8 and 64, added 24

  # Weight decay: Lower range (best: 0.05)
  weight_decay:
    distribution: log_uniform_values
    min: 0.03     # CHANGED: Lower range 0.03-0.08 (best was 0.05)
    max: 0.08

  # Dropout
  dropout:
    distribution: uniform
    min: 0.1
    max: 0.35

  # Gradient clipping
  grad-clip:
    values: [0.5, 1.0, 2.0, 5.0]

  # ============================================================================
  # ADVANCED TRAINING FEATURES (Option B) - From production command
  # ============================================================================

  # LR Scheduler
  lr-scheduler:
    values: ['none', 'cosine', 'plateau', 'step']

  # Warmup epochs (for cosine/step schedulers)
  warmup-epochs:
    values: [0, 5, 10]

  # Adam/AdamW beta parameters
  beta1:
    distribution: uniform
    min: 0.85
    max: 0.95

  beta2:
    distribution: uniform
    min: 0.995
    max: 0.9999

  # Gradient accumulation
  accumulation-steps:
    values: [1, 2, 4]

  # ============================================================================
  # LOSS WEIGHTS - From production command
  # ============================================================================

  command_weight:
    values: [1.0, 2.0, 3.0]

  param_type_weight:
    value: 1.0

  param_value_weight:
    value: 1.0

  operation_weight:
    values: [0.0, 1.0, 2.0]

  grammar_weight:
    values: [0.0, 0.05, 0.1, 0.2]

  # Label smoothing: Disabled (conflicts with class weights)
  label_smoothing:
    value: 0.0

# ============================================================================
# SWEEP STRATEGY NOTES
# ============================================================================
# This sweep combines:
# 1. sweep_longer_epochs.yaml: Focus on larger models (256-512 dim)
# 2. sweep_longer_epochs_2.yaml: Focus on tiny models (32-128 dim) based on
#    27v7pl9i findings that smaller models generalize better
# 3. Production command: Advanced training features (schedulers, warmup, etc.)
#
# Expected outcomes:
# - Tiny models (32-64 dim) may achieve best generalization
# - Cosine scheduler + warmup likely to help convergence
# - Grammar constraints may improve token validity
# - Gradient accumulation enables effective larger batch training
#
# Search space size: ~10M+ configurations
# Bayesian optimization will focus on promising regions
# Expected runtime: 24-48 hours on Lambda GPU
# ============================================================================
