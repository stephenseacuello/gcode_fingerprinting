# Grammar Constraints Optimization Sweep
#
# Purpose: Find optimal grammar_weight for G-code constraint enforcement
# Focus: Improve grammar quality while maintaining prediction accuracy
#
# Current Challenges:
#   - Model can generate invalid G-code (G2/G3 without R parameter)
#   - No modal state tracking (bare coordinates without motion command)
#   - Rapid moves (G0) with feed rate (F) parameter
#
# Strategy: Test different grammar_weight values to balance constraints vs accuracy
# Expected Duration: ~2 hours (6 runs × 20min each)
# Success Criteria:
#   - Grammar quality score > 90%
#   - Maintain high prediction accuracy
#   - Diverse numeric predictions (not collapsed)

project: gcode-fingerprinting
command:
  - scripts/run_sweep_agent.sh
  - ${program}
  - ${args}
program: scripts/train_multihead.py
method: grid
metric:
  name: val/overall_acc
  goal: maximize

parameters:
  # NEW: Grammar constraint weight (main parameter to tune)
  grammar_weight:
    values: [0.0, 0.1, 0.2, 0.5, 1.0]

  # NEW: Dropout regularization (test different values)
  dropout:
    values: [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]

  # FIXED: Use hybrid 1-digit vocabulary (best performing)
  data-dir:
    value: "outputs/processed_hybrid"

  vocab-path:
    value: "data/vocabulary_1digit_hybrid.json"

  # FIXED: Best hyperparameters from previous sweeps
  hidden_dim:
    value: 256

  num_layers:
    value: 5

  num_heads:
    value: 8

  batch_size:
    value: 32

  learning_rate:
    value: 0.00017242889465641294

  label_smoothing:
    value: 0.1

  weight_decay:
    value: 0.1

  # FIXED: Training configuration
  max-epochs:
    value: 15

  patience:
    value: 5

  command_weight:
    value: 3.0

  operation_weight:
    value: 2.0

  # Output directory
  output-dir:
    value: "outputs/sweep_grammar"

# Total runs: 36 configurations (6 grammar_weight × 6 dropout)
# Estimated time per run: ~20 minutes
# Total sweep time: ~12 hours
#
# Expected Outcomes:
# - Find optimal grammar_weight that balances constraint enforcement and accuracy
# - Find optimal dropout that prevents overfitting without underfitting
# - Grammar weight = 0.0: Baseline without constraints
# - Grammar weight = 0.05-0.2: Expected sweet spot
# - Grammar weight = 0.5-1.0: Test stronger constraint enforcement
# - Dropout = 0.0: No regularization (may overfit)
# - Dropout = 0.1-0.2: Expected sweet spot for regularization
# - Dropout = 0.3-0.5: Test stronger regularization
# - Metrics to track:
#   * Grammar quality score (from validate_grammar.py)
#   * Value diversity (not predicting single value)
#   * Prediction accuracy (all heads)
#   * Training stability (convergence)
#   * Overfitting indicators (train vs val loss gap)
