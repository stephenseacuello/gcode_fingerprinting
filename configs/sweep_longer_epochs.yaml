# W&B Sweep Configuration for Long-Running Lambda GPU Training
# Comprehensive hyperparameter search with 250 epoch budget for hybrid 1-digit regression model
# Optimized for 24-hour Lambda GPU runs with focused search space based on prior results

project: gcode-fingerprinting
entity: seacuello-university-of-rhode-island
name: lambda-250epoch-comprehensive-sweep

program: scripts/train_multihead.py
method: bayes
metric:
  name: val/param_type_acc
  goal: maximize

# Run count: Aim for 40-50 runs over 24 hours
# With early termination, expect 30-40 full runs and 10-20 early stops
run_cap: 100

# Environment setup (Lambda uses CUDA, not MPS)
command:
  - ${env}
  - PYTHONPATH=src
  - python
  - ${program}
  - --use-wandb
  - ${args}

# Early termination: More aggressive for 250 epoch runs
# Stop poor performers early to maximize exploration
early_terminate:
  type: hyperband
  min_iter: 25  # Allow 25 epochs minimum before termination
  s: 3          # More aggressive pruning

parameters:
  # Data parameters (fixed)
  data-dir:
    value: outputs/processed_hybrid

  vocab-path:
    value: data/vocabulary_1digit_hybrid.json

  # Class weights for addressing imbalance
  class-weights-path:
    value: outputs/class_weights_hybrid.json

  # Extended training budget for Lambda GPU
  max-epochs:
    value: 250

  patience:
    value: 35  # Higher patience for long runs - allow ~14% of epochs for convergence

  # Learning rate: Focused around best-performing range (0.00016 was optimal)
  # Still exploring broadly but weighted toward successful values
  learning_rate:
    distribution: log_uniform_values
    min: 0.00005   # Slightly higher minimum
    max: 0.0005    # Lower maximum, focus on sweet spot

  # Batch size: Previous best results used 16-32
  # Keeping 64 for exploration but expect 16-32 to dominate
  batch_size:
    values: [16, 32, 64]

  # Model architecture
  # NOTE: hidden_dim must be divisible by num_heads for PyTorch MultiheadAttention
  # Safe combinations: (256,4), (256,8), (384,4), (384,6), (384,8), (512,4), (512,8)
  # Previous best: hidden_dim=384 with num_heads=4
  hidden_dim:
    values: [256, 384, 512]

  num_heads:
    values: [4, 6, 8]  # Added 6 for better 384-dim exploration

  num_layers:
    values: [4, 5, 6, 7]  # Added 7 for deeper models

  # Regularization: Previous best used weight_decay=0.05
  weight_decay:
    values: [0.01, 0.05, 0.1]

  # DISABLED: Label smoothing conflicts with class weights
  # Using class weights instead to address imbalance
  label_smoothing:
    value: 0.0

  # Dropout: Tighter range around typical values
  dropout:
    distribution: uniform
    min: 0.15
    max: 0.35

  # Gradient clipping for training stability with long runs
  grad-clip:
    values: [0.5, 1.0, 2.0, 5.0]

# ============================================================================
# ADVANCED PARAMETERS (Commented out - require training script updates)
# ============================================================================
# To enable these powerful optimizations, add support to train_multihead.py:
#
#  lr_scheduler:
#    values: ['cosine', 'plateau', 'step', 'none']
#
#  warmup_epochs:
#    values: [0, 5, 10]
#
#  beta1:
#    distribution: uniform
#    min: 0.85
#    max: 0.95
#
#  beta2:
#    distribution: uniform
#    min: 0.995
#    max: 0.9999
#
#  accumulation_steps:
#    values: [1, 2, 4]
#
#  layer-lr-decay:
#    values: [1.0, 0.9, 0.95]
#
#  stochastic-depth:
#    values: [0.0, 0.1, 0.2]
# ============================================================================
