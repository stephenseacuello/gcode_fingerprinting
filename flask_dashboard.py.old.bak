"""
Enhanced Real-time monitoring dashboard with Phase 1 & 2 features.

Features:
- Top-K predictions
- Live confusion matrix
- Dark mode
- CSV export
- Sensor heatmap
- 3D position plot
- Performance metrics

Run with: python flask_dashboard_enhanced.py
Then open: http://localhost:5000
"""
import sys
sys.path.insert(0, 'src')

from flask import Flask, render_template, jsonify, request, send_file
from flask_socketio import SocketIO, emit
import torch
import numpy as np
import pandas as pd
import json
import logging
from pathlib import Path
from collections import deque, defaultdict
from datetime import datetime
import io
import base64
from sklearn.manifold import TSNE
import redis

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

from miracle.model.model import MM_DTAE_LSTM, ModelConfig
from miracle.model.multihead_lm import MultiHeadGCodeLM
from miracle.dataset.target_utils import TokenDecomposer
from miracle.utilities.gcode_tokenizer import GCodeTokenizer


# Initialize Flask app
app = Flask(__name__)
app.config['SECRET_KEY'] = 'gcode-fingerprinting-2025'

# Initialize SocketIO with CORS support
socketio = SocketIO(
    app,
    cors_allowed_origins="*",
    async_mode='threading',
    logger=False,
    engineio_logger=False
)

# Initialize Redis for caching (optional - graceful fallback if not available)
try:
    redis_client = redis.Redis(host='localhost', port=6379, db=0, decode_responses=True)
    redis_client.ping()
    logger.info("Redis connection established")
    REDIS_AVAILABLE = True
except (redis.ConnectionError, redis.RedisError) as e:
    logger.warning(f"Redis not available: {e}. Caching disabled.")
    redis_client = None
    REDIS_AVAILABLE = False

# Global state
state = {
    'model': None,
    'model_type': None,  # 'baseline' or 'multihead'
    'decomposer': None,  # For multi-head token decomposition
    'tokenizer': None,
    'metadata': None,  # Preprocessing metadata with master_columns
    'buffer': deque(maxlen=64),
    'predictions_history': [],
    'csv_data': None,
    'current_idx': 0,
    'running': False,
    'config': {},
    'ground_truth': [],  # For confusion matrix
    'confusion_matrix': defaultdict(lambda: defaultdict(int)),  # Live confusion matrix
    'statistics': defaultdict(list),  # Running statistics
    'fingerprints': [],  # For t-SNE
    'generation_history': deque(maxlen=50),  # Last 50 generated commands
    'command_types': defaultdict(int),  # Track command type distribution
    'accuracy_over_time': [],  # Track accuracy metrics
    # Generation settings
    'gen_settings': {
        'enable_autoregressive': True,
        'max_tokens': 15,
        'temperature': 1.0,
        'top_p': 1.0,
        'beam_size': 1,
        'use_beam_search': False,
    }
}


# ============================================================================
# Error Handling Utilities
# ============================================================================

def emit_error(error_message, error_type="error", hint=None, exception=None):
    """
    Emit standardized error messages via WebSocket.

    Args:
        error_message: Human-readable error message
        error_type: Type of error (error, warning, info)
        hint: Optional hint for resolving the error
        exception: Optional exception object for logging
    """
    from flask_socketio import emit

    error_data = {
        'error': error_message,
        'type': error_type,
        'timestamp': datetime.now().isoformat()
    }

    if hint:
        error_data['hint'] = hint

    if exception:
        logger.error(f"{error_message}: {str(exception)}", exc_info=True)
        error_data['details'] = str(exception)
    else:
        logger.error(error_message)

    emit('error', error_data)


def validate_model_loaded():
    """Validate that model is loaded and ready."""
    if state['model'] is None:
        emit_error(
            'No model loaded',
            hint='Please select and load a model from the dropdown before starting inference.'
        )
        return False
    return True


def validate_csv_loaded():
    """Validate that CSV data is loaded and ready."""
    if state['csv_data'] is None:
        emit_error(
            'No CSV data loaded',
            hint='Please select and load a CSV file from the dropdown before starting inference.'
        )
        return False
    if len(state['csv_data']) == 0:
        emit_error(
            'CSV file is empty',
            hint='The loaded CSV file contains no data rows.'
        )
        return False
    return True


# ============================================================================
# Model and Data Loading
# ============================================================================

def load_model(checkpoint_path):
    """Load model from checkpoint - supports both baseline and multi-head models."""
    device = 'cpu'
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
    config_dict = checkpoint['config']

    # Detect model type
    is_multihead = 'n_commands' in config_dict or 'multihead' in str(checkpoint_path).lower()

    if is_multihead:
        logger.info("Loading Multi-Head model (Phase 2)")
        model_type = 'multihead'

        # Infer architecture from checkpoint state dict
        state_dict_key = 'multihead_state_dict' if 'multihead_state_dict' in checkpoint else 'model_state_dict'
        state_dict = checkpoint[state_dict_key]

        # Extract head sizes from state dict
        n_commands = state_dict['command_head.4.weight'].shape[0]
        n_param_types = state_dict['param_type_head.4.weight'].shape[0]
        n_param_values = state_dict['param_value_head.4.weight'].shape[0]

        logger.info(f"Detected architecture: n_commands={n_commands}, n_param_types={n_param_types}, n_param_values={n_param_values}")

        # Multi-head checkpoints - architecture is inferred from state dict
        model = MultiHeadGCodeLM(
            d_model=config_dict.get('hidden_dim', 128),
            n_commands=n_commands,
            n_param_types=n_param_types,
            n_param_values=n_param_values,
            nhead=config_dict.get('num_heads', config_dict.get('nhead', 4)),
            num_layers=config_dict.get('num_layers', 2),
            dropout=0.1,
            vocab_size=170,  # Vocabulary v2
        ).to(device)

        # Load state dict - multi-head checkpoints have multihead_state_dict
        if 'multihead_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['multihead_state_dict'])
        elif 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            raise KeyError("Checkpoint missing 'multihead_state_dict' or 'model_state_dict'")

        model.eval()

        # Load tokenizer (vocab v2 for multi-head)
        vocab_path = Path('data/gcode_vocab_v2.json')
        if vocab_path.exists():
            tokenizer = GCodeTokenizer.load(vocab_path)
        else:
            vocab_path = Path('data/vocabulary.json')
            tokenizer = GCodeTokenizer.load(vocab_path) if vocab_path.exists() else None

        # Load decomposer for token reconstruction
        decomposer = TokenDecomposer(vocab_path) if vocab_path.exists() else None

        # Load preprocessing metadata
        data_dir = Path('outputs/processed_v2')
        metadata_path = data_dir / 'train_sequences_metadata.json'
        metadata = None
        if metadata_path.exists():
            with open(metadata_path) as f:
                metadata = json.load(f)

        # Create config object for compatibility
        config = type('Config', (), config_dict)()

        logger.info(f"âœ… Multi-Head model loaded: {config_dict.get('hidden_dim', 128)}d, "
                   f"{config_dict.get('n_commands', 15)} commands, vocab={config_dict.get('vocab_size', 170)}")

        return model, tokenizer, config, metadata, model_type, decomposer

    else:
        logger.info("Loading Baseline model (Phase 1)")
        model_type = 'baseline'

        # Create baseline model
        config = ModelConfig(**config_dict)
        model = MM_DTAE_LSTM(config).to(device)
        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()

        # Load tokenizer (vocab v1 for baseline)
        vocab_path = Path('data/vocabulary.json')
        if vocab_path.exists():
            tokenizer = GCodeTokenizer.load(vocab_path)
        else:
            tokenizer = None

        # Load preprocessing metadata
        data_dir = Path('outputs/processed')
        metadata_path = data_dir / 'train_sequences_metadata.json'
        metadata = None
        if metadata_path.exists():
            with open(metadata_path) as f:
                metadata = json.load(f)

        logger.info(f"âœ… Baseline model loaded: {config.hidden_dim}d, vocab={config.vocab_size}")

        return model, tokenizer, config, metadata, model_type, None


def extract_features_with_validation(row, metadata, model_config):
    """
    Extract features from CSV row with proper validation and error handling.

    Args:
        row: pandas Series representing one CSV row
        metadata: Preprocessing metadata dict (may be None)
        model_config: ModelConfig object from loaded model

    Returns:
        Tuple of (continuous_features, categorical_features)

    Raises:
        ValueError: If feature dimensions don't match model expectations
    """
    # Expected dimensions from model
    expected_cont_dim = model_config.sensor_dims[0] if hasattr(model_config, 'sensor_dims') else None
    expected_cat_dim = model_config.sensor_dims[1] if hasattr(model_config, 'sensor_dims') and len(model_config.sensor_dims) > 1 else None

    # Method 1: Use metadata if available (preferred)
    if metadata and 'master_columns' in metadata:
        master_cols = metadata['master_columns']
        cat_cols = metadata.get('categorical_columns', ['stat', 'unit', 'dist', 'coor'])

        logger.debug(f"Using metadata: {len(master_cols)} continuous features, {len(cat_cols)} categorical features")

        # Validate dimensions match model expectations
        if expected_cont_dim is not None and len(master_cols) != expected_cont_dim:
            error_msg = (
                f"Dimension mismatch: Metadata specifies {len(master_cols)} continuous features "
                f"but model expects {expected_cont_dim}. "
                f"This usually means the CSV was preprocessed with different settings than the model was trained with."
            )
            logger.error(error_msg)
            raise ValueError(error_msg)

        if expected_cat_dim is not None and len(cat_cols) != expected_cat_dim:
            logger.warning(
                f"Categorical dimension mismatch: Metadata has {len(cat_cols)} features "
                f"but model expects {expected_cat_dim}. Adjusting..."
            )
            # Pad or truncate categorical features
            cat_cols = (cat_cols + ['stat', 'unit', 'dist', 'coor'])[:expected_cat_dim]

        # Extract continuous features with zero-padding for missing columns
        continuous = np.zeros(len(master_cols), dtype=np.float32)
        missing_cols = []
        for i, col in enumerate(master_cols):
            if col in row.index:
                continuous[i] = float(row[col])
            else:
                missing_cols.append(col)

        if missing_cols:
            logger.debug(f"Zero-padding {len(missing_cols)} missing sensors: {missing_cols[:5]}...")

        # Extract categorical features
        categorical = np.zeros(len(cat_cols), dtype=np.float32)
        for i, col in enumerate(cat_cols):
            if col in row.index:
                categorical[i] = float(row[col])

        return continuous, categorical

    # Method 2: Fallback to column-based detection (legacy)
    logger.warning("No metadata available, using fallback column detection. This may cause dimension mismatches!")

    exclude_cols = ['time', 'gcode_line_num', 'gcode_text', 'gcode_tokens',
                    't_console', 'gcode_line', 'gcode_string', 'raw_json',
                    'vel', 'plane', 'line', 'posx', 'posy', 'posz', 'feed', 'momo']
    cat_cols = ['stat', 'unit', 'dist', 'coor']

    cont_cols = [col for col in row.index if col not in exclude_cols and col not in cat_cols]
    continuous = row[cont_cols].values.astype(np.float32)
    categorical = row[[col for col in cat_cols if col in row.index]].values.astype(np.float32)

    # Validate dimensions
    if expected_cont_dim is not None and len(continuous) != expected_cont_dim:
        logger.warning(
            f"Fallback extraction: Got {len(continuous)} continuous features, "
            f"model expects {expected_cont_dim}. Padding/truncating..."
        )
        # Pad or truncate
        if len(continuous) < expected_cont_dim:
            continuous = np.pad(continuous, (0, expected_cont_dim - len(continuous)), 'constant')
        else:
            continuous = continuous[:expected_cont_dim]

    if expected_cat_dim is not None and len(categorical) != expected_cat_dim:
        logger.warning(
            f"Fallback extraction: Got {len(categorical)} categorical features, "
            f"model expects {expected_cat_dim}. Padding/truncating..."
        )
        # Pad or truncate
        if len(categorical) < expected_cat_dim:
            categorical = np.pad(categorical, (0, expected_cat_dim - len(categorical)), 'constant')
        else:
            categorical = categorical[:expected_cat_dim]

    return continuous, categorical


def edit_distance(s1, s2):
    """Calculate Levenshtein edit distance between two strings."""
    if len(s1) < len(s2):
        return edit_distance(s2, s1)

    if len(s2) == 0:
        return len(s1)

    previous_row = range(len(s2) + 1)
    for i, c1 in enumerate(s1):
        current_row = [i + 1]
        for j, c2 in enumerate(s2):
            insertions = previous_row[j + 1] + 1
            deletions = current_row[j] + 1
            substitutions = previous_row[j] + (c1 != c2)
            current_row.append(min(insertions, deletions, substitutions))
        previous_row = current_row

    return previous_row[-1]


def nucleus_sampling(logits, top_p=0.9, temperature=1.0):
    """Apply nucleus (top-p) sampling to logits."""
    if temperature != 1.0:
        logits = logits / temperature

    # Sort logits in descending order
    sorted_indices = np.argsort(logits)[::-1]
    sorted_logits = logits[sorted_indices]

    # Convert to probabilities
    probs = np.exp(sorted_logits - sorted_logits.max())
    probs = probs / probs.sum()

    # Compute cumulative probabilities
    cumulative_probs = np.cumsum(probs)

    # Remove tokens with cumulative probability above the threshold
    sorted_indices_to_remove = cumulative_probs > top_p
    # Keep at least one token
    if sorted_indices_to_remove[0]:
        sorted_indices_to_remove[0] = False

    # Set logits of removed tokens to -inf
    filtered_logits = logits.copy()
    filtered_logits[sorted_indices[sorted_indices_to_remove]] = -np.inf

    # Sample from filtered distribution
    probs = np.exp(filtered_logits - filtered_logits.max())
    probs = probs / probs.sum()

    # Sample token
    token_id = np.random.choice(len(probs), p=probs)
    return token_id, probs[token_id]


def beam_search_generate(model, memory, tokenizer, beam_size=3, max_tokens=15, special_tokens=None, model_type='baseline'):
    """Generate G-code using beam search."""
    device = 'cpu'
    eos_id = tokenizer.vocab.get('EOS', 2)

    if special_tokens is None:
        special_tokens = {'PAD', 'BOS', 'EOS', 'UNK', 'MASK', '<PAD>', '<BOS>', '<EOS>', '<UNK>', '<MASK>'}

    # Initialize beams: (score, tokens, confidence)
    beams = [(0.0, [1], 1.0)]  # Start with BOS token

    for step in range(max_tokens):
        candidates = []

        for score, tokens, conf in beams:
            # Check if beam already ended
            if tokens[-1] == eos_id:
                candidates.append((score, tokens, conf))
                continue

            # Get logits
            current_tokens = torch.tensor([tokens], dtype=torch.long, device=device)

            if model_type == 'multihead':
                # Multi-head model
                multihead_outputs = model(memory, current_tokens)
                next_logits = multihead_outputs['command_logits'][0, -1].detach().cpu().numpy()
            else:
                # Baseline model
                step_logits = model.gcode_head(memory, current_tokens)
                next_logits = step_logits[0, -1].detach().cpu().numpy()

            # Get top-k tokens
            top_indices = np.argsort(next_logits)[-beam_size * 2:][::-1]

            probs = np.exp(next_logits - next_logits.max())
            probs = probs / probs.sum()

            for idx in top_indices:
                token_id = int(idx)
                token_prob = float(probs[token_id])
                token_score = score + np.log(token_prob + 1e-10)

                # Decode to check if valid
                decoded = tokenizer.decode([token_id])
                if isinstance(decoded, list):
                    token_text = decoded[0] if decoded else ''
                else:
                    token_text = decoded

                # Skip special tokens except EOS
                if token_text in special_tokens and token_id != eos_id:
                    continue

                new_tokens = tokens + [token_id]
                new_conf = conf * token_prob
                candidates.append((token_score, new_tokens, new_conf))

        # Keep top beam_size beams
        beams = sorted(candidates, key=lambda x: x[0], reverse=True)[:beam_size]

        # Stop if all beams ended
        if all(tokens[-1] == eos_id for _, tokens, _ in beams):
            break

    # Return best beam
    best_score, best_tokens, best_conf = beams[0]

    # Decode tokens
    result_tokens = []
    for token_id in best_tokens[1:]:  # Skip BOS
        if token_id == eos_id:
            break
        decoded = tokenizer.decode([token_id])
        if isinstance(decoded, list):
            token_text = decoded[0] if decoded else ''
        else:
            token_text = decoded
        if token_text and token_text not in special_tokens:
            result_tokens.append(token_text)

    return result_tokens, best_conf


def process_sample(continuous, categorical, ground_truth_gcode=None):
    """Process a single sensor sample and return predictions."""
    if state['model'] is None:
        logger.warning("process_sample called but no model loaded")
        return None

    # Add to buffer
    state['buffer'].append({
        'continuous': continuous,
        'categorical': categorical,
        'timestamp': datetime.now()
    })

    # Need full window
    if len(state['buffer']) < 64:
        return None

    # Stack into windows with error handling
    try:
        cont_window = np.stack([s['continuous'] for s in state['buffer']])
        cat_window = np.stack([s['categorical'] for s in state['buffer']])
    except ValueError as e:
        logger.error(f"Failed to stack features into window: {e}")
        logger.error(f"Feature shapes in buffer: continuous={[s['continuous'].shape for s in list(state['buffer'])[:3]]}")
        logger.error(f"This usually indicates inconsistent feature dimensions across samples")
        # Clear buffer to prevent continued errors
        state['buffer'].clear()
        return None

    # Convert to torch
    device = 'cpu'
    try:
        cont_tensor = torch.from_numpy(cont_window).unsqueeze(0).float().to(device)
        cat_tensor = torch.from_numpy(cat_window).unsqueeze(0).float().to(device)
    except Exception as e:
        logger.error(f"Failed to convert features to tensors: {e}")
        logger.error(f"cont_window.shape: {cont_window.shape}, cat_window.shape: {cat_window.shape}")
        return None

    mods = [cont_tensor, cat_tensor]

    # Inference with error handling - handle both baseline and multi-head models
    try:
        with torch.no_grad():
            if state['model_type'] == 'multihead':
                # Multi-head model: needs LSTM encoder for memory
                # Create a simple LSTM encoder if not already in model
                if not hasattr(state['model'], 'lstm_encoder'):
                    # Use the continuous features as input to an LSTM to get memory
                    lstm = torch.nn.LSTM(cont_tensor.shape[-1], 128, 2, batch_first=True, bidirectional=False)
                    lstm = lstm.to(device).eval()
                    state['model'].lstm_encoder = lstm

                # Get memory from LSTM
                memory, _ = state['model'].lstm_encoder(cont_tensor)  # [B, T, D]

                # Store for later use in G-code generation
                outputs = {
                    'memory': memory,
                    'fingerprint': memory[:, -1, :],  # Use last hidden state as fingerprint
                    'anom': torch.zeros(1, 1).to(device),  # Placeholder
                }
            else:
                # Baseline model
                outputs = state['model'](
                    mods=mods,
                    lengths=torch.tensor([64]).to(device),
                    gcode_in=None,
                    modality_dropout_p=0.0
                )
    except RuntimeError as e:
        logger.error(f"Model inference failed: {e}")
        logger.error(f"Input tensor shapes: continuous={cont_tensor.shape}, categorical={cat_tensor.shape}")
        logger.error(f"Expected model input dims: {state['config'].sensor_dims if state.get('config') else 'unknown'}")
        return None
    except Exception as e:
        logger.error(f"Unexpected error during inference: {e}", exc_info=True)
        return None

    # Extract predictions
    predictions = {
        'fingerprint': outputs['fingerprint'][0].cpu().numpy().tolist() if 'fingerprint' in outputs else [],
        'anomaly_score': float(outputs['anom'][0].cpu().numpy()[0]) if 'anom' in outputs else 0.0,
        'timestamp': state['buffer'][-1]['timestamp'].isoformat()
    }

    # Decode G-code with Top-K using logits from BOS token
    if state['tokenizer'] and 'memory' in outputs:
        try:
            memory = outputs['memory']  # [B, T, D]

            # ===== TOKEN-LEVEL PREDICTION (Current approach) =====
            # Use a single BOS token to get logits for next token prediction
            bos_tokens = torch.full((1, 1), 1, dtype=torch.long, device=device)  # [B, 1]

            if state['model_type'] == 'multihead':
                # Multi-head model returns a dictionary with different head outputs
                multihead_outputs = state['model'](memory, bos_tokens)  # Dict with type_logits, command_logits, etc.

                # For token-level prediction, use command head (most important for G-commands)
                command_logits = multihead_outputs['command_logits'][0, -1].detach().cpu().numpy()  # [n_commands]

                # Also get other heads for reference
                type_logits = multihead_outputs['type_logits'][0, -1].detach().cpu().numpy()
                param_type_logits = multihead_outputs['param_type_logits'][0, -1].detach().cpu().numpy()
                param_value_logits = multihead_outputs['param_value_logits'][0, -1].detach().cpu().numpy()

                # Use command head as primary prediction (this achieved 100% accuracy!)
                last_logits = command_logits
            else:
                # Baseline model
                gcode_logits = state['model'].gcode_head(memory, bos_tokens)  # [B, 1, vocab_size]
                last_logits = gcode_logits[0, -1].detach().cpu().numpy()  # [vocab_size]

            # Get top-k predictions
            top_k = 10  # Get more to filter out special tokens
            top_indices = np.argsort(last_logits)[-top_k:][::-1]
            top_scores = last_logits[top_indices]

            # Convert to probabilities
            top_probs = np.exp(top_scores - top_scores.max())  # Numerical stability
            top_probs = top_probs / top_probs.sum()

            # Decode top-k and filter special tokens
            top_predictions = []
            special_tokens = {'PAD', 'BOS', 'EOS', 'UNK', 'MASK', '<PAD>', '<BOS>', '<EOS>', '<UNK>', '<MASK>'}

            for idx, prob in zip(top_indices, top_probs):
                try:
                    token_id = int(idx)
                    gcode_decoded = state['tokenizer'].decode([token_id])

                    # Handle case where decode returns a list
                    if isinstance(gcode_decoded, list):
                        gcode_text = gcode_decoded[0] if gcode_decoded else ''
                    else:
                        gcode_text = gcode_decoded

                    # Skip special tokens and empty strings
                    if gcode_text and gcode_text not in special_tokens:
                        top_predictions.append({
                            'gcode': gcode_text,
                            'confidence': float(prob),
                            'token_id': token_id
                        })

                        # Stop once we have 5 valid predictions
                        if len(top_predictions) >= 5:
                            break
                except Exception as decode_err:
                    print(f"Token decode error for idx {idx}: {decode_err}")
                    continue

            # Ensure we have at least one prediction
            if top_predictions:
                predictions['top_k'] = top_predictions[:5]  # Top 5
                predictions['gcode_text'] = top_predictions[0]['gcode']
                predictions['gcode_confidence'] = top_predictions[0]['confidence']
            else:
                # Fallback to highest scoring token even if it's special
                highest_idx = np.argmax(last_logits)
                fallback_decoded = state['tokenizer'].decode([int(highest_idx)])

                # Handle case where decode returns a list
                if isinstance(fallback_decoded, list):
                    fallback_text = fallback_decoded[0] if fallback_decoded else '<UNK>'
                else:
                    fallback_text = fallback_decoded

                predictions['top_k'] = [{'gcode': fallback_text, 'confidence': 0.1}]
                predictions['gcode_text'] = fallback_text
                predictions['gcode_confidence'] = 0.1

            # ===== FULL COMMAND GENERATION (Autoregressive) =====
            if state['gen_settings']['enable_autoregressive']:
                # Use beam search or greedy/sampling
                if state['gen_settings']['use_beam_search']:
                    full_command_tokens, full_command_confidence = beam_search_generate(
                        state['model'],
                        memory,
                        state['tokenizer'],
                        beam_size=state['gen_settings']['beam_size'],
                        max_tokens=state['gen_settings']['max_tokens'],
                        special_tokens=special_tokens,
                        model_type=state['model_type']
                    )
                else:
                    # Greedy or nucleus sampling
                    full_command_tokens = []
                    full_command_confidence = 1.0
                    token_confidences = []  # Per-token confidence for breakdown
                    current_tokens = torch.full((1, 1), 1, dtype=torch.long, device=device)

                    max_tokens = state['gen_settings']['max_tokens']
                    temperature = state['gen_settings']['temperature']
                    top_p = state['gen_settings']['top_p']
                    eos_id = state['tokenizer'].vocab.get('EOS', 2)

                    for _ in range(max_tokens):
                        if state['model_type'] == 'multihead':
                            # Multi-head model
                            multihead_outputs = state['model'](memory, current_tokens)
                            # Use command head for generation (achieved 100% accuracy)
                            next_token_logits = multihead_outputs['command_logits'][0, -1].detach().cpu().numpy()
                        else:
                            # Baseline model
                            step_logits = state['model'].gcode_head(memory, current_tokens)
                            next_token_logits = step_logits[0, -1].detach().cpu().numpy()

                        # Apply temperature and sampling
                        if temperature != 1.0 or top_p < 1.0:
                            next_token_id, token_confidence = nucleus_sampling(
                                next_token_logits,
                                top_p=top_p,
                                temperature=temperature
                            )
                        else:
                            # Greedy
                            next_token_id = int(np.argmax(next_token_logits))
                            probs = np.exp(next_token_logits - next_token_logits.max())
                            probs = probs / probs.sum()
                            token_confidence = float(probs[next_token_id])

                        # Decode token
                        decoded = state['tokenizer'].decode([next_token_id])
                        if isinstance(decoded, list):
                            token_text = decoded[0] if decoded else ''
                        else:
                            token_text = decoded

                        # Stop if EOS or special token
                        if next_token_id == eos_id or token_text in special_tokens:
                            break

                        # Add to sequence
                        full_command_tokens.append(token_text)
                        token_confidences.append(token_confidence)
                        full_command_confidence *= token_confidence

                        # Update current_tokens
                        new_token = torch.tensor([[next_token_id]], dtype=torch.long, device=device)
                        current_tokens = torch.cat([current_tokens, new_token], dim=1)

                        # Stop early conditions
                        if token_text in ['M30', 'M2'] or token_confidence < 0.01:
                            break

                    # Store per-token breakdown
                    predictions['token_breakdown'] = [
                        {'token': tok, 'confidence': float(conf)}
                        for tok, conf in zip(full_command_tokens, token_confidences)
                    ]

                # Join tokens
                full_command_text = ' '.join(full_command_tokens) if full_command_tokens else '<EMPTY>'
                predictions['full_command'] = full_command_text
                predictions['full_command_confidence'] = float(
                    full_command_confidence ** (1.0 / max(len(full_command_tokens), 1))
                )

                # Track command type
                if full_command_tokens:
                    command_type = full_command_tokens[0]  # First token (G1, M3, etc.)
                    state['command_types'][command_type] += 1

                # Calculate metrics if ground truth available
                if ground_truth_gcode:
                    edit_dist = edit_distance(full_command_text, ground_truth_gcode)
                    predictions['edit_distance'] = edit_dist
                    predictions['ground_truth'] = ground_truth_gcode
                    predictions['match'] = (full_command_text == ground_truth_gcode)

                # Add to generation history
                state['generation_history'].append({
                    'predicted': full_command_text,
                    'ground_truth': ground_truth_gcode if ground_truth_gcode else None,
                    'confidence': predictions['full_command_confidence'],
                    'timestamp': predictions['timestamp'],
                    'token_breakdown': predictions.get('token_breakdown', [])
                })
            else:
                # Autoregressive disabled
                predictions['full_command'] = '<DISABLED>'
                predictions['full_command_confidence'] = 0.0
                predictions['token_breakdown'] = []

            # Update confusion matrix if ground truth available
            if ground_truth_gcode:
                predicted = predictions['gcode_text']
                state['confusion_matrix'][ground_truth_gcode][predicted] += 1

        except Exception as e:
            import traceback
            print(f"\n=== G-code generation error ===")
            print(f"Error: {e}")
            traceback.print_exc()
            print(f"================================\n")
            predictions['gcode_text'] = '<ERROR>'
            predictions['gcode_confidence'] = 0.0
            predictions['top_k'] = []
            predictions['full_command'] = '<ERROR>'
            predictions['full_command_confidence'] = 0.0
    else:
        predictions['gcode_text'] = '<NO_MODEL>'
        predictions['gcode_confidence'] = 0.0
        predictions['top_k'] = []
        predictions['full_command'] = '<NO_MODEL>'
        predictions['full_command_confidence'] = 0.0

    # Store fingerprint for t-SNE
    if 'fingerprint' in predictions:
        state['fingerprints'].append(predictions['fingerprint'])
        # Keep only last 500 for performance
        if len(state['fingerprints']) > 500:
            state['fingerprints'] = state['fingerprints'][-500:]

    # Update running statistics
    state['statistics']['anomaly'].append(predictions['anomaly_score'])
    state['statistics']['confidence'].append(predictions['gcode_confidence'])

    # Compute running stats
    predictions['running_stats'] = {
        'anomaly_mean': float(np.mean(state['statistics']['anomaly'][-100:])),
        'anomaly_std': float(np.std(state['statistics']['anomaly'][-100:])),
        'confidence_mean': float(np.mean(state['statistics']['confidence'][-100:])),
        'confidence_std': float(np.std(state['statistics']['confidence'][-100:])),
    }

    return predictions


# Routes
@app.route('/')
def index():
    """Serve the enhanced dashboard page."""
    return render_template('dashboard_enhanced.html')


@app.route('/api/models')
def get_models():
    """Get list of available models."""
    models = []

    # Check multi-head models (Phase 2) - PRIORITIZE THESE!
    multihead_dir = Path('outputs/multihead_aug_v2')
    if multihead_dir.exists():
        checkpoint_best = multihead_dir / 'checkpoint_best.pt'
        if checkpoint_best.exists():
            models.append({
                'path': str(checkpoint_best),
                'name': 'ðŸ† Multi-Head (100% Command Acc)',
                'type': 'multihead_phase2'
            })
        checkpoint_latest = multihead_dir / 'checkpoint_latest.pt'
        if checkpoint_latest.exists():
            models.append({
                'path': str(checkpoint_latest),
                'name': 'Multi-Head (latest)',
                'type': 'multihead_phase2'
            })

    # Check sweep output directory (default location for W&B sweeps)
    sweep_dir = Path('outputs/multihead_v2')
    if sweep_dir.exists():
        checkpoint_best = sweep_dir / 'checkpoint_best.pt'
        if checkpoint_best.exists():
            models.append({
                'path': str(checkpoint_best),
                'name': 'ðŸ”¥ Sweep Best Model',
                'type': 'multihead_sweep'
            })
        checkpoint_latest = sweep_dir / 'checkpoint_latest.pt'
        if checkpoint_latest.exists() and not checkpoint_best.exists():
            models.append({
                'path': str(checkpoint_latest),
                'name': 'Sweep Latest Model',
                'type': 'multihead_sweep'
            })

    # Check wandb_sweeps (most common from sweeps)
    wandb_dir = Path('outputs/wandb_sweeps')
    if wandb_dir.exists():
        for model_dir in wandb_dir.glob('gcode_model_*/checkpoint_best.pt'):
            models.append({
                'path': str(model_dir),
                'name': model_dir.parent.name,
                'type': 'wandb_sweep'
            })
        for model_dir in wandb_dir.glob('gcode_model_*/checkpoint_latest.pt'):
            if not (model_dir.parent / 'checkpoint_best.pt').exists():
                models.append({
                    'path': str(model_dir),
                    'name': model_dir.parent.name + ' (latest)',
                    'type': 'wandb_sweep'
                })

    # Check training_clean
    clean_dir = Path('outputs/training_clean')
    if clean_dir.exists():
        for model_dir in clean_dir.glob('gcode_model_*/checkpoint_best.pt'):
            models.append({
                'path': str(model_dir),
                'name': model_dir.parent.name,
                'type': 'clean'
            })
        for model_dir in clean_dir.glob('gcode_model_*/checkpoint_latest.pt'):
            if not (model_dir.parent / 'checkpoint_best.pt').exists():
                models.append({
                    'path': str(model_dir),
                    'name': model_dir.parent.name + ' (latest)',
                    'type': 'clean'
                })

    # Fallback to regular training
    train_dir = Path('outputs/training')
    if train_dir.exists():
        for model_dir in train_dir.glob('gcode_model_*/checkpoint_best.pt'):
            models.append({
                'path': str(model_dir),
                'name': model_dir.parent.name,
                'type': 'training'
            })

    return jsonify(models)


@app.route('/api/csv_files')
def get_csv_files():
    """Get list of available CSV files."""
    csv_files = []
    data_dir = Path('data')

    if data_dir.exists():
        for csv_file in sorted(data_dir.glob('*_aligned.csv')):
            csv_files.append({
                'path': str(csv_file),
                'name': csv_file.name
            })

    return jsonify(csv_files)


@app.route('/api/load_model', methods=['POST'])
def load_model_endpoint():
    """Load a model with comprehensive error handling."""
    try:
        data = request.json
        if not data:
            logger.error("No data provided in load_model request")
            return jsonify({'success': False, 'error': 'No data provided'})

        model_path = data.get('path')
        if not model_path:
            logger.error("No model path provided")
            return jsonify({'success': False, 'error': 'No model path provided'})

        model_path_obj = Path(model_path)
        if not model_path_obj.exists():
            logger.error(f"Model path does not exist: {model_path}")
            return jsonify({
                'success': False,
                'error': f'Model file not found: {model_path}',
                'hint': 'Make sure the model checkpoint file exists at the specified path.'
            })

        logger.info(f"Loading model from: {model_path}")
        model, tokenizer, config, metadata, model_type, decomposer = load_model(model_path)

        state['model'] = model
        state['model_type'] = model_type
        state['decomposer'] = decomposer
        state['tokenizer'] = tokenizer
        state['config'] = config  # Store config object (not dict)
        state['config_dict'] = config.__dict__  # Also store dict for JSON responses
        state['metadata'] = metadata  # Store preprocessing metadata

        logger.info(f"âœ“ Model loaded successfully: {model_path}")
        logger.info(f"  Model type: {model_type}")
        if hasattr(config, 'sensor_dims'):
            logger.info(f"  Model expects: {config.sensor_dims} features (continuous, categorical)")
        if metadata:
            logger.info(f"  Metadata: {len(metadata.get('master_columns', []))} continuous, "
                       f"{len(metadata.get('categorical_columns', []))} categorical features")

        # Reset state
        state['buffer'].clear()
        state['predictions_history'].clear()
        state['confusion_matrix'].clear()
        state['statistics'].clear()
        state['fingerprints'].clear()

        return jsonify({
            'success': True,
            'config': state['config_dict'],
            'n_features': metadata['n_continuous_features'] if metadata else None
        })

    except FileNotFoundError as e:
        logger.error(f"File not found while loading model: {e}")
        return jsonify({
            'success': False,
            'error': 'Model file or dependency not found',
            'hint': 'Check that the model checkpoint and vocabulary.json exist.',
            'details': str(e)
        })
    except (KeyError, AttributeError) as e:
        logger.error(f"Model structure error: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': 'Invalid model checkpoint format',
            'hint': 'The checkpoint file may be corrupted or from an incompatible version.',
            'details': str(e)
        })
    except Exception as e:
        logger.error(f"Unexpected error loading model: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': 'Failed to load model',
            'hint': 'Check logs for detailed error information.',
            'details': str(e)
        })


@app.route('/api/load_csv', methods=['POST'])
def load_csv_endpoint():
    """Load a CSV file with comprehensive error handling."""
    try:
        data = request.json
        if not data:
            logger.error("No data provided in load_csv request")
            return jsonify({'success': False, 'error': 'No data provided'})

        csv_path = data.get('path')
        if not csv_path:
            logger.error("No CSV path provided")
            return jsonify({'success': False, 'error': 'No CSV path provided'})

        csv_path_obj = Path(csv_path)
        if not csv_path_obj.exists():
            logger.error(f"CSV path does not exist: {csv_path}")
            return jsonify({
                'success': False,
                'error': f'CSV file not found: {csv_path}',
                'hint': 'Make sure the CSV file exists at the specified path.'
            })

        logger.info(f"Loading CSV from: {csv_path}")
        df = pd.read_csv(csv_path)

        if len(df) == 0:
            logger.warning(f"CSV file is empty: {csv_path}")
            return jsonify({
                'success': False,
                'error': 'CSV file is empty',
                'hint': 'The CSV file contains no data rows.'
            })

        state['csv_data'] = df
        state['current_idx'] = 0

        logger.info(f"âœ“ CSV loaded successfully: {len(df)} rows from {csv_path}")
        return jsonify({
            'success': True,
            'total_rows': len(df),
            'columns': list(df.columns)
        })

    except pd.errors.EmptyDataError as e:
        logger.error(f"Empty CSV file: {e}")
        return jsonify({
            'success': False,
            'error': 'CSV file is empty or malformed',
            'hint': 'Check that the CSV file is not corrupted and contains data.',
            'details': str(e)
        })
    except pd.errors.ParserError as e:
        logger.error(f"CSV parsing error: {e}")
        return jsonify({
            'success': False,
            'error': 'Failed to parse CSV file',
            'hint': 'The CSV file may be malformed or use an unexpected format.',
            'details': str(e)
        })
    except Exception as e:
        logger.error(f"Unexpected error loading CSV: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': 'Failed to load CSV',
            'hint': 'Check logs for detailed error information.',
            'details': str(e)
        })


@app.route('/api/control', methods=['POST'])
def control():
    """Control playback."""
    data = request.json
    action = data.get('action')

    if action == 'start':
        state['running'] = True
    elif action == 'pause':
        state['running'] = False
    elif action == 'reset':
        state['running'] = False
        state['current_idx'] = 0
        state['buffer'].clear()
        state['predictions_history'].clear()

    return jsonify({'success': True})


@app.route('/api/status')
def status():
    """Get current status."""
    return jsonify({
        'running': state['running'],
        'current_idx': state['current_idx'],
        'total_rows': len(state['csv_data']) if state['csv_data'] is not None else 0,
        'buffer_size': len(state['buffer']),
    })


@app.route('/api/step')
def step():
    """Process one timestep."""
    if state['csv_data'] is None:
        return jsonify({'success': False, 'error': 'No CSV loaded'})

    if not state['running']:
        return jsonify({'success': False, 'error': 'Not running'})

    if state['current_idx'] >= len(state['csv_data']):
        return jsonify({'success': False, 'error': 'End of data'})

    # Get current row
    row = state['csv_data'].iloc[state['current_idx']]

    # Extract features with validation
    try:
        continuous, categorical = extract_features_with_validation(
            row,
            state.get('metadata'),
            state['config'] if state['model'] else None
        )
    except ValueError as e:
        logger.error(f"Feature extraction failed: {e}")
        return jsonify({
            'success': False,
            'error': f'Feature dimension mismatch: {str(e)}',
            'hint': 'Make sure the CSV was preprocessed with the same settings as the model was trained with.'
        })
    except Exception as e:
        logger.error(f"Unexpected error during feature extraction: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': f'Feature extraction error: {str(e)}'
        })

    # Ground truth (if available)
    ground_truth = row.get('gcode_string', None)

    # Process
    predictions = process_sample(continuous, categorical, ground_truth)

    # Increment
    state['current_idx'] += 1

    # Store history
    if predictions:
        state['predictions_history'].append(predictions)
        # Keep only last 200 for performance
        if len(state['predictions_history']) > 200:
            state['predictions_history'] = state['predictions_history'][-200:]

    # Prepare sensor data for visualization (all sensors with values)
    if state.get('metadata') and 'master_columns' in state['metadata']:
        sensor_data = {}
        for i, col in enumerate(state['metadata']['master_columns']):
            if col in row.index:
                sensor_data[col] = float(row[col])
            else:
                sensor_data[col] = 0.0  # Zero-padded sensors
    else:
        # Fallback
        sensor_data = {col: float(row[col]) for col in cont_cols[:50] if col in row.index}

    # Prepare 3D position data
    position_3d = {
        'x': float(row['mpox']) if 'mpox' in row.index else 0,
        'y': float(row['mpoy']) if 'mpoy' in row.index else 0,
        'z': float(row['mpoz']) if 'mpoz' in row.index else 0,
    }

    return jsonify({
        'success': True,
        'predictions': predictions,
        'sensor_data': sensor_data,
        'position_3d': position_3d,
        'history': state['predictions_history'][-50:],  # Last 50 for charts
    })


@app.route('/api/confusion_matrix')
def get_confusion_matrix():
    """Get current confusion matrix with Redis caching."""
    # Create cache key based on confusion matrix hash
    cm_str = json.dumps(state['confusion_matrix'], sort_keys=True)
    cache_key = f"confusion_matrix:{hash(cm_str)}"

    # Try to get from Redis cache
    if REDIS_AVAILABLE:
        try:
            cached_result = redis_client.get(cache_key)
            if cached_result:
                logger.debug("Confusion matrix cache hit")
                return jsonify(json.loads(cached_result))
        except Exception as e:
            logger.warning(f"Redis get failed: {e}")

    # Cache miss - build confusion matrix
    matrix_data = []
    all_gcodes = set()

    for true_label in state['confusion_matrix']:
        all_gcodes.add(true_label)
        for pred_label in state['confusion_matrix'][true_label]:
            all_gcodes.add(pred_label)

    all_gcodes = sorted(list(all_gcodes))

    # Build matrix
    for true_label in all_gcodes:
        row = []
        for pred_label in all_gcodes:
            count = state['confusion_matrix'].get(true_label, {}).get(pred_label, 0)
            row.append(count)
        matrix_data.append(row)

    result = {
        'matrix': matrix_data,
        'labels': all_gcodes
    }

    # Cache result in Redis (TTL: 30 seconds)
    if REDIS_AVAILABLE:
        try:
            redis_client.setex(cache_key, 30, json.dumps(result))
            logger.debug("Confusion matrix result cached")
        except Exception as e:
            logger.warning(f"Redis setex failed: {e}")

    return jsonify(result)


@app.route('/api/export', methods=['GET'])
def export_data():
    """Export session data as CSV."""
    if not state['predictions_history']:
        return jsonify({'success': False, 'error': 'No data to export'})

    # Create DataFrame
    export_data = []
    for i, pred in enumerate(state['predictions_history']):
        export_data.append({
            'index': i,
            'timestamp': pred['timestamp'],
            'gcode_predicted': pred['gcode_text'],
            'confidence': pred['gcode_confidence'],
            'anomaly_score': pred['anomaly_score'],
        })

    df = pd.DataFrame(export_data)

    # Create CSV in memory
    output = io.StringIO()
    df.to_csv(output, index=False)
    output.seek(0)

    # Send as file
    return send_file(
        io.BytesIO(output.getvalue().encode()),
        mimetype='text/csv',
        as_attachment=True,
        download_name=f'dashboard_export_{datetime.now().strftime("%Y%m%d_%H%M%S")}.csv'
    )


@app.route('/api/statistics')
def get_statistics():
    """Get running statistics."""
    stats = {}

    for key, values in state['statistics'].items():
        if values:
            recent = values[-100:]  # Last 100 samples
            stats[key] = {
                'mean': float(np.mean(recent)),
                'std': float(np.std(recent)),
                'min': float(np.min(recent)),
                'max': float(np.max(recent)),
            }

    return jsonify(stats)


@app.route('/api/settings', methods=['GET', 'POST'])
def settings():
    """Get or update generation settings."""
    if request.method == 'POST':
        data = request.json
        # Update settings
        for key in ['enable_autoregressive', 'max_tokens', 'temperature', 'top_p', 'beam_size', 'use_beam_search']:
            if key in data:
                state['gen_settings'][key] = data[key]
        return jsonify({'success': True, 'settings': state['gen_settings']})
    else:
        return jsonify(state['gen_settings'])


@app.route('/api/generation_history')
def get_generation_history():
    """Get recent generation history."""
    return jsonify({
        'history': list(state['generation_history'])
    })


@app.route('/api/command_types')
def get_command_types():
    """Get command type distribution."""
    return jsonify({
        'types': dict(state['command_types']),
        'total': sum(state['command_types'].values())
    })


@app.route('/api/export_gcode', methods=['GET'])
def export_gcode():
    """Export generated G-code commands."""
    if not state['generation_history']:
        return jsonify({'success': False, 'error': 'No data to export'})

    # Create text file
    output = io.StringIO()
    output.write("; Generated G-Code from Dashboard\n")
    output.write(f"; Exported: {datetime.now().isoformat()}\n\n")

    for i, entry in enumerate(state['generation_history'], 1):
        output.write(f"; Command {i}\n")
        output.write(f"; Predicted: {entry['predicted']}\n")
        if entry['ground_truth']:
            output.write(f"; Ground Truth: {entry['ground_truth']}\n")
        output.write(f"; Confidence: {entry['confidence']:.4f}\n")
        output.write(f"{entry['predicted']}\n\n")

    output.seek(0)

    return send_file(
        io.BytesIO(output.getvalue().encode()),
        mimetype='text/plain',
        as_attachment=True,
        download_name=f'generated_gcode_{datetime.now().strftime("%Y%m%d_%H%M%S")}.nc'
    )


@app.route('/api/tsne')
def get_tsne():
    """Get t-SNE projection of fingerprint embeddings with Redis caching."""
    try:
        if not state['fingerprints'] or len(state['fingerprints']) < 10:
            return jsonify({'success': False, 'error': 'Not enough data for t-SNE (need at least 10 samples)'})

        # Get fingerprints
        embeddings = np.array(state['fingerprints'])

        # Create cache key based on fingerprints hash
        cache_key = f"tsne:{hash(embeddings.tobytes())}:{len(embeddings)}"

        # Try to get from Redis cache
        if REDIS_AVAILABLE:
            try:
                cached_result = redis_client.get(cache_key)
                if cached_result:
                    logger.info("t-SNE cache hit")
                    return jsonify(json.loads(cached_result))
            except Exception as e:
                logger.warning(f"Redis get failed: {e}")

        # Cache miss - compute t-SNE
        logger.info(f"t-SNE cache miss - computing for {len(embeddings)} samples")
        perplexity = min(30, len(embeddings) - 1)
        tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42, max_iter=300)
        embeddings_2d = tsne.fit_transform(embeddings)

        # Get corresponding G-code labels from history
        labels = []
        for i, entry in enumerate(list(state['generation_history'])[-len(embeddings):]):
            labels.append(entry.get('predicted', 'Unknown'))

        # Pad labels if needed
        while len(labels) < len(embeddings):
            labels.insert(0, 'Unknown')

        result = {
            'success': True,
            'embeddings': embeddings_2d.tolist(),
            'labels': labels,
            'n_samples': len(embeddings)
        }

        # Cache result in Redis (TTL: 60 seconds)
        if REDIS_AVAILABLE:
            try:
                redis_client.setex(cache_key, 60, json.dumps(result))
                logger.info("t-SNE result cached")
            except Exception as e:
                logger.warning(f"Redis setex failed: {e}")

        return jsonify(result)
    except Exception as e:
        logger.error(f"t-SNE error: {e}")
        return jsonify({'success': False, 'error': str(e)})


# ============================================================================
# WebSocket Event Handlers
# ============================================================================

@socketio.on('connect')
def handle_connect():
    """Handle WebSocket connection."""
    logger.info("Client connected")
    emit('connection_status', {'status': 'connected', 'message': 'WebSocket connected successfully'})


@socketio.on('disconnect')
def handle_disconnect():
    """Handle WebSocket disconnection."""
    logger.info("Client disconnected")
    # Stop inference if running
    if state['running']:
        state['running'] = False


@socketio.on('start_inference')
def handle_start_inference(data=None):
    """Start streaming inference results via WebSocket with comprehensive error handling."""
    try:
        # Validate prerequisites
        if not validate_csv_loaded():
            return
        if not validate_model_loaded():
            return

        logger.info("Starting WebSocket inference stream")
        state['running'] = True
        state['current_idx'] = 0

        # Reset state
        state['buffer'].clear()
        state['predictions_history'].clear()
        state['confusion_matrix'].clear()
        state['fingerprints'].clear()
        state['generation_history'].clear()

        emit('inference_started', {'status': 'started', 'total_rows': len(state['csv_data'])})

        # Stream predictions in real-time
        error_count = 0
        max_consecutive_errors = 5

        while state['running'] and state['current_idx'] < len(state['csv_data']):
            try:
                # Get current row
                row = state['csv_data'].iloc[state['current_idx']]

                # Extract features with validation
                try:
                    continuous, categorical = extract_features_with_validation(
                        row,
                        state.get('metadata'),
                        state['config'] if state['model'] else None
                    )
                    error_count = 0  # Reset error counter on success
                except ValueError as e:
                    logger.error(f"Feature extraction failed at row {state['current_idx']}: {e}")
                    emit_error(
                        f'Feature dimension mismatch at row {state["current_idx"]}',
                        hint='Make sure the CSV was preprocessed with the same settings as the model was trained with.',
                        exception=e
                    )
                    state['running'] = False
                    return

                # Ground truth (if available)
                ground_truth = row.get('gcode_string', None)

                # Process sample
                try:
                    predictions = process_sample(continuous, categorical, ground_truth)
                except Exception as e:
                    logger.error(f"Processing error at row {state['current_idx']}: {e}", exc_info=True)
                    error_count += 1
                    if error_count >= max_consecutive_errors:
                        emit_error(
                            f'Too many consecutive processing errors ({error_count})',
                            hint='Check model compatibility and data format.',
                            exception=e
                        )
                        state['running'] = False
                        return
                    # Skip this sample and continue
                    state['current_idx'] += 1
                    continue

                # Increment
                state['current_idx'] += 1

                # Store history
                if predictions:
                    state['predictions_history'].append(predictions)
                    # Keep only last 200 for performance
                    if len(state['predictions_history']) > 200:
                        state['predictions_history'] = state['predictions_history'][-200:]

                    # Extract sensor data for visualization (all sensors with values)
                    sensor_data = {}
                    if state.get('metadata') and 'master_columns' in state['metadata']:
                        for i, col in enumerate(state['metadata']['master_columns']):
                            if col in row.index:
                                sensor_data[col] = float(row[col])
                            else:
                                sensor_data[col] = 0.0  # Zero-padded sensors

                    # Extract 3D position data
                    position_3d = {
                        'x': float(row['mpox']) if 'mpox' in row.index else 0,
                        'y': float(row['mpoy']) if 'mpoy' in row.index else 0,
                        'z': float(row['mpoz']) if 'mpoz' in row.index else 0,
                    }

                    # Add sensor_data and position_3d to predictions for frontend
                    predictions['sensor_data'] = sensor_data
                    predictions['position_3d'] = position_3d

                    # Emit real-time update with sensor and position data
                    emit('prediction_update', {
                        'predictions': predictions,
                        'current_idx': state['current_idx'],
                        'total_rows': len(state['csv_data']),
                        'progress': (state['current_idx'] / len(state['csv_data'])) * 100
                    })

                # Small delay to prevent overwhelming the client
                socketio.sleep(0.01)  # 10ms delay = ~100 updates/second max

            except Exception as e:
                # Catch any unexpected errors in the processing loop
                logger.error(f"Unexpected error in inference loop at row {state['current_idx']}: {e}", exc_info=True)
                error_count += 1
                if error_count >= max_consecutive_errors:
                    emit_error(
                        f'Too many consecutive errors in inference loop',
                        hint='Check logs for details. Consider reloading the model or CSV.',
                        exception=e
                    )
                    state['running'] = False
                    return
                # Skip this sample and continue
                state['current_idx'] += 1

        # Finished
        if state['current_idx'] >= len(state['csv_data']):
            logger.info("Inference stream completed")
            emit('inference_completed', {'status': 'completed', 'total_processed': state['current_idx']})
            state['running'] = False

    except Exception as e:
        # Catch any top-level errors
        logger.error(f"Critical error in start_inference: {e}", exc_info=True)
        emit_error(
            'Critical error during inference',
            hint='Check logs for details. Try reloading the dashboard.',
            exception=e
        )
        state['running'] = False


@socketio.on('stop_inference')
def handle_stop_inference():
    """Stop the inference stream."""
    logger.info("Stopping WebSocket inference stream")
    state['running'] = False
    emit('inference_stopped', {'status': 'stopped', 'current_idx': state['current_idx']})


@socketio.on('request_status')
def handle_request_status():
    """Send current dashboard status."""
    emit('status_update', {
        'running': state['running'],
        'current_idx': state['current_idx'],
        'total_rows': len(state['csv_data']) if state['csv_data'] is not None else 0,
        'buffer_size': len(state['buffer']),
        'model_loaded': state['model'] is not None,
        'csv_loaded': state['csv_data'] is not None,
    })


if __name__ == '__main__':
    print("\n" + "="*60)
    print("ðŸš€ Enhanced G-Code Fingerprinting Dashboard v2.5")
    print("="*60)
    print("\nCore Features:")
    print("  âœ… Token-level & Full command predictions")
    print("  âœ… Live confusion matrix")
    print("  âœ… Dark mode & CSV export")
    print("  âœ… Sensor heatmap (all 232 sensors)")
    print("  âœ… 3D position tracking")
    print("\nAdvanced Features:")
    print("  ðŸ”¥ Beam search generation")
    print("  ðŸ”¥ Nucleus sampling (temperature, top-p)")
    print("  ðŸ”¥ Ground truth comparison & edit distance")
    print("  ðŸ”¥ Generation history (last 50 commands)")
    print("  ðŸ”¥ Command type distribution analytics")
    print("  ðŸ”¥ Per-token confidence breakdown")
    print("  ðŸ”¥ G-Code export (.nc files)")
    print("\nðŸ† NEW - Multi-Head Model Support (Phase 2):")
    print("  âœ¨ 100% G-command accuracy!")
    print("  âœ¨ Hierarchical token prediction (4 heads)")
    print("  âœ¨ Automatic model detection (baseline vs multi-head)")
    print("  âœ¨ Vocabulary v2 support (170 tokens)")
    print("\nQuick Wins v2.5:")
    print("  âš¡ WebSocket support (real-time streaming)")
    print("  âš¡ Redis caching for t-SNE & analytics")
    print("  âš¡ Optimized chart rendering")
    print("  âš¡ Enhanced error handling")
    print("  âš¡ Multi-head architecture support")
    print("\nAPI Endpoints:")
    print("  ðŸ“¡ /api/settings - Control generation parameters")
    print("  ðŸ“¡ /api/generation_history - View prediction history")
    print("  ðŸ“¡ /api/command_types - Command distribution")
    print("  ðŸ“¡ /api/export_gcode - Export G-code")
    print("  ðŸ“¡ /api/tsne - t-SNE embeddings visualization")
    print("\nWebSocket Events:")
    print("  ðŸ”Œ connect/disconnect - Connection management")
    print("  ðŸ”Œ start_inference - Begin streaming predictions")
    print("  ðŸ”Œ stop_inference - Pause streaming")
    print("  ðŸ”Œ prediction_update - Real-time prediction events")
    print(f"\nRedis: {'âœ… Connected' if REDIS_AVAILABLE else 'âŒ Disabled'}")
    print("\nðŸ’¡ Tip: Select 'ðŸ† Multi-Head (100% Command Acc)' from")
    print("         the model dropdown to use the Phase 2 model!")
    print("\nStarting server...")
    print("Open: http://localhost:5001")
    print("="*60 + "\n")

    socketio.run(app, debug=True, port=5001, allow_unsafe_werkzeug=True)
