{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - API Usage Guide\n",
    "\n",
    "Learn how to interact with the G-code fingerprinting FastAPI server.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Overview](#1.-Overview)\n",
    "2. [Starting the API Server](#2.-Starting-the-API-Server)\n",
    "3. [Health Check & Info Endpoints](#3.-Health-Check-&-Info-Endpoints)\n",
    "4. [Single Prediction Requests](#4.-Single-Prediction-Requests)\n",
    "5. [Batch Predictions](#5.-Batch-Predictions)\n",
    "6. [Fingerprint Extraction](#6.-Fingerprint-Extraction)\n",
    "7. [Dynamic Checkpoint Loading](#7.-Dynamic-Checkpoint-Loading)\n",
    "8. [Async Requests with aiohttp](#8.-Async-Requests-with-aiohttp)\n",
    "9. [Error Handling](#9.-Error-Handling)\n",
    "10. [Performance Benchmarking](#10.-Performance-Benchmarking)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview\n",
    "\n",
    "The G-code Fingerprinting API provides REST endpoints for:\n",
    "\n",
    "- **Prediction**: Convert sensor data to G-code sequences\n",
    "- **Fingerprinting**: Extract machine embeddings\n",
    "- **Batch Processing**: Handle multiple samples efficiently\n",
    "- **Model Management**: Dynamically load different checkpoints\n",
    "\n",
    "### API Architecture\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                    FastAPI Server (port 8000)                │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│  GET  /           → API info and available endpoints        │\n",
    "│  GET  /health     → Server health and model status          │\n",
    "│  GET  /info       → Model configuration details             │\n",
    "│  POST /predict    → Single sample prediction                │\n",
    "│  POST /batch_predict → Batch predictions (up to 32)        │\n",
    "│  POST /fingerprint   → Extract machine fingerprint         │\n",
    "│  POST /load_checkpoint → Load different model              │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Environment Setup\n",
    "# ============================================================\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "# Project root\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root / 'src'))\n",
    "\n",
    "# API Configuration\n",
    "API_BASE_URL = 'http://localhost:8000'\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"G-CODE FINGERPRINTING API CLIENT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"API Base URL: {API_BASE_URL}\")\n",
    "print(f\"Project Root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Starting the API Server\n",
    "\n",
    "Before using the API, start the FastAPI server in a separate terminal:\n",
    "\n",
    "```bash\n",
    "# Option 1: Run directly\n",
    "PYTHONPATH=src .venv/bin/python src/miracle/api/server.py\n",
    "\n",
    "# Option 2: Run with uvicorn (hot reload)\n",
    "PYTHONPATH=src .venv/bin/uvicorn miracle.api.server:app --reload --host 0.0.0.0 --port 8000\n",
    "\n",
    "# Option 3: Production mode\n",
    "PYTHONPATH=src .venv/bin/uvicorn miracle.api.server:app --host 0.0.0.0 --port 8000 --workers 4\n",
    "```\n",
    "\n",
    "The server will:\n",
    "1. Load the default checkpoint from `outputs/training_50epoch/checkpoint_best.pt`\n",
    "2. Start listening on port 8000\n",
    "3. Provide interactive docs at http://localhost:8000/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to check server status\n",
    "def check_server_status():\n",
    "    \"\"\"Check if the API server is running.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f'{API_BASE_URL}/', timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            return True, response.json()\n",
    "        return False, None\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        return False, None\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "# Check if server is running\n",
    "is_running, server_info = check_server_status()\n",
    "\n",
    "if is_running:\n",
    "    print(\"\\n✓ API Server is running!\")\n",
    "    print(f\"  Name: {server_info.get('name', 'Unknown')}\")\n",
    "    print(f\"  Version: {server_info.get('version', 'Unknown')}\")\n",
    "    print(f\"  Status: {server_info.get('status', 'Unknown')}\")\n",
    "    print(f\"\\n  Available endpoints:\")\n",
    "    for name, path in server_info.get('endpoints', {}).items():\n",
    "        print(f\"    • {name}: {path}\")\n",
    "else:\n",
    "    print(\"\\n✗ API Server is not running!\")\n",
    "    print(\"  Start the server with:\")\n",
    "    print(\"  PYTHONPATH=src .venv/bin/python src/miracle/api/server.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Health Check & Info Endpoints\n",
    "\n",
    "These endpoints provide server status and model information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Health Check\n",
    "def get_health():\n",
    "    \"\"\"Get API health status.\"\"\"\n",
    "    response = requests.get(f'{API_BASE_URL}/health')\n",
    "    return response.json()\n",
    "\n",
    "try:\n",
    "    health = get_health()\n",
    "    print(\"\\nHealth Check Response:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"  Status: {health.get('status', 'unknown')}\")\n",
    "    print(f\"  Model Loaded: {health.get('model_loaded', False)}\")\n",
    "    print(f\"  Model Version: {health.get('model_version', 'none')}\")\n",
    "    print(f\"  Uptime: {health.get('uptime_seconds', 0):.1f} seconds\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nHealth check failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Information\n",
    "def get_model_info():\n",
    "    \"\"\"Get detailed model information.\"\"\"\n",
    "    response = requests.get(f'{API_BASE_URL}/info')\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return {'error': response.json()}\n",
    "\n",
    "try:\n",
    "    info = get_model_info()\n",
    "    if 'error' not in info:\n",
    "        print(\"\\nModel Information:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"  Model Name: {info.get('model_name', 'unknown')}\")\n",
    "        print(f\"  Version: {info.get('model_version', 'unknown')}\")\n",
    "        print(f\"  Vocabulary Size: {info.get('vocab_size', 0)}\")\n",
    "        print(f\"  Hidden Dimension: {info.get('d_model', 0)}\")\n",
    "        print(f\"  Parameters: {info.get('num_parameters', 0):,}\")\n",
    "        print(f\"\\n  Supported Endpoints:\")\n",
    "        for endpoint in info.get('supported_endpoints', []):\n",
    "            print(f\"    • {endpoint}\")\n",
    "        print(f\"\\n  Generation Methods:\")\n",
    "        for method in info.get('supported_generation_methods', []):\n",
    "            print(f\"    • {method}\")\n",
    "    else:\n",
    "        print(f\"\\nCould not get model info: {info.get('error')}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nInfo request failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Single Prediction Requests\n",
    "\n",
    "Make predictions on a single sensor data sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample sensor data\n",
    "def generate_sample_sensor_data(seq_len=64, continuous_dim=155, categorical_dim=4):\n",
    "    \"\"\"Generate random sensor data for testing.\"\"\"\n",
    "    return {\n",
    "        'continuous': np.random.randn(seq_len, continuous_dim).tolist(),\n",
    "        'categorical': np.random.randint(0, 10, (seq_len, categorical_dim)).tolist()\n",
    "    }\n",
    "\n",
    "# Create sample data\n",
    "sample_data = generate_sample_sensor_data()\n",
    "\n",
    "print(\"Sample Sensor Data:\")\n",
    "print(f\"  Continuous shape: [{len(sample_data['continuous'])}, {len(sample_data['continuous'][0])}]\")\n",
    "print(f\"  Categorical shape: [{len(sample_data['categorical'])}, {len(sample_data['categorical'][0])}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction request\n",
    "def predict_gcode(sensor_data, return_fingerprint=False, inference_config=None):\n",
    "    \"\"\"Send prediction request to API.\"\"\"\n",
    "    payload = {\n",
    "        'sensor_data': sensor_data,\n",
    "        'return_fingerprint': return_fingerprint,\n",
    "    }\n",
    "    if inference_config:\n",
    "        payload['inference_config'] = inference_config\n",
    "    \n",
    "    response = requests.post(\n",
    "        f'{API_BASE_URL}/predict',\n",
    "        json=payload\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return {'error': response.json(), 'status_code': response.status_code}\n",
    "\n",
    "# Make prediction\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    result = predict_gcode(sample_data, return_fingerprint=True)\n",
    "    client_time = (time.time() - start_time) * 1000\n",
    "    \n",
    "    if 'error' not in result:\n",
    "        print(\"\\nPrediction Result:\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"  Model Version: {result.get('model_version', 'unknown')}\")\n",
    "        print(f\"  Server Inference Time: {result.get('inference_time_ms', 0):.1f} ms\")\n",
    "        print(f\"  Client Round-trip Time: {client_time:.1f} ms\")\n",
    "        \n",
    "        gcode = result.get('gcode_sequence', [])\n",
    "        print(f\"\\n  G-code Sequence ({len(gcode)} tokens):\")\n",
    "        print(f\"    {' '.join(gcode[:15])}{'...' if len(gcode) > 15 else ''}\")\n",
    "        \n",
    "        if result.get('fingerprint'):\n",
    "            fp = result['fingerprint']\n",
    "            print(f\"\\n  Fingerprint:\")\n",
    "            print(f\"    Dimension: {len(fp)}\")\n",
    "            print(f\"    Norm: {np.linalg.norm(fp):.4f}\")\n",
    "            print(f\"    First 5 values: {fp[:5]}\")\n",
    "    else:\n",
    "        print(f\"\\nPrediction failed: {result.get('error')}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nRequest failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different generation methods\n",
    "generation_methods = ['greedy', 'sampling', 'top_k', 'top_p']\n",
    "\n",
    "print(\"\\nTesting Generation Methods:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for method in generation_methods:\n",
    "    try:\n",
    "        config = {\n",
    "            'method': method,\n",
    "            'temperature': 0.8,\n",
    "            'max_length': 32\n",
    "        }\n",
    "        \n",
    "        result = predict_gcode(sample_data, inference_config=config)\n",
    "        \n",
    "        if 'error' not in result:\n",
    "            gcode = result.get('gcode_sequence', [])\n",
    "            print(f\"\\n  {method.upper()}:\")\n",
    "            print(f\"    Time: {result.get('inference_time_ms', 0):.1f} ms\")\n",
    "            print(f\"    Tokens: {' '.join(gcode[:10])}...\")\n",
    "        else:\n",
    "            print(f\"\\n  {method.upper()}: Failed\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n  {method.upper()}: Error - {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Batch Predictions\n",
    "\n",
    "Process multiple samples in a single request for better efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch prediction function\n",
    "def batch_predict(sensor_data_list, return_fingerprint=False):\n",
    "    \"\"\"Send batch prediction request.\"\"\"\n",
    "    payload = {\n",
    "        'sensor_data_batch': sensor_data_list,\n",
    "        'return_fingerprint': return_fingerprint,\n",
    "    }\n",
    "    \n",
    "    response = requests.post(\n",
    "        f'{API_BASE_URL}/batch_predict',\n",
    "        json=payload\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return {'error': response.json(), 'status_code': response.status_code}\n",
    "\n",
    "# Generate batch of samples\n",
    "batch_sizes = [1, 4, 8, 16]\n",
    "\n",
    "print(\"\\nBatch Prediction Benchmarks:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"{'Batch Size':<12} {'Total Time (ms)':<18} {'Per Sample (ms)':<15}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    try:\n",
    "        batch_data = [generate_sample_sensor_data() for _ in range(batch_size)]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        result = batch_predict(batch_data)\n",
    "        total_time = (time.time() - start_time) * 1000\n",
    "        \n",
    "        if 'error' not in result:\n",
    "            server_time = result.get('total_inference_time_ms', 0)\n",
    "            per_sample = total_time / batch_size\n",
    "            print(f\"{batch_size:<12} {server_time:<18.1f} {per_sample:<15.1f}\")\n",
    "        else:\n",
    "            print(f\"{batch_size:<12} {'Error':<18} {'-':<15}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{batch_size:<12} {'Failed':<18} {str(e)[:15]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Fingerprint Extraction\n",
    "\n",
    "Extract machine fingerprints (embeddings) from sensor data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fingerprint extraction function\n",
    "def extract_fingerprint(sensor_data):\n",
    "    \"\"\"Extract machine fingerprint from sensor data.\"\"\"\n",
    "    payload = {'sensor_data': sensor_data}\n",
    "    \n",
    "    response = requests.post(\n",
    "        f'{API_BASE_URL}/fingerprint',\n",
    "        json=payload\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return {'error': response.json()}\n",
    "\n",
    "# Extract fingerprints from multiple samples\n",
    "print(\"\\nFingerprint Extraction:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "fingerprints = []\n",
    "n_samples = 5\n",
    "\n",
    "for i in range(n_samples):\n",
    "    try:\n",
    "        sample = generate_sample_sensor_data()\n",
    "        result = extract_fingerprint(sample)\n",
    "        \n",
    "        if 'error' not in result:\n",
    "            fp = np.array(result['fingerprint'])\n",
    "            fingerprints.append(fp)\n",
    "            print(f\"  Sample {i+1}: dim={result['embedding_dim']}, norm={result['norm']:.4f}\")\n",
    "        else:\n",
    "            print(f\"  Sample {i+1}: Error\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Sample {i+1}: Failed - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute pairwise similarities\n",
    "if len(fingerprints) >= 2:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Normalize fingerprints\n",
    "    fps_normalized = [fp / np.linalg.norm(fp) for fp in fingerprints]\n",
    "    \n",
    "    # Compute cosine similarity matrix\n",
    "    n = len(fps_normalized)\n",
    "    similarity_matrix = np.zeros((n, n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            similarity_matrix[i, j] = np.dot(fps_normalized[i], fps_normalized[j])\n",
    "    \n",
    "    print(\"\\nCosine Similarity Matrix:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"       \", end=\"\")\n",
    "    for i in range(n):\n",
    "        print(f\"  S{i+1}   \", end=\"\")\n",
    "    print()\n",
    "    \n",
    "    for i in range(n):\n",
    "        print(f\"  S{i+1} \", end=\"\")\n",
    "        for j in range(n):\n",
    "            print(f\" {similarity_matrix[i,j]:5.3f} \", end=\"\")\n",
    "        print()\n",
    "    \n",
    "    # Visualize\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    im = ax.imshow(similarity_matrix, cmap='coolwarm', vmin=0, vmax=1)\n",
    "    ax.set_xticks(range(n))\n",
    "    ax.set_yticks(range(n))\n",
    "    ax.set_xticklabels([f'S{i+1}' for i in range(n)])\n",
    "    ax.set_yticklabels([f'S{i+1}' for i in range(n)])\n",
    "    ax.set_title('Fingerprint Cosine Similarity')\n",
    "    plt.colorbar(im)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Dynamic Checkpoint Loading\n",
    "\n",
    "Load different model checkpoints without restarting the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available checkpoints\n",
    "import glob\n",
    "\n",
    "checkpoint_patterns = [\n",
    "    'outputs/*/checkpoint_best.pt',\n",
    "    'outputs/final_model/checkpoint_best.pt',\n",
    "]\n",
    "\n",
    "available_checkpoints = []\n",
    "for pattern in checkpoint_patterns:\n",
    "    available_checkpoints.extend(glob.glob(str(project_root / pattern)))\n",
    "\n",
    "print(\"\\nAvailable Checkpoints:\")\n",
    "print(\"-\" * 50)\n",
    "for i, cp in enumerate(available_checkpoints[:5]):\n",
    "    rel_path = Path(cp).relative_to(project_root)\n",
    "    size_mb = Path(cp).stat().st_size / (1024 * 1024)\n",
    "    print(f\"  {i+1}. {rel_path} ({size_mb:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint function\n",
    "def load_checkpoint(checkpoint_path, vocab_path=None, device='cpu'):\n",
    "    \"\"\"Load a new checkpoint on the server.\"\"\"\n",
    "    payload = {\n",
    "        'checkpoint_path': checkpoint_path,\n",
    "        'vocab_path': vocab_path,\n",
    "        'device': device\n",
    "    }\n",
    "    \n",
    "    response = requests.post(\n",
    "        f'{API_BASE_URL}/load_checkpoint',\n",
    "        json=payload\n",
    "    )\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "# Example: Load a specific checkpoint\n",
    "# Uncomment to test\n",
    "# if available_checkpoints:\n",
    "#     result = load_checkpoint(\n",
    "#         checkpoint_path=str(available_checkpoints[0]),\n",
    "#         vocab_path='data/gcode_vocab_v2.json',\n",
    "#         device='cpu'\n",
    "#     )\n",
    "#     print(f\"Load result: {result}\")\n",
    "\n",
    "print(\"\\nTo load a checkpoint, use:\")\n",
    "print(\"  result = load_checkpoint('outputs/model/checkpoint_best.pt')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Async Requests with aiohttp\n",
    "\n",
    "For high-throughput applications, use async requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Async client example\n",
    "import asyncio\n",
    "\n",
    "try:\n",
    "    import aiohttp\n",
    "    AIOHTTP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    AIOHTTP_AVAILABLE = False\n",
    "    print(\"aiohttp not installed. Install with: pip install aiohttp\")\n",
    "\n",
    "if AIOHTTP_AVAILABLE:\n",
    "    async def async_predict(session, sensor_data):\n",
    "        \"\"\"Async prediction request.\"\"\"\n",
    "        async with session.post(\n",
    "            f'{API_BASE_URL}/predict',\n",
    "            json={'sensor_data': sensor_data}\n",
    "        ) as response:\n",
    "            return await response.json()\n",
    "    \n",
    "    async def run_concurrent_predictions(n_requests=10):\n",
    "        \"\"\"Run multiple predictions concurrently.\"\"\"\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            # Generate sample data\n",
    "            samples = [generate_sample_sensor_data() for _ in range(n_requests)]\n",
    "            \n",
    "            # Create tasks\n",
    "            start = time.time()\n",
    "            tasks = [async_predict(session, sample) for sample in samples]\n",
    "            results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "            total_time = time.time() - start\n",
    "            \n",
    "            return results, total_time\n",
    "    \n",
    "    # Run concurrent predictions\n",
    "    # Note: In Jupyter, use nest_asyncio if needed\n",
    "    try:\n",
    "        import nest_asyncio\n",
    "        nest_asyncio.apply()\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    print(\"\\nAsync Concurrent Predictions:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for n_requests in [5, 10, 20]:\n",
    "        try:\n",
    "            results, total_time = asyncio.get_event_loop().run_until_complete(\n",
    "                run_concurrent_predictions(n_requests)\n",
    "            )\n",
    "            successful = sum(1 for r in results if not isinstance(r, Exception) and 'error' not in r)\n",
    "            print(f\"  {n_requests} requests: {total_time:.2f}s total, {successful}/{n_requests} successful\")\n",
    "        except Exception as e:\n",
    "            print(f\"  {n_requests} requests: Failed - {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Error Handling\n",
    "\n",
    "Handle common API errors gracefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API client with error handling\n",
    "class GCodeAPIClient:\n",
    "    \"\"\"API client with proper error handling.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url='http://localhost:8000', timeout=30):\n",
    "        self.base_url = base_url\n",
    "        self.timeout = timeout\n",
    "    \n",
    "    def _request(self, method, endpoint, **kwargs):\n",
    "        \"\"\"Make HTTP request with error handling.\"\"\"\n",
    "        url = f\"{self.base_url}{endpoint}\"\n",
    "        kwargs['timeout'] = self.timeout\n",
    "        \n",
    "        try:\n",
    "            if method == 'GET':\n",
    "                response = requests.get(url, **kwargs)\n",
    "            elif method == 'POST':\n",
    "                response = requests.post(url, **kwargs)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported method: {method}\")\n",
    "            \n",
    "            # Check for HTTP errors\n",
    "            response.raise_for_status()\n",
    "            return {'success': True, 'data': response.json()}\n",
    "            \n",
    "        except requests.exceptions.ConnectionError:\n",
    "            return {'success': False, 'error': 'Connection failed. Is the server running?'}\n",
    "        except requests.exceptions.Timeout:\n",
    "            return {'success': False, 'error': f'Request timed out after {self.timeout}s'}\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            return {'success': False, 'error': f'HTTP error: {e}', 'status_code': response.status_code}\n",
    "        except Exception as e:\n",
    "            return {'success': False, 'error': str(e)}\n",
    "    \n",
    "    def health(self):\n",
    "        return self._request('GET', '/health')\n",
    "    \n",
    "    def predict(self, sensor_data, **kwargs):\n",
    "        payload = {'sensor_data': sensor_data, **kwargs}\n",
    "        return self._request('POST', '/predict', json=payload)\n",
    "    \n",
    "    def fingerprint(self, sensor_data):\n",
    "        payload = {'sensor_data': sensor_data}\n",
    "        return self._request('POST', '/fingerprint', json=payload)\n",
    "\n",
    "# Test error handling\n",
    "print(\"\\nError Handling Examples:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "client = GCodeAPIClient()\n",
    "\n",
    "# Test health check\n",
    "result = client.health()\n",
    "print(f\"\\n  Health check: {'Success' if result['success'] else result['error']}\")\n",
    "\n",
    "# Test with valid data\n",
    "result = client.predict(generate_sample_sensor_data())\n",
    "print(f\"  Valid prediction: {'Success' if result['success'] else result['error']}\")\n",
    "\n",
    "# Test with invalid data (wrong shape)\n",
    "invalid_data = {'continuous': [[1, 2, 3]], 'categorical': [[1, 2]]}  # Wrong dimensions\n",
    "result = client.predict(invalid_data)\n",
    "print(f\"  Invalid data: {'Success' if result['success'] else 'Handled error'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Performance Benchmarking\n",
    "\n",
    "Benchmark API performance under various conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive benchmark\n",
    "def benchmark_api(n_iterations=20):\n",
    "    \"\"\"Run performance benchmark.\"\"\"\n",
    "    results = {\n",
    "        'latencies': [],\n",
    "        'server_times': [],\n",
    "        'successful': 0,\n",
    "        'failed': 0\n",
    "    }\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        sample = generate_sample_sensor_data()\n",
    "        \n",
    "        start = time.time()\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f'{API_BASE_URL}/predict',\n",
    "                json={'sensor_data': sample},\n",
    "                timeout=30\n",
    "            )\n",
    "            latency = (time.time() - start) * 1000\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                results['latencies'].append(latency)\n",
    "                results['server_times'].append(data.get('inference_time_ms', 0))\n",
    "                results['successful'] += 1\n",
    "            else:\n",
    "                results['failed'] += 1\n",
    "        except Exception:\n",
    "            results['failed'] += 1\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run benchmark\n",
    "print(\"\\nRunning Performance Benchmark (20 iterations)...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    bench_results = benchmark_api(20)\n",
    "    \n",
    "    if bench_results['latencies']:\n",
    "        latencies = np.array(bench_results['latencies'])\n",
    "        server_times = np.array(bench_results['server_times'])\n",
    "        \n",
    "        print(f\"\\n  Requests: {bench_results['successful']} successful, {bench_results['failed']} failed\")\n",
    "        print(f\"\\n  Client Latency (ms):\")\n",
    "        print(f\"    Mean: {latencies.mean():.1f}\")\n",
    "        print(f\"    Std:  {latencies.std():.1f}\")\n",
    "        print(f\"    P50:  {np.percentile(latencies, 50):.1f}\")\n",
    "        print(f\"    P95:  {np.percentile(latencies, 95):.1f}\")\n",
    "        print(f\"    P99:  {np.percentile(latencies, 99):.1f}\")\n",
    "        \n",
    "        print(f\"\\n  Server Inference Time (ms):\")\n",
    "        print(f\"    Mean: {server_times.mean():.1f}\")\n",
    "        print(f\"    Std:  {server_times.std():.1f}\")\n",
    "        \n",
    "        network_overhead = latencies.mean() - server_times.mean()\n",
    "        print(f\"\\n  Network Overhead: {network_overhead:.1f} ms\")\n",
    "    else:\n",
    "        print(\"  No successful requests\")\n",
    "except Exception as e:\n",
    "    print(f\"  Benchmark failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "- **Starting the API**: FastAPI server with uvicorn\n",
    "- **Health checks**: Monitor server and model status\n",
    "- **Predictions**: Single and batch prediction requests\n",
    "- **Fingerprints**: Extract machine embeddings\n",
    "- **Dynamic loading**: Change models without restart\n",
    "- **Async requests**: High-throughput with aiohttp\n",
    "- **Error handling**: Robust API client patterns\n",
    "- **Benchmarking**: Measure performance metrics\n",
    "\n",
    "### Quick Reference\n",
    "\n",
    "```python\n",
    "# Health check\n",
    "requests.get('http://localhost:8000/health')\n",
    "\n",
    "# Single prediction\n",
    "requests.post('http://localhost:8000/predict', json={'sensor_data': data})\n",
    "\n",
    "# Batch prediction\n",
    "requests.post('http://localhost:8000/batch_predict', json={'sensor_data_batch': [data1, data2]})\n",
    "\n",
    "# Fingerprint\n",
    "requests.post('http://localhost:8000/fingerprint', json={'sensor_data': data})\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Navigation:**\n",
    "← [Previous: 04_inference_prediction](04_inference_prediction.ipynb) |\n",
    "[Next: 06_dashboard_usage](06_dashboard_usage.ipynb) →\n",
    "\n",
    "**Related:** [03_training_models](03_training_models.ipynb) | [08_model_evaluation](08_model_evaluation.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
