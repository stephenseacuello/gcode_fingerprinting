{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Error Analysis\n",
    "\n",
    "Deep dive into model errors to understand failure modes and identify improvement opportunities.\n",
    "\n",
    "## Contents\n",
    "1. [Setup](#1-setup)\n",
    "2. [Error Pattern Analysis](#2-error-pattern-analysis)\n",
    "3. [Hard Example Mining](#3-hard-example-mining)\n",
    "4. [Failure Mode Taxonomy](#4-failure-mode-taxonomy)\n",
    "5. [Per-Operation Error Analysis](#5-per-operation-error-analysis)\n",
    "6. [Confusion Pattern Mining](#6-confusion-pattern-mining)\n",
    "7. [Dataset Bias Analysis](#7-dataset-bias-analysis)\n",
    "8. [Actionable Recommendations](#8-actionable-recommendations)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root / 'src'))\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, Counter\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Environment check\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Device: {'MPS' if torch.backends.mps.is_available() else 'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and vocabulary\n",
    "from miracle.model.backbone import MMDTAELSTMBackbone\n",
    "from miracle.model.multihead_lm import MultiHeadGCodeLM\n",
    "\n",
    "# Paths\n",
    "VOCAB_PATH = project_root / 'data' / 'gcode_vocab_v2.json'\n",
    "CHECKPOINT_PATH = project_root / 'outputs' / 'final_model' / 'checkpoint_best.pt'\n",
    "DATA_DIR = project_root / 'outputs' / 'processed_v2'\n",
    "\n",
    "# Load vocabulary\n",
    "with open(VOCAB_PATH) as f:\n",
    "    vocab = json.load(f)\n",
    "    \n",
    "token_to_id = vocab['token_to_id']\n",
    "id_to_token = {v: k for k, v in token_to_id.items()}\n",
    "\n",
    "print(f\"Vocabulary size: {len(token_to_id)}\")\n",
    "\n",
    "# Load checkpoint\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    checkpoint = torch.load(CHECKPOINT_PATH, map_location=device, weights_only=False)\n",
    "    config = checkpoint.get('config', {})\n",
    "    print(f\"Loaded checkpoint from epoch {checkpoint.get('epoch', 'unknown')}\")\n",
    "else:\n",
    "    print(f\"Checkpoint not found at {CHECKPOINT_PATH}\")\n",
    "    config = {'hidden_dim': 256, 'num_layers': 4, 'num_heads': 8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "backbone = MMDTAELSTMBackbone(\n",
    "    continuous_dim=155,\n",
    "    categorical_dims=[10, 10, 50, 50],\n",
    "    d_model=config.get('hidden_dim', 256),\n",
    "    num_layers=config.get('num_layers', 4),\n",
    "    num_heads=config.get('num_heads', 8),\n",
    "    dropout=config.get('dropout', 0.1)\n",
    ").to(device)\n",
    "\n",
    "lm = MultiHeadGCodeLM(\n",
    "    d_model=config.get('hidden_dim', 256),\n",
    "    vocab_sizes=vocab.get('head_vocab_sizes', {'type': 10, 'command': 50, 'param_type': 30, 'param_value': 100})\n",
    ").to(device)\n",
    "\n",
    "# Load weights if available\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    backbone.load_state_dict(checkpoint['backbone_state_dict'])\n",
    "    lm.load_state_dict(checkpoint['lm_state_dict'])\n",
    "    print(\"Model weights loaded\")\n",
    "\n",
    "backbone.eval()\n",
    "lm.eval()\n",
    "print(f\"Models loaded on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Error Pattern Analysis\n",
    "\n",
    "Systematically analyze error patterns across different prediction heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ErrorAnalyzer:\n",
    "    \"\"\"Comprehensive error analysis for multi-head G-code prediction.\"\"\"\n",
    "    \n",
    "    def __init__(self, backbone, lm, vocab, device):\n",
    "        self.backbone = backbone\n",
    "        self.lm = lm\n",
    "        self.vocab = vocab\n",
    "        self.device = device\n",
    "        self.errors = defaultdict(list)\n",
    "        self.predictions = []\n",
    "        \n",
    "    def analyze_batch(self, continuous, categorical, targets):\n",
    "        \"\"\"Analyze errors in a batch of predictions.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            hidden = self.backbone(continuous.to(self.device), categorical.to(self.device))\n",
    "            preds = self.lm(hidden)\n",
    "            \n",
    "        results = []\n",
    "        for head in ['type', 'command', 'param_type', 'param_value']:\n",
    "            if head in preds and head in targets:\n",
    "                pred_ids = preds[head].argmax(dim=-1).cpu()\n",
    "                target_ids = targets[head].cpu()\n",
    "                \n",
    "                # Find errors\n",
    "                errors = pred_ids != target_ids\n",
    "                error_indices = torch.where(errors)\n",
    "                \n",
    "                for batch_idx, seq_idx in zip(*error_indices):\n",
    "                    batch_idx, seq_idx = batch_idx.item(), seq_idx.item()\n",
    "                    pred_id = pred_ids[batch_idx, seq_idx].item()\n",
    "                    target_id = target_ids[batch_idx, seq_idx].item()\n",
    "                    \n",
    "                    # Get confidence\n",
    "                    probs = torch.softmax(preds[head][batch_idx, seq_idx], dim=-1)\n",
    "                    confidence = probs[pred_id].item()\n",
    "                    \n",
    "                    self.errors[head].append({\n",
    "                        'batch_idx': batch_idx,\n",
    "                        'seq_idx': seq_idx,\n",
    "                        'pred_id': pred_id,\n",
    "                        'target_id': target_id,\n",
    "                        'confidence': confidence,\n",
    "                        'target_prob': probs[target_id].item()\n",
    "                    })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_error_summary(self):\n",
    "        \"\"\"Get summary statistics for errors.\"\"\"\n",
    "        summary = {}\n",
    "        for head, errors in self.errors.items():\n",
    "            if errors:\n",
    "                confidences = [e['confidence'] for e in errors]\n",
    "                target_probs = [e['target_prob'] for e in errors]\n",
    "                summary[head] = {\n",
    "                    'total_errors': len(errors),\n",
    "                    'avg_confidence': np.mean(confidences),\n",
    "                    'avg_target_prob': np.mean(target_probs),\n",
    "                    'high_conf_errors': sum(1 for c in confidences if c > 0.8),\n",
    "                    'low_conf_errors': sum(1 for c in confidences if c < 0.3)\n",
    "                }\n",
    "        return summary\n",
    "    \n",
    "    def get_confusion_pairs(self, head, top_k=10):\n",
    "        \"\"\"Get most common confusion pairs for a head.\"\"\"\n",
    "        pairs = Counter()\n",
    "        for error in self.errors[head]:\n",
    "            pairs[(error['target_id'], error['pred_id'])] += 1\n",
    "        return pairs.most_common(top_k)\n",
    "\n",
    "# Initialize analyzer\n",
    "analyzer = ErrorAnalyzer(backbone, lm, vocab, device)\n",
    "print(\"ErrorAnalyzer initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data and run analysis\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load test set\n",
    "test_path = DATA_DIR / 'test.pt'\n",
    "\n",
    "if test_path.exists():\n",
    "    test_data = torch.load(test_path, weights_only=False)\n",
    "    print(f\"Test data loaded: {len(test_data.get('continuous', []))} samples\")\n",
    "    \n",
    "    # Create DataLoader\n",
    "    test_dataset = TensorDataset(\n",
    "        torch.tensor(test_data['continuous'], dtype=torch.float32),\n",
    "        torch.tensor(test_data['categorical'], dtype=torch.long),\n",
    "        torch.tensor(test_data.get('type_targets', np.zeros((len(test_data['continuous']), 64))), dtype=torch.long),\n",
    "        torch.tensor(test_data.get('command_targets', np.zeros((len(test_data['continuous']), 64))), dtype=torch.long),\n",
    "        torch.tensor(test_data.get('param_type_targets', np.zeros((len(test_data['continuous']), 64))), dtype=torch.long),\n",
    "        torch.tensor(test_data.get('param_value_targets', np.zeros((len(test_data['continuous']), 64))), dtype=torch.long)\n",
    "    )\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "    \n",
    "    # Run analysis\n",
    "    for batch in tqdm(test_loader, desc=\"Analyzing errors\"):\n",
    "        continuous, categorical, type_t, cmd_t, pt_t, pv_t = batch\n",
    "        targets = {\n",
    "            'type': type_t,\n",
    "            'command': cmd_t,\n",
    "            'param_type': pt_t,\n",
    "            'param_value': pv_t\n",
    "        }\n",
    "        analyzer.analyze_batch(continuous, categorical, targets)\n",
    "    \n",
    "    # Print summary\n",
    "    summary = analyzer.get_error_summary()\n",
    "    for head, stats in summary.items():\n",
    "        print(f\"\\n{head.upper()} Head:\")\n",
    "        for key, value in stats.items():\n",
    "            print(f\"  {key}: {value:.4f}\" if isinstance(value, float) else f\"  {key}: {value}\")\n",
    "else:\n",
    "    print(f\"Test data not found at {test_path}\")\n",
    "    print(\"Using synthetic data for demonstration...\")\n",
    "    \n",
    "    # Create synthetic errors for demo\n",
    "    for head in ['type', 'command', 'param_type', 'param_value']:\n",
    "        for _ in range(100):\n",
    "            analyzer.errors[head].append({\n",
    "                'batch_idx': np.random.randint(0, 10),\n",
    "                'seq_idx': np.random.randint(0, 64),\n",
    "                'pred_id': np.random.randint(0, 20),\n",
    "                'target_id': np.random.randint(0, 20),\n",
    "                'confidence': np.random.random(),\n",
    "                'target_prob': np.random.random() * 0.3\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize error distribution by confidence\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "for ax, head in zip(axes.flat, ['type', 'command', 'param_type', 'param_value']):\n",
    "    if analyzer.errors[head]:\n",
    "        confidences = [e['confidence'] for e in analyzer.errors[head]]\n",
    "        target_probs = [e['target_prob'] for e in analyzer.errors[head]]\n",
    "        \n",
    "        ax.scatter(confidences, target_probs, alpha=0.5, s=20)\n",
    "        ax.axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='50% target prob')\n",
    "        ax.axvline(x=0.5, color='g', linestyle='--', alpha=0.5, label='50% confidence')\n",
    "        ax.set_xlabel('Prediction Confidence')\n",
    "        ax.set_ylabel('Target Probability')\n",
    "        ax.set_title(f'{head.upper()} Head Error Distribution')\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(project_root / 'reports' / 'error_confidence_scatter.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nError Type Classification:\")\n",
    "print(\"- High confidence + Low target prob: Model confidently wrong (concerning)\")\n",
    "print(\"- Low confidence + Low target prob: Model uncertain (expected)\")\n",
    "print(\"- High confidence + High target prob: Near-miss errors (close alternatives)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hard Example Mining\n",
    "\n",
    "Identify and analyze the hardest examples for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HardExampleMiner:\n",
    "    \"\"\"Mine hard examples based on various criteria.\"\"\"\n",
    "    \n",
    "    def __init__(self, errors_by_head):\n",
    "        self.errors = errors_by_head\n",
    "        \n",
    "    def get_high_confidence_errors(self, head, threshold=0.9, top_k=20):\n",
    "        \"\"\"Get errors where model was highly confident but wrong.\"\"\"\n",
    "        high_conf = [e for e in self.errors[head] if e['confidence'] > threshold]\n",
    "        return sorted(high_conf, key=lambda x: -x['confidence'])[:top_k]\n",
    "    \n",
    "    def get_repeated_errors(self, head, min_count=3):\n",
    "        \"\"\"Find error patterns that repeat frequently.\"\"\"\n",
    "        pattern_counts = Counter()\n",
    "        for e in self.errors[head]:\n",
    "            pattern = (e['target_id'], e['pred_id'])\n",
    "            pattern_counts[pattern] += 1\n",
    "        return [(p, c) for p, c in pattern_counts.items() if c >= min_count]\n",
    "    \n",
    "    def get_sequential_errors(self, head, window=3):\n",
    "        \"\"\"Find cases where multiple sequential positions have errors.\"\"\"\n",
    "        by_batch = defaultdict(list)\n",
    "        for e in self.errors[head]:\n",
    "            by_batch[e['batch_idx']].append(e['seq_idx'])\n",
    "        \n",
    "        sequential_cases = []\n",
    "        for batch_idx, positions in by_batch.items():\n",
    "            positions = sorted(positions)\n",
    "            for i in range(len(positions) - window + 1):\n",
    "                if positions[i + window - 1] - positions[i] <= window:\n",
    "                    sequential_cases.append((batch_idx, positions[i:i+window]))\n",
    "        \n",
    "        return sequential_cases[:20]\n",
    "    \n",
    "    def get_cross_head_errors(self):\n",
    "        \"\"\"Find positions where multiple heads fail.\"\"\"\n",
    "        position_errors = defaultdict(set)\n",
    "        for head, errors in self.errors.items():\n",
    "            for e in errors:\n",
    "                key = (e['batch_idx'], e['seq_idx'])\n",
    "                position_errors[key].add(head)\n",
    "        \n",
    "        multi_head_failures = [\n",
    "            (pos, heads) for pos, heads in position_errors.items()\n",
    "            if len(heads) >= 2\n",
    "        ]\n",
    "        return sorted(multi_head_failures, key=lambda x: -len(x[1]))[:20]\n",
    "\n",
    "# Initialize miner\n",
    "miner = HardExampleMiner(analyzer.errors)\n",
    "\n",
    "# Analyze high-confidence errors\n",
    "print(\"High-Confidence Errors (>90% confidence but wrong):\")\n",
    "print(\"=\"*60)\n",
    "for head in ['type', 'command', 'param_type', 'param_value']:\n",
    "    high_conf = miner.get_high_confidence_errors(head, threshold=0.9)\n",
    "    print(f\"\\n{head.upper()}: {len(high_conf)} high-confidence errors\")\n",
    "    if high_conf:\n",
    "        for e in high_conf[:3]:\n",
    "            print(f\"  Predicted: {e['pred_id']} vs Target: {e['target_id']} (conf: {e['confidence']:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize hard examples\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Cross-head failures\n",
    "cross_head = miner.get_cross_head_errors()\n",
    "if cross_head:\n",
    "    failure_counts = Counter([len(heads) for _, heads in cross_head])\n",
    "    axes[0].bar(failure_counts.keys(), failure_counts.values(), color='coral')\n",
    "    axes[0].set_xlabel('Number of Heads Failing')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].set_title('Cross-Head Failure Distribution')\n",
    "    axes[0].set_xticks(list(failure_counts.keys()))\n",
    "else:\n",
    "    axes[0].text(0.5, 0.5, 'No cross-head failures found', ha='center', va='center')\n",
    "    axes[0].set_title('Cross-Head Failure Distribution')\n",
    "\n",
    "# Sequential errors\n",
    "seq_errors = defaultdict(int)\n",
    "for head in ['type', 'command', 'param_type', 'param_value']:\n",
    "    seq = miner.get_sequential_errors(head)\n",
    "    seq_errors[head] = len(seq)\n",
    "\n",
    "axes[1].bar(seq_errors.keys(), seq_errors.values(), color='steelblue')\n",
    "axes[1].set_xlabel('Prediction Head')\n",
    "axes[1].set_ylabel('Sequential Error Clusters')\n",
    "axes[1].set_title('Sequential Error Patterns by Head')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(project_root / 'reports' / 'hard_examples_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Failure Mode Taxonomy\n",
    "\n",
    "Categorize errors into distinct failure modes for targeted improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FailureModeTaxonomy:\n",
    "    \"\"\"Classify errors into distinct failure modes.\"\"\"\n",
    "    \n",
    "    MODES = {\n",
    "        'confident_wrong': 'High confidence (>0.8) but incorrect',\n",
    "        'uncertain_wrong': 'Low confidence (<0.3) and incorrect',\n",
    "        'near_miss': 'Target was 2nd or 3rd choice (target_prob > 0.2)',\n",
    "        'complete_miss': 'Target probability very low (<0.05)',\n",
    "        'boundary_error': 'Error at sequence boundaries (first/last 5 tokens)',\n",
    "        'rare_class': 'Target is a rare class (appears <1% in training)',\n",
    "    }\n",
    "    \n",
    "    def __init__(self, errors, class_frequencies=None):\n",
    "        self.errors = errors\n",
    "        self.class_freq = class_frequencies or {}\n",
    "        self.classified = defaultdict(lambda: defaultdict(list))\n",
    "        \n",
    "    def classify_errors(self, seq_length=64):\n",
    "        \"\"\"Classify all errors into failure modes.\"\"\"\n",
    "        for head, error_list in self.errors.items():\n",
    "            for error in error_list:\n",
    "                modes = self._get_modes(error, seq_length, head)\n",
    "                for mode in modes:\n",
    "                    self.classified[head][mode].append(error)\n",
    "        return self.classified\n",
    "    \n",
    "    def _get_modes(self, error, seq_length, head):\n",
    "        \"\"\"Determine which failure modes apply to an error.\"\"\"\n",
    "        modes = []\n",
    "        \n",
    "        if error['confidence'] > 0.8:\n",
    "            modes.append('confident_wrong')\n",
    "        elif error['confidence'] < 0.3:\n",
    "            modes.append('uncertain_wrong')\n",
    "            \n",
    "        if error['target_prob'] > 0.2:\n",
    "            modes.append('near_miss')\n",
    "        elif error['target_prob'] < 0.05:\n",
    "            modes.append('complete_miss')\n",
    "            \n",
    "        if error['seq_idx'] < 5 or error['seq_idx'] >= seq_length - 5:\n",
    "            modes.append('boundary_error')\n",
    "            \n",
    "        if head in self.class_freq and error['target_id'] in self.class_freq[head]:\n",
    "            if self.class_freq[head][error['target_id']] < 0.01:\n",
    "                modes.append('rare_class')\n",
    "        \n",
    "        return modes if modes else ['unclassified']\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"Get summary of failure modes.\"\"\"\n",
    "        summary = {}\n",
    "        for head, modes in self.classified.items():\n",
    "            summary[head] = {mode: len(errors) for mode, errors in modes.items()}\n",
    "        return summary\n",
    "\n",
    "# Classify errors\n",
    "taxonomy = FailureModeTaxonomy(analyzer.errors)\n",
    "taxonomy.classify_errors()\n",
    "failure_summary = taxonomy.get_summary()\n",
    "\n",
    "print(\"Failure Mode Summary:\")\n",
    "print(\"=\"*60)\n",
    "for head, modes in failure_summary.items():\n",
    "    print(f\"\\n{head.upper()}:\")\n",
    "    for mode, count in sorted(modes.items(), key=lambda x: -x[1]):\n",
    "        desc = taxonomy.MODES.get(mode, 'Unknown mode')\n",
    "        print(f\"  {mode}: {count} ({desc})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize failure mode distribution\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "heads = list(failure_summary.keys())\n",
    "modes = list(taxonomy.MODES.keys())\n",
    "x = np.arange(len(heads))\n",
    "width = 0.12\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(modes)))\n",
    "\n",
    "for i, mode in enumerate(modes):\n",
    "    values = [failure_summary.get(head, {}).get(mode, 0) for head in heads]\n",
    "    ax.bar(x + i * width, values, width, label=mode, color=colors[i])\n",
    "\n",
    "ax.set_xlabel('Prediction Head')\n",
    "ax.set_ylabel('Error Count')\n",
    "ax.set_title('Failure Mode Distribution by Head')\n",
    "ax.set_xticks(x + width * (len(modes) - 1) / 2)\n",
    "ax.set_xticklabels([h.upper() for h in heads])\n",
    "ax.legend(bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(project_root / 'reports' / 'failure_modes.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Per-Operation Error Analysis\n",
    "\n",
    "Analyze errors by G-code operation type (G0, G1, G2, M commands, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_errors_by_operation(errors, id_to_token):\n",
    "    \"\"\"Group errors by operation type.\"\"\"\n",
    "    operation_errors = defaultdict(list)\n",
    "    \n",
    "    for error in errors.get('command', []):\n",
    "        target_id = error['target_id']\n",
    "        if target_id in id_to_token:\n",
    "            token = id_to_token[target_id]\n",
    "            # Extract operation prefix (G0, G1, M3, etc.)\n",
    "            if token.startswith('G') or token.startswith('M'):\n",
    "                op_prefix = ''.join([c for c in token[:3] if c.isalpha() or c.isdigit()])\n",
    "                operation_errors[op_prefix].append(error)\n",
    "            else:\n",
    "                operation_errors['OTHER'].append(error)\n",
    "        else:\n",
    "            operation_errors['UNKNOWN'].append(error)\n",
    "    \n",
    "    return operation_errors\n",
    "\n",
    "# Analyze by operation\n",
    "op_errors = analyze_errors_by_operation(analyzer.errors, id_to_token)\n",
    "\n",
    "print(\"Errors by Operation Type:\")\n",
    "print(\"=\"*60)\n",
    "for op, errors in sorted(op_errors.items(), key=lambda x: -len(x[1])):\n",
    "    avg_conf = np.mean([e['confidence'] for e in errors]) if errors else 0\n",
    "    print(f\"{op:10} | {len(errors):5} errors | avg confidence: {avg_conf:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize operation-specific errors\n",
    "if op_errors:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Error count by operation\n",
    "    ops = list(op_errors.keys())[:10]  # Top 10\n",
    "    counts = [len(op_errors[op]) for op in ops]\n",
    "    axes[0].barh(ops, counts, color='steelblue')\n",
    "    axes[0].set_xlabel('Error Count')\n",
    "    axes[0].set_title('Errors by G-code Operation')\n",
    "    axes[0].invert_yaxis()\n",
    "    \n",
    "    # Confidence distribution by operation\n",
    "    conf_data = []\n",
    "    labels = []\n",
    "    for op in ops[:5]:\n",
    "        if op_errors[op]:\n",
    "            conf_data.append([e['confidence'] for e in op_errors[op]])\n",
    "            labels.append(op)\n",
    "    \n",
    "    if conf_data:\n",
    "        axes[1].boxplot(conf_data, labels=labels)\n",
    "        axes[1].set_ylabel('Prediction Confidence')\n",
    "        axes[1].set_title('Error Confidence by Operation')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(project_root / 'reports' / 'operation_errors.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No operation-specific errors to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Confusion Pattern Mining\n",
    "\n",
    "Identify systematic confusion patterns between token classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mine_confusion_patterns(errors, id_to_token, top_k=15):\n",
    "    \"\"\"Find most common confusion patterns.\"\"\"\n",
    "    patterns = {}\n",
    "    \n",
    "    for head in ['type', 'command', 'param_type', 'param_value']:\n",
    "        if head in errors:\n",
    "            pair_counts = Counter()\n",
    "            for e in errors[head]:\n",
    "                target = e['target_id']\n",
    "                pred = e['pred_id']\n",
    "                pair_counts[(target, pred)] += 1\n",
    "            \n",
    "            patterns[head] = pair_counts.most_common(top_k)\n",
    "    \n",
    "    return patterns\n",
    "\n",
    "# Mine patterns\n",
    "confusion_patterns = mine_confusion_patterns(analyzer.errors, id_to_token)\n",
    "\n",
    "print(\"Top Confusion Patterns:\")\n",
    "print(\"=\"*60)\n",
    "for head, patterns in confusion_patterns.items():\n",
    "    print(f\"\\n{head.upper()} Head:\")\n",
    "    for (target, pred), count in patterns[:5]:\n",
    "        target_name = id_to_token.get(target, f\"ID:{target}\")\n",
    "        pred_name = id_to_token.get(pred, f\"ID:{pred}\")\n",
    "        print(f\"  {target_name:15} → {pred_name:15} ({count} times)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion pattern heatmap\n",
    "def create_confusion_heatmap(patterns, head, top_k=10):\n",
    "    \"\"\"Create heatmap for top confusion patterns.\"\"\"\n",
    "    if head not in patterns or not patterns[head]:\n",
    "        return None\n",
    "    \n",
    "    # Get top classes involved in confusions\n",
    "    involved_classes = set()\n",
    "    for (target, pred), count in patterns[head][:top_k]:\n",
    "        involved_classes.add(target)\n",
    "        involved_classes.add(pred)\n",
    "    \n",
    "    involved_classes = sorted(involved_classes)\n",
    "    n = len(involved_classes)\n",
    "    \n",
    "    # Build confusion matrix\n",
    "    matrix = np.zeros((n, n))\n",
    "    for (target, pred), count in patterns[head]:\n",
    "        if target in involved_classes and pred in involved_classes:\n",
    "            i = involved_classes.index(target)\n",
    "            j = involved_classes.index(pred)\n",
    "            matrix[i, j] = count\n",
    "    \n",
    "    return matrix, involved_classes\n",
    "\n",
    "# Plot heatmaps\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "for ax, head in zip(axes.flat, ['type', 'command', 'param_type', 'param_value']):\n",
    "    result = create_confusion_heatmap(confusion_patterns, head)\n",
    "    if result:\n",
    "        matrix, classes = result\n",
    "        class_labels = [id_to_token.get(c, str(c))[:8] for c in classes]\n",
    "        sns.heatmap(matrix, ax=ax, cmap='YlOrRd', \n",
    "                   xticklabels=class_labels, yticklabels=class_labels,\n",
    "                   annot=True, fmt='.0f', cbar_kws={'label': 'Error Count'})\n",
    "        ax.set_xlabel('Predicted')\n",
    "        ax.set_ylabel('Target')\n",
    "        ax.set_title(f'{head.upper()} Confusion Patterns')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No data', ha='center', va='center')\n",
    "        ax.set_title(f'{head.upper()} Confusion Patterns')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(project_root / 'reports' / 'confusion_patterns.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Dataset Bias Analysis\n",
    "\n",
    "Analyze if errors correlate with dataset characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetBiasAnalyzer:\n",
    "    \"\"\"Analyze correlation between errors and dataset properties.\"\"\"\n",
    "    \n",
    "    def __init__(self, errors, data_stats=None):\n",
    "        self.errors = errors\n",
    "        self.data_stats = data_stats or {}\n",
    "        \n",
    "    def analyze_position_bias(self, seq_length=64):\n",
    "        \"\"\"Check if errors cluster at certain sequence positions.\"\"\"\n",
    "        position_counts = defaultdict(lambda: defaultdict(int))\n",
    "        \n",
    "        for head, error_list in self.errors.items():\n",
    "            for e in error_list:\n",
    "                # Bin positions into groups\n",
    "                pos = e['seq_idx']\n",
    "                if pos < 5:\n",
    "                    bin_name = 'start (0-4)'\n",
    "                elif pos < seq_length // 2:\n",
    "                    bin_name = 'early (5-31)'\n",
    "                elif pos < seq_length - 5:\n",
    "                    bin_name = 'late (32-58)'\n",
    "                else:\n",
    "                    bin_name = 'end (59-63)'\n",
    "                position_counts[head][bin_name] += 1\n",
    "        \n",
    "        return position_counts\n",
    "    \n",
    "    def analyze_class_balance(self):\n",
    "        \"\"\"Check if errors correlate with class frequency.\"\"\"\n",
    "        target_counts = defaultdict(lambda: defaultdict(int))\n",
    "        \n",
    "        for head, error_list in self.errors.items():\n",
    "            for e in error_list:\n",
    "                target_counts[head][e['target_id']] += 1\n",
    "        \n",
    "        return target_counts\n",
    "\n",
    "# Analyze biases\n",
    "bias_analyzer = DatasetBiasAnalyzer(analyzer.errors)\n",
    "\n",
    "# Position bias\n",
    "position_bias = bias_analyzer.analyze_position_bias()\n",
    "print(\"Position Bias Analysis:\")\n",
    "print(\"=\"*60)\n",
    "for head, positions in position_bias.items():\n",
    "    print(f\"\\n{head.upper()}:\")\n",
    "    for pos, count in sorted(positions.items()):\n",
    "        print(f\"  {pos}: {count} errors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize position bias\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Position bias bar chart\n",
    "pos_order = ['start (0-4)', 'early (5-31)', 'late (32-58)', 'end (59-63)']\n",
    "heads = list(position_bias.keys())\n",
    "x = np.arange(len(pos_order))\n",
    "width = 0.2\n",
    "\n",
    "for i, head in enumerate(heads):\n",
    "    values = [position_bias[head].get(pos, 0) for pos in pos_order]\n",
    "    axes[0].bar(x + i * width, values, width, label=head.upper())\n",
    "\n",
    "axes[0].set_xlabel('Sequence Position')\n",
    "axes[0].set_ylabel('Error Count')\n",
    "axes[0].set_title('Errors by Sequence Position')\n",
    "axes[0].set_xticks(x + width * (len(heads) - 1) / 2)\n",
    "axes[0].set_xticklabels(pos_order, rotation=15)\n",
    "axes[0].legend()\n",
    "\n",
    "# Class frequency vs error rate (if we had class frequencies)\n",
    "# For now, show error distribution by target class\n",
    "class_errors = bias_analyzer.analyze_class_balance()\n",
    "if class_errors:\n",
    "    head = 'command'  # Focus on command head\n",
    "    if head in class_errors:\n",
    "        top_classes = sorted(class_errors[head].items(), key=lambda x: -x[1])[:20]\n",
    "        classes, counts = zip(*top_classes) if top_classes else ([], [])\n",
    "        axes[1].bar(range(len(classes)), counts, color='coral')\n",
    "        axes[1].set_xlabel('Target Class ID')\n",
    "        axes[1].set_ylabel('Error Count')\n",
    "        axes[1].set_title(f'Top 20 Error-Prone Classes ({head.upper()} Head)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(project_root / 'reports' / 'dataset_bias.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Actionable Recommendations\n",
    "\n",
    "Based on error analysis, generate specific improvement recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recommendations(error_summary, failure_modes, confusion_patterns, position_bias):\n",
    "    \"\"\"Generate actionable recommendations based on error analysis.\"\"\"\n",
    "    recommendations = []\n",
    "    \n",
    "    # 1. High confidence errors\n",
    "    for head, stats in error_summary.items():\n",
    "        if stats.get('high_conf_errors', 0) > 10:\n",
    "            recommendations.append({\n",
    "                'priority': 'HIGH',\n",
    "                'category': 'Model Confidence',\n",
    "                'issue': f\"{head.upper()} head has {stats['high_conf_errors']} high-confidence errors\",\n",
    "                'recommendation': 'Consider label smoothing, temperature scaling, or focal loss to improve calibration',\n",
    "                'expected_impact': 'Reduce overconfident wrong predictions'\n",
    "            })\n",
    "    \n",
    "    # 2. Failure mode specific\n",
    "    for head, modes in failure_modes.items():\n",
    "        if modes.get('boundary_error', 0) > 20:\n",
    "            recommendations.append({\n",
    "                'priority': 'MEDIUM',\n",
    "                'category': 'Sequence Modeling',\n",
    "                'issue': f\"{head.upper()} has {modes['boundary_error']} boundary errors\",\n",
    "                'recommendation': 'Add positional encoding or use bidirectional context for sequence boundaries',\n",
    "                'expected_impact': 'Improve predictions at sequence start/end'\n",
    "            })\n",
    "        \n",
    "        if modes.get('rare_class', 0) > 15:\n",
    "            recommendations.append({\n",
    "                'priority': 'HIGH',\n",
    "                'category': 'Data Imbalance',\n",
    "                'issue': f\"{head.upper()} struggles with rare classes ({modes['rare_class']} errors)\",\n",
    "                'recommendation': 'Increase class weights for rare classes or oversample rare examples',\n",
    "                'expected_impact': 'Better handling of long-tail distribution'\n",
    "            })\n",
    "    \n",
    "    # 3. Confusion patterns\n",
    "    for head, patterns in confusion_patterns.items():\n",
    "        if patterns and patterns[0][1] > 15:  # Top confusion > 15 occurrences\n",
    "            (target, pred), count = patterns[0]\n",
    "            recommendations.append({\n",
    "                'priority': 'MEDIUM',\n",
    "                'category': 'Feature Engineering',\n",
    "                'issue': f\"{head.upper()}: Frequent confusion between class {target} and {pred} ({count} times)\",\n",
    "                'recommendation': 'Add discriminative features to distinguish these classes, or merge if semantically similar',\n",
    "                'expected_impact': 'Reduce systematic confusion'\n",
    "            })\n",
    "    \n",
    "    # 4. Position bias\n",
    "    for head, positions in position_bias.items():\n",
    "        start_errors = positions.get('start (0-4)', 0)\n",
    "        end_errors = positions.get('end (59-63)', 0)\n",
    "        middle_errors = positions.get('early (5-31)', 0) + positions.get('late (32-58)', 0)\n",
    "        \n",
    "        if (start_errors + end_errors) > middle_errors * 0.5:\n",
    "            recommendations.append({\n",
    "                'priority': 'MEDIUM',\n",
    "                'category': 'Architecture',\n",
    "                'issue': f\"{head.upper()} has disproportionate boundary errors\",\n",
    "                'recommendation': 'Consider adding special boundary tokens or using different padding strategy',\n",
    "                'expected_impact': 'More consistent predictions across sequence'\n",
    "            })\n",
    "    \n",
    "    return sorted(recommendations, key=lambda x: {'HIGH': 0, 'MEDIUM': 1, 'LOW': 2}[x['priority']])\n",
    "\n",
    "# Generate recommendations\n",
    "recommendations = generate_recommendations(\n",
    "    analyzer.get_error_summary(),\n",
    "    taxonomy.get_summary(),\n",
    "    confusion_patterns,\n",
    "    position_bias\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ACTIONABLE RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"\\n[{rec['priority']}] Recommendation #{i}\")\n",
    "    print(f\"Category: {rec['category']}\")\n",
    "    print(f\"Issue: {rec['issue']}\")\n",
    "    print(f\"Action: {rec['recommendation']}\")\n",
    "    print(f\"Expected Impact: {rec['expected_impact']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save error analysis report\n",
    "report = {\n",
    "    'error_summary': analyzer.get_error_summary(),\n",
    "    'failure_modes': taxonomy.get_summary(),\n",
    "    'top_confusion_patterns': {\n",
    "        head: patterns[:5] for head, patterns in confusion_patterns.items()\n",
    "    },\n",
    "    'position_bias': dict(position_bias),\n",
    "    'recommendations': recommendations\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "report_path = project_root / 'reports' / 'error_analysis_report.json'\n",
    "report_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Convert defaultdicts to regular dicts for JSON serialization\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, defaultdict):\n",
    "        return dict(obj)\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, (list, tuple)):\n",
    "        return [convert_to_serializable(item) for item in obj]\n",
    "    elif isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    return obj\n",
    "\n",
    "with open(report_path, 'w') as f:\n",
    "    json.dump(convert_to_serializable(report), f, indent=2)\n",
    "\n",
    "print(f\"\\nError analysis report saved to: {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook provides comprehensive error analysis:\n",
    "\n",
    "1. **Error Pattern Analysis**: Systematic breakdown of errors by head and confidence\n",
    "2. **Hard Example Mining**: Identifies the most challenging cases for the model\n",
    "3. **Failure Mode Taxonomy**: Categorizes errors into actionable failure types\n",
    "4. **Per-Operation Analysis**: Shows which G-code operations are most error-prone\n",
    "5. **Confusion Mining**: Reveals systematic confusion patterns between classes\n",
    "6. **Dataset Bias**: Identifies position and class-frequency related biases\n",
    "7. **Recommendations**: Generates prioritized, actionable improvement suggestions\n",
    "\n",
    "---\n",
    "\n",
    "**Navigation:**\n",
    "← [Previous: 11_model_interpretability](11_model_interpretability.ipynb) |\n",
    "[Next: 13_deployment_guide](13_deployment_guide.ipynb) →"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
