{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. Robustness Testing\n",
    "\n",
    "Evaluate model robustness against noise, sensor failures, and adversarial perturbations.\n",
    "\n",
    "## Contents\n",
    "1. [Setup](#1-setup)\n",
    "2. [Gaussian Noise Robustness](#2-gaussian-noise-robustness)\n",
    "3. [Sensor Dropout Testing](#3-sensor-dropout-testing)\n",
    "4. [Temporal Perturbations](#4-temporal-perturbations)\n",
    "5. [Adversarial Robustness](#5-adversarial-robustness)\n",
    "6. [Out-of-Distribution Detection](#6-out-of-distribution-detection)\n",
    "7. [Robustness Report](#7-robustness-report)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root / 'src'))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple, Optional, Callable\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Environment check\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Device: {'MPS' if torch.backends.mps.is_available() else 'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and data\n",
    "from miracle.model.backbone import MMDTAELSTMBackbone\n",
    "from miracle.model.multihead_lm import MultiHeadGCodeLM\n",
    "\n",
    "# Paths\n",
    "VOCAB_PATH = project_root / 'data' / 'gcode_vocab_v2.json'\n",
    "CHECKPOINT_PATH = project_root / 'outputs' / 'final_model' / 'checkpoint_best.pt'\n",
    "DATA_DIR = project_root / 'outputs' / 'processed_v2'\n",
    "\n",
    "# Load vocabulary\n",
    "with open(VOCAB_PATH) as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "# Load checkpoint\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    checkpoint = torch.load(CHECKPOINT_PATH, map_location=device, weights_only=False)\n",
    "    config = checkpoint.get('config', {})\n",
    "else:\n",
    "    config = {'hidden_dim': 256, 'num_layers': 4, 'num_heads': 8}\n",
    "\n",
    "# Initialize models\n",
    "backbone = MMDTAELSTMBackbone(\n",
    "    continuous_dim=155,\n",
    "    categorical_dims=[10, 10, 50, 50],\n",
    "    d_model=config.get('hidden_dim', 256),\n",
    "    num_layers=config.get('num_layers', 4),\n",
    "    num_heads=config.get('num_heads', 8),\n",
    "    dropout=0.0\n",
    ").to(device)\n",
    "\n",
    "lm = MultiHeadGCodeLM(\n",
    "    d_model=config.get('hidden_dim', 256),\n",
    "    vocab_sizes=vocab.get('head_vocab_sizes', {'type': 10, 'command': 50, 'param_type': 30, 'param_value': 100})\n",
    ").to(device)\n",
    "\n",
    "# Load weights if available\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    backbone.load_state_dict(checkpoint['backbone_state_dict'])\n",
    "    lm.load_state_dict(checkpoint['lm_state_dict'])\n",
    "\n",
    "backbone.eval()\n",
    "lm.eval()\n",
    "print(f\"Models loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_path = DATA_DIR / 'test.pt'\n",
    "\n",
    "if test_path.exists():\n",
    "    test_data = torch.load(test_path, weights_only=False)\n",
    "    continuous_data = torch.tensor(test_data['continuous'], dtype=torch.float32)\n",
    "    categorical_data = torch.tensor(test_data['categorical'], dtype=torch.long)\n",
    "    print(f\"Test data: {continuous_data.shape}\")\n",
    "else:\n",
    "    # Create synthetic test data\n",
    "    print(\"Using synthetic test data\")\n",
    "    continuous_data = torch.randn(100, 64, 155)\n",
    "    categorical_data = torch.randint(0, 10, (100, 64, 4))\n",
    "\n",
    "# Use subset for testing\n",
    "N_SAMPLES = min(50, len(continuous_data))\n",
    "test_continuous = continuous_data[:N_SAMPLES].to(device)\n",
    "test_categorical = categorical_data[:N_SAMPLES].to(device)\n",
    "print(f\"Using {N_SAMPLES} samples for robustness testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def compute_accuracy(backbone, lm, continuous, categorical, head='command'):\n",
    "    \"\"\"Compute accuracy for a given head.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        hidden = backbone(continuous, categorical)\n",
    "        preds = lm(hidden)\n",
    "        # For robustness testing, we compare against clean predictions\n",
    "        return preds[head].argmax(dim=-1)\n",
    "\n",
    "def agreement_rate(pred1, pred2):\n",
    "    \"\"\"Compute agreement rate between two prediction sets.\"\"\"\n",
    "    return (pred1 == pred2).float().mean().item()\n",
    "\n",
    "# Get baseline predictions\n",
    "baseline_preds = {}\n",
    "for head in ['type', 'command', 'param_type', 'param_value']:\n",
    "    baseline_preds[head] = compute_accuracy(backbone, lm, test_continuous, test_categorical, head)\n",
    "\n",
    "print(\"Baseline predictions computed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gaussian Noise Robustness\n",
    "\n",
    "Test model robustness to Gaussian noise at various intensities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gaussian_noise(data, std):\n",
    "    \"\"\"Add Gaussian noise with given standard deviation.\"\"\"\n",
    "    noise = torch.randn_like(data) * std\n",
    "    return data + noise\n",
    "\n",
    "def test_noise_robustness(backbone, lm, continuous, categorical, baseline_preds, \n",
    "                          noise_levels=[0.01, 0.05, 0.1, 0.2, 0.5, 1.0]):\n",
    "    \"\"\"Test robustness across noise levels.\"\"\"\n",
    "    results = defaultdict(list)\n",
    "    \n",
    "    for std in tqdm(noise_levels, desc=\"Testing noise levels\"):\n",
    "        noisy_continuous = add_gaussian_noise(continuous, std)\n",
    "        \n",
    "        for head in ['type', 'command', 'param_type', 'param_value']:\n",
    "            noisy_preds = compute_accuracy(backbone, lm, noisy_continuous, categorical, head)\n",
    "            agreement = agreement_rate(baseline_preds[head], noisy_preds)\n",
    "            results[head].append(agreement)\n",
    "    \n",
    "    return results, noise_levels\n",
    "\n",
    "# Run noise robustness tests\n",
    "noise_levels = [0.0, 0.01, 0.05, 0.1, 0.2, 0.5, 1.0, 2.0]\n",
    "noise_results, noise_levels = test_noise_robustness(\n",
    "    backbone, lm, test_continuous, test_categorical, baseline_preds, noise_levels\n",
    ")\n",
    "\n",
    "print(\"\\nNoise Robustness Results:\")\n",
    "print(f\"{'Noise Std':<12}\", end=\"\")\n",
    "for head in ['type', 'command', 'param_type', 'param_value']:\n",
    "    print(f\"{head:<15}\", end=\"\")\n",
    "print()\n",
    "print(\"-\" * 72)\n",
    "\n",
    "for i, std in enumerate(noise_levels):\n",
    "    print(f\"{std:<12.2f}\", end=\"\")\n",
    "    for head in ['type', 'command', 'param_type', 'param_value']:\n",
    "        print(f\"{noise_results[head][i]:.4f}         \", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize noise robustness\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "colors = ['steelblue', 'coral', 'forestgreen', 'purple']\n",
    "for head, color in zip(['type', 'command', 'param_type', 'param_value'], colors):\n",
    "    ax.plot(noise_levels, noise_results[head], 'o-', label=head.upper(), color=color, linewidth=2, markersize=8)\n",
    "\n",
    "ax.axhline(y=0.9, color='gray', linestyle='--', alpha=0.5, label='90% threshold')\n",
    "ax.set_xlabel('Noise Standard Deviation')\n",
    "ax.set_ylabel('Agreement with Clean Predictions')\n",
    "ax.set_title('Model Robustness to Gaussian Noise')\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(project_root / 'reports' / 'noise_robustness.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sensor Dropout Testing\n",
    "\n",
    "Simulate sensor failures by dropping individual sensor channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_sensor_dropout(data, dropout_rate, channel_mask=None):\n",
    "    \"\"\"Apply dropout to sensor channels.\"\"\"\n",
    "    if channel_mask is None:\n",
    "        # Random dropout\n",
    "        mask = torch.rand(data.shape[-1], device=data.device) > dropout_rate\n",
    "    else:\n",
    "        mask = channel_mask\n",
    "    \n",
    "    return data * mask.float()\n",
    "\n",
    "def test_dropout_robustness(backbone, lm, continuous, categorical, baseline_preds,\n",
    "                            dropout_rates=[0.1, 0.2, 0.3, 0.5, 0.7, 0.9]):\n",
    "    \"\"\"Test robustness to sensor dropout.\"\"\"\n",
    "    results = defaultdict(list)\n",
    "    \n",
    "    for rate in tqdm(dropout_rates, desc=\"Testing dropout rates\"):\n",
    "        # Average over multiple random masks\n",
    "        head_agreements = defaultdict(list)\n",
    "        \n",
    "        for _ in range(5):  # 5 random masks per rate\n",
    "            dropped_continuous = apply_sensor_dropout(continuous, rate)\n",
    "            \n",
    "            for head in ['type', 'command', 'param_type', 'param_value']:\n",
    "                dropped_preds = compute_accuracy(backbone, lm, dropped_continuous, categorical, head)\n",
    "                agreement = agreement_rate(baseline_preds[head], dropped_preds)\n",
    "                head_agreements[head].append(agreement)\n",
    "        \n",
    "        for head in ['type', 'command', 'param_type', 'param_value']:\n",
    "            results[head].append(np.mean(head_agreements[head]))\n",
    "    \n",
    "    return results, dropout_rates\n",
    "\n",
    "# Run dropout tests\n",
    "dropout_rates = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "dropout_results, dropout_rates = test_dropout_robustness(\n",
    "    backbone, lm, test_continuous, test_categorical, baseline_preds, dropout_rates\n",
    ")\n",
    "\n",
    "print(\"\\nSensor Dropout Robustness:\")\n",
    "for i, rate in enumerate(dropout_rates):\n",
    "    print(f\"Dropout {rate:.1%}: \", end=\"\")\n",
    "    for head in ['type', 'command']:\n",
    "        print(f\"{head}={dropout_results[head][i]:.3f} \", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test specific sensor group failures\n",
    "def test_sensor_group_failure(backbone, lm, continuous, categorical, baseline_preds):\n",
    "    \"\"\"Test failure of specific sensor groups.\"\"\"\n",
    "    # Define sensor groups (hypothetical groupings)\n",
    "    n_features = continuous.shape[-1]\n",
    "    sensor_groups = {\n",
    "        'position': slice(0, 30),\n",
    "        'velocity': slice(30, 60),\n",
    "        'acceleration': slice(60, 90),\n",
    "        'force': slice(90, 120),\n",
    "        'misc': slice(120, n_features)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for group_name, group_slice in sensor_groups.items():\n",
    "        # Zero out the sensor group\n",
    "        masked_continuous = continuous.clone()\n",
    "        masked_continuous[:, :, group_slice] = 0\n",
    "        \n",
    "        group_results = {}\n",
    "        for head in ['type', 'command', 'param_type', 'param_value']:\n",
    "            masked_preds = compute_accuracy(backbone, lm, masked_continuous, categorical, head)\n",
    "            agreement = agreement_rate(baseline_preds[head], masked_preds)\n",
    "            group_results[head] = agreement\n",
    "        \n",
    "        results[group_name] = group_results\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run sensor group tests\n",
    "group_results = test_sensor_group_failure(\n",
    "    backbone, lm, test_continuous, test_categorical, baseline_preds\n",
    ")\n",
    "\n",
    "print(\"\\nSensor Group Failure Impact:\")\n",
    "print(\"=\"*60)\n",
    "for group, results in group_results.items():\n",
    "    print(f\"\\n{group.upper()} sensors disabled:\")\n",
    "    for head, agreement in results.items():\n",
    "        degradation = (1 - agreement) * 100\n",
    "        print(f\"  {head}: {agreement:.3f} ({degradation:.1f}% degradation)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sensor group impact\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "groups = list(group_results.keys())\n",
    "x = np.arange(len(groups))\n",
    "width = 0.2\n",
    "\n",
    "for i, head in enumerate(['type', 'command', 'param_type', 'param_value']):\n",
    "    values = [group_results[g][head] for g in groups]\n",
    "    ax.bar(x + i * width, values, width, label=head.upper())\n",
    "\n",
    "ax.axhline(y=0.9, color='red', linestyle='--', alpha=0.5, label='90% threshold')\n",
    "ax.set_xlabel('Disabled Sensor Group')\n",
    "ax.set_ylabel('Agreement with Baseline')\n",
    "ax.set_title('Impact of Sensor Group Failures')\n",
    "ax.set_xticks(x + width * 1.5)\n",
    "ax.set_xticklabels([g.upper() for g in groups])\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(project_root / 'reports' / 'sensor_group_robustness.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Temporal Perturbations\n",
    "\n",
    "Test robustness to time-domain perturbations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_time_jitter(data, jitter_std=0.1):\n",
    "    \"\"\"Apply random time jittering to sequence.\"\"\"\n",
    "    B, T, C = data.shape\n",
    "    # Create jittered indices\n",
    "    jitter = torch.randn(B, T, device=data.device) * jitter_std\n",
    "    indices = torch.arange(T, device=data.device).float().unsqueeze(0) + jitter\n",
    "    indices = indices.clamp(0, T - 1).long()\n",
    "    \n",
    "    # Gather with jittered indices\n",
    "    jittered = torch.gather(data, 1, indices.unsqueeze(-1).expand(-1, -1, C))\n",
    "    return jittered\n",
    "\n",
    "def apply_time_warping(data, warp_factor=0.2):\n",
    "    \"\"\"Apply non-linear time warping.\"\"\"\n",
    "    B, T, C = data.shape\n",
    "    \n",
    "    # Create warped time indices\n",
    "    t = torch.linspace(0, 1, T, device=data.device)\n",
    "    # Random warp anchors\n",
    "    warp = torch.randn(B, 3, device=data.device) * warp_factor\n",
    "    \n",
    "    # Simple cubic warping\n",
    "    warped_t = t.unsqueeze(0) + warp[:, 0:1] * t * (1 - t) + warp[:, 1:2] * t**2 * (1 - t)\n",
    "    warped_t = warped_t.clamp(0, 1)\n",
    "    indices = (warped_t * (T - 1)).long()\n",
    "    \n",
    "    warped = torch.gather(data, 1, indices.unsqueeze(-1).expand(-1, -1, C))\n",
    "    return warped\n",
    "\n",
    "def apply_temporal_dropout(data, drop_rate=0.1):\n",
    "    \"\"\"Drop random time steps.\"\"\"\n",
    "    B, T, C = data.shape\n",
    "    mask = torch.rand(B, T, 1, device=data.device) > drop_rate\n",
    "    return data * mask.float()\n",
    "\n",
    "# Test temporal perturbations\n",
    "temporal_tests = {\n",
    "    'time_jitter_0.05': lambda x: apply_time_jitter(x, 0.05),\n",
    "    'time_jitter_0.1': lambda x: apply_time_jitter(x, 0.1),\n",
    "    'time_jitter_0.2': lambda x: apply_time_jitter(x, 0.2),\n",
    "    'time_warp_0.1': lambda x: apply_time_warping(x, 0.1),\n",
    "    'time_warp_0.2': lambda x: apply_time_warping(x, 0.2),\n",
    "    'temporal_dropout_0.1': lambda x: apply_temporal_dropout(x, 0.1),\n",
    "    'temporal_dropout_0.2': lambda x: apply_temporal_dropout(x, 0.2),\n",
    "}\n",
    "\n",
    "temporal_results = {}\n",
    "for name, transform in tqdm(temporal_tests.items(), desc=\"Testing temporal perturbations\"):\n",
    "    transformed = transform(test_continuous)\n",
    "    temporal_results[name] = {}\n",
    "    for head in ['type', 'command', 'param_type', 'param_value']:\n",
    "        preds = compute_accuracy(backbone, lm, transformed, test_categorical, head)\n",
    "        agreement = agreement_rate(baseline_preds[head], preds)\n",
    "        temporal_results[name][head] = agreement\n",
    "\n",
    "print(\"\\nTemporal Perturbation Results:\")\n",
    "print(\"=\"*70)\n",
    "for name, results in temporal_results.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    for head, agreement in results.items():\n",
    "        print(f\"  {head}: {agreement:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize temporal robustness\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "perturbations = list(temporal_results.keys())\n",
    "x = np.arange(len(perturbations))\n",
    "width = 0.2\n",
    "\n",
    "for i, head in enumerate(['type', 'command', 'param_type', 'param_value']):\n",
    "    values = [temporal_results[p][head] for p in perturbations]\n",
    "    ax.bar(x + i * width, values, width, label=head.upper())\n",
    "\n",
    "ax.axhline(y=0.9, color='red', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('Perturbation Type')\n",
    "ax.set_ylabel('Agreement with Baseline')\n",
    "ax.set_title('Robustness to Temporal Perturbations')\n",
    "ax.set_xticks(x + width * 1.5)\n",
    "ax.set_xticklabels(perturbations, rotation=30, ha='right')\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(project_root / 'reports' / 'temporal_robustness.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Adversarial Robustness\n",
    "\n",
    "Test robustness to adversarial perturbations using FGSM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_attack(backbone, lm, continuous, categorical, epsilon=0.1, head='command'):\n",
    "    \"\"\"Fast Gradient Sign Method attack.\"\"\"\n",
    "    continuous = continuous.clone().requires_grad_(True)\n",
    "    \n",
    "    # Forward pass\n",
    "    hidden = backbone(continuous, categorical)\n",
    "    preds = lm(hidden)\n",
    "    \n",
    "    # Get predicted class\n",
    "    pred_class = preds[head].argmax(dim=-1)\n",
    "    \n",
    "    # Compute loss (maximize cross-entropy to flip predictions)\n",
    "    # We use the predicted class as target to maximize confidence\n",
    "    B, T, V = preds[head].shape\n",
    "    loss = F.cross_entropy(\n",
    "        preds[head].view(-1, V),\n",
    "        pred_class.view(-1)\n",
    "    )\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Generate adversarial example\n",
    "    perturbation = epsilon * continuous.grad.sign()\n",
    "    adversarial = continuous + perturbation\n",
    "    \n",
    "    return adversarial.detach()\n",
    "\n",
    "def test_adversarial_robustness(backbone, lm, continuous, categorical, baseline_preds,\n",
    "                                 epsilons=[0.01, 0.05, 0.1, 0.2, 0.5]):\n",
    "    \"\"\"Test robustness to adversarial attacks.\"\"\"\n",
    "    results = defaultdict(list)\n",
    "    \n",
    "    for eps in tqdm(epsilons, desc=\"Testing adversarial epsilons\"):\n",
    "        # Attack targeting command head\n",
    "        adversarial = fgsm_attack(backbone, lm, continuous, categorical, eps, 'command')\n",
    "        \n",
    "        for head in ['type', 'command', 'param_type', 'param_value']:\n",
    "            adv_preds = compute_accuracy(backbone, lm, adversarial, categorical, head)\n",
    "            agreement = agreement_rate(baseline_preds[head], adv_preds)\n",
    "            results[head].append(agreement)\n",
    "    \n",
    "    return results, epsilons\n",
    "\n",
    "# Run adversarial tests\n",
    "epsilons = [0.0, 0.01, 0.05, 0.1, 0.2, 0.3, 0.5]\n",
    "adv_results, epsilons = test_adversarial_robustness(\n",
    "    backbone, lm, test_continuous, test_categorical, baseline_preds, epsilons\n",
    ")\n",
    "\n",
    "print(\"\\nAdversarial Robustness (FGSM):\")\n",
    "print(\"=\"*60)\n",
    "for i, eps in enumerate(epsilons):\n",
    "    print(f\"Epsilon {eps:.2f}: \", end=\"\")\n",
    "    for head in ['type', 'command']:\n",
    "        print(f\"{head}={adv_results[head][i]:.3f} \", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize adversarial robustness\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Agreement vs epsilon\n",
    "for head in ['type', 'command', 'param_type', 'param_value']:\n",
    "    axes[0].plot(epsilons, adv_results[head], 'o-', label=head.upper(), linewidth=2)\n",
    "\n",
    "axes[0].axhline(y=0.9, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[0].set_xlabel('Epsilon (Perturbation Magnitude)')\n",
    "axes[0].set_ylabel('Agreement with Clean Predictions')\n",
    "axes[0].set_title('FGSM Adversarial Robustness')\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim(0, 1.05)\n",
    "\n",
    "# Compare Gaussian noise vs adversarial\n",
    "# Use comparable perturbation magnitudes\n",
    "axes[1].plot(noise_levels[:len(noise_results['command'])], noise_results['command'], \n",
    "            'o-', label='Gaussian Noise', linewidth=2)\n",
    "axes[1].plot(epsilons[:len(adv_results['command'])], adv_results['command'], \n",
    "            's--', label='FGSM Attack', linewidth=2)\n",
    "axes[1].set_xlabel('Perturbation Magnitude')\n",
    "axes[1].set_ylabel('Agreement (Command Head)')\n",
    "axes[1].set_title('Gaussian Noise vs Adversarial Perturbations')\n",
    "axes[1].legend()\n",
    "axes[1].set_ylim(0, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(project_root / 'reports' / 'adversarial_robustness.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Out-of-Distribution Detection\n",
    "\n",
    "Detect and handle out-of-distribution inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confidence_stats(backbone, lm, continuous, categorical):\n",
    "    \"\"\"Compute prediction confidence statistics.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        hidden = backbone(continuous, categorical)\n",
    "        preds = lm(hidden)\n",
    "        \n",
    "        stats = {}\n",
    "        for head in ['type', 'command', 'param_type', 'param_value']:\n",
    "            probs = F.softmax(preds[head], dim=-1)\n",
    "            max_probs = probs.max(dim=-1)[0]\n",
    "            entropy = -(probs * torch.log(probs + 1e-10)).sum(dim=-1)\n",
    "            \n",
    "            stats[head] = {\n",
    "                'mean_confidence': max_probs.mean().item(),\n",
    "                'min_confidence': max_probs.min().item(),\n",
    "                'mean_entropy': entropy.mean().item(),\n",
    "                'max_entropy': entropy.max().item()\n",
    "            }\n",
    "        \n",
    "        return stats\n",
    "\n",
    "# Compute baseline confidence\n",
    "baseline_confidence = compute_confidence_stats(backbone, lm, test_continuous, test_categorical)\n",
    "\n",
    "print(\"Baseline Confidence Statistics:\")\n",
    "for head, stats in baseline_confidence.items():\n",
    "    print(f\"\\n{head.upper()}:\")\n",
    "    for key, value in stats.items():\n",
    "        print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate OOD samples\n",
    "def generate_ood_samples(in_dist_data, method='gaussian'):\n",
    "    \"\"\"Generate out-of-distribution samples.\"\"\"\n",
    "    B, T, C = in_dist_data.shape\n",
    "    \n",
    "    if method == 'gaussian':\n",
    "        # Pure Gaussian noise\n",
    "        return torch.randn_like(in_dist_data) * in_dist_data.std()\n",
    "    \n",
    "    elif method == 'uniform':\n",
    "        # Uniform random\n",
    "        return torch.rand_like(in_dist_data) * 2 - 1\n",
    "    \n",
    "    elif method == 'extreme':\n",
    "        # Extreme values\n",
    "        return torch.randn_like(in_dist_data) * in_dist_data.std() * 10\n",
    "    \n",
    "    elif method == 'constant':\n",
    "        # Constant input\n",
    "        return torch.zeros_like(in_dist_data)\n",
    "    \n",
    "    elif method == 'shuffled':\n",
    "        # Shuffled feature dimensions\n",
    "        perm = torch.randperm(C)\n",
    "        return in_dist_data[:, :, perm]\n",
    "\n",
    "# Test OOD detection\n",
    "ood_methods = ['gaussian', 'uniform', 'extreme', 'constant', 'shuffled']\n",
    "ood_results = {}\n",
    "\n",
    "for method in ood_methods:\n",
    "    ood_data = generate_ood_samples(test_continuous, method)\n",
    "    ood_stats = compute_confidence_stats(backbone, lm, ood_data, test_categorical)\n",
    "    ood_results[method] = ood_stats\n",
    "\n",
    "print(\"\\nOOD Detection Results:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Method':<15} {'Command Conf':<15} {'Command Entropy':<18} {'Detected?'}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for method, stats in ood_results.items():\n",
    "    cmd_conf = stats['command']['mean_confidence']\n",
    "    cmd_ent = stats['command']['mean_entropy']\n",
    "    baseline_conf = baseline_confidence['command']['mean_confidence']\n",
    "    detected = cmd_conf < baseline_conf * 0.9 or cmd_ent > baseline_confidence['command']['mean_entropy'] * 1.5\n",
    "    print(f\"{method:<15} {cmd_conf:<15.4f} {cmd_ent:<18.4f} {'Yes' if detected else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize OOD detection\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Confidence comparison\n",
    "methods = ['baseline'] + ood_methods\n",
    "confidences = [baseline_confidence['command']['mean_confidence']]\n",
    "confidences += [ood_results[m]['command']['mean_confidence'] for m in ood_methods]\n",
    "\n",
    "colors = ['forestgreen'] + ['coral'] * len(ood_methods)\n",
    "bars = axes[0].bar(methods, confidences, color=colors)\n",
    "axes[0].axhline(y=baseline_confidence['command']['mean_confidence'] * 0.9, \n",
    "                color='red', linestyle='--', label='Detection threshold')\n",
    "axes[0].set_xlabel('Data Type')\n",
    "axes[0].set_ylabel('Mean Confidence')\n",
    "axes[0].set_title('Confidence: In-Distribution vs OOD')\n",
    "axes[0].tick_params(axis='x', rotation=30)\n",
    "axes[0].legend()\n",
    "\n",
    "# Entropy comparison\n",
    "entropies = [baseline_confidence['command']['mean_entropy']]\n",
    "entropies += [ood_results[m]['command']['mean_entropy'] for m in ood_methods]\n",
    "\n",
    "bars = axes[1].bar(methods, entropies, color=colors)\n",
    "axes[1].axhline(y=baseline_confidence['command']['mean_entropy'] * 1.5,\n",
    "                color='red', linestyle='--', label='Detection threshold')\n",
    "axes[1].set_xlabel('Data Type')\n",
    "axes[1].set_ylabel('Mean Entropy')\n",
    "axes[1].set_title('Entropy: In-Distribution vs OOD')\n",
    "axes[1].tick_params(axis='x', rotation=30)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(project_root / 'reports' / 'ood_detection.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Robustness Report\n",
    "\n",
    "Generate comprehensive robustness report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_robustness_report():\n",
    "    \"\"\"Generate comprehensive robustness report.\"\"\"\n",
    "    report = {\n",
    "        'summary': {\n",
    "            'num_test_samples': N_SAMPLES,\n",
    "            'device': str(device),\n",
    "        },\n",
    "        'noise_robustness': {\n",
    "            'noise_levels': noise_levels,\n",
    "            'results': {head: list(vals) for head, vals in noise_results.items()},\n",
    "            'critical_noise_level': None  # Noise level where agreement drops below 90%\n",
    "        },\n",
    "        'sensor_dropout': {\n",
    "            'dropout_rates': dropout_rates,\n",
    "            'results': {head: list(vals) for head, vals in dropout_results.items()},\n",
    "            'critical_dropout_rate': None\n",
    "        },\n",
    "        'sensor_group_failures': group_results,\n",
    "        'temporal_perturbations': temporal_results,\n",
    "        'adversarial_robustness': {\n",
    "            'epsilons': epsilons,\n",
    "            'results': {head: list(vals) for head, vals in adv_results.items()}\n",
    "        },\n",
    "        'ood_detection': {\n",
    "            'baseline': baseline_confidence,\n",
    "            'ood_results': ood_results\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Find critical thresholds\n",
    "    for i, level in enumerate(noise_levels):\n",
    "        if noise_results['command'][i] < 0.9:\n",
    "            report['noise_robustness']['critical_noise_level'] = noise_levels[i-1] if i > 0 else 0\n",
    "            break\n",
    "    \n",
    "    for i, rate in enumerate(dropout_rates):\n",
    "        if dropout_results['command'][i] < 0.9:\n",
    "            report['sensor_dropout']['critical_dropout_rate'] = dropout_rates[i-1] if i > 0 else 0\n",
    "            break\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Generate and save report\n",
    "robustness_report = generate_robustness_report()\n",
    "\n",
    "# Save to JSON\n",
    "report_path = project_root / 'reports' / 'robustness_report.json'\n",
    "report_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(report_path, 'w') as f:\n",
    "    json.dump(robustness_report, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Robustness report saved to: {report_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ROBUSTNESS TEST SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. GAUSSIAN NOISE:\")\n",
    "critical = robustness_report['noise_robustness']['critical_noise_level']\n",
    "print(f\"   Critical noise level (90% threshold): {critical if critical else 'Not reached'}\")\n",
    "print(f\"   Noise robustness rating: {'HIGH' if critical is None or critical > 0.5 else 'MEDIUM' if critical > 0.1 else 'LOW'}\")\n",
    "\n",
    "print(\"\\n2. SENSOR DROPOUT:\")\n",
    "critical = robustness_report['sensor_dropout']['critical_dropout_rate']\n",
    "print(f\"   Critical dropout rate (90% threshold): {critical if critical else 'Not reached'}\")\n",
    "print(f\"   Dropout robustness rating: {'HIGH' if critical is None or critical > 0.5 else 'MEDIUM' if critical > 0.2 else 'LOW'}\")\n",
    "\n",
    "print(\"\\n3. SENSOR GROUP FAILURES:\")\n",
    "worst_group = min(group_results.items(), key=lambda x: x[1]['command'])\n",
    "print(f\"   Most critical sensor group: {worst_group[0].upper()}\")\n",
    "print(f\"   Impact when disabled: {(1-worst_group[1]['command'])*100:.1f}% degradation\")\n",
    "\n",
    "print(\"\\n4. TEMPORAL PERTURBATIONS:\")\n",
    "worst_temporal = min(temporal_results.items(), key=lambda x: x[1]['command'])\n",
    "print(f\"   Most impactful perturbation: {worst_temporal[0]}\")\n",
    "print(f\"   Agreement: {worst_temporal[1]['command']:.3f}\")\n",
    "\n",
    "print(\"\\n5. ADVERSARIAL ROBUSTNESS:\")\n",
    "for i, eps in enumerate(epsilons):\n",
    "    if adv_results['command'][i] < 0.9:\n",
    "        print(f\"   Vulnerable at epsilon: {eps}\")\n",
    "        break\n",
    "else:\n",
    "    print(f\"   Robust up to epsilon: {epsilons[-1]}\")\n",
    "\n",
    "print(\"\\n6. OOD DETECTION:\")\n",
    "detected_count = sum(1 for m in ood_methods \n",
    "                     if ood_results[m]['command']['mean_confidence'] < \n",
    "                        baseline_confidence['command']['mean_confidence'] * 0.9)\n",
    "print(f\"   OOD types detected: {detected_count}/{len(ood_methods)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook provides comprehensive robustness testing:\n",
    "\n",
    "1. **Gaussian Noise**: Tests degradation under various noise levels\n",
    "2. **Sensor Dropout**: Simulates sensor failures and channel dropping\n",
    "3. **Temporal Perturbations**: Tests time jitter, warping, and temporal dropout\n",
    "4. **Adversarial Robustness**: FGSM attacks to find model vulnerabilities\n",
    "5. **OOD Detection**: Identifies out-of-distribution inputs using confidence/entropy\n",
    "6. **Comprehensive Report**: Actionable summary with critical thresholds\n",
    "\n",
    "---\n",
    "\n",
    "**Navigation:**\n",
    "← [Previous: 13_deployment_guide](13_deployment_guide.ipynb) |\n",
    "[Next: 15_data_augmentation](15_data_augmentation.ipynb) →"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
