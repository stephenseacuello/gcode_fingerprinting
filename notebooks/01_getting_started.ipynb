{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Getting Started with G-code Fingerprinting\n",
    "\n",
    "Welcome! This notebook provides a complete overview of the G-code fingerprinting project.\n",
    "\n",
    "## What is G-code Fingerprinting?\n",
    "\n",
    "This project uses deep learning to:\n",
    "1. **Predict G-code sequences** from CNC machine sensor data\n",
    "2. **Extract machine fingerprints** - unique embeddings that identify specific machines\n",
    "3. **Enable reverse engineering** of machine operations from sensor readings\n",
    "\n",
    "## Project Architecture\n",
    "\n",
    "```\n",
    "Sensor Data → MM-DTAE-LSTM Backbone → Embeddings → Multi-Head LM → G-code Predictions\n",
    "                                              ↓\n",
    "                                    Machine Fingerprint\n",
    "```\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the project structure\n",
    "- Verify environment setup\n",
    "- Run a complete inference example\n",
    "- Explore the token decomposition system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root / 'src'))\n",
    "\n",
    "print(f\"✓ Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Project Structure\n",
    "\n",
    "```\n",
    "gcode_fingerprinting/\n",
    "├── data/                      # Raw G-code and vocabulary files\n",
    "├── src/miracle/               # Core package\n",
    "│   ├── model/                 # Model architectures\n",
    "│   │   ├── model.py          # MM-DTAE-LSTM backbone\n",
    "│   │   └── multihead_lm.py   # Multi-head language model\n",
    "│   ├── dataset/               # Data processing\n",
    "│   │   ├── preprocessing.py  # Data preparation\n",
    "│   │   └── target_utils.py   # Token decomposition\n",
    "│   └── api/                   # FastAPI server\n",
    "├── scripts/                   # Training and evaluation\n",
    "├── configs/                   # Configuration files\n",
    "├── outputs/                   # Model checkpoints and results\n",
    "└── notebooks/                 # Tutorial notebooks (you are here!)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify key directories exist\n",
    "import os\n",
    "\n",
    "key_dirs = ['data', 'src', 'scripts', 'configs', 'outputs']\n",
    "print(\"Directory check:\")\n",
    "for d in key_dirs:\n",
    "    path = project_root / d\n",
    "    exists = \"✓\" if path.exists() else \"✗\"\n",
    "    print(f\"  {exists} {d}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Token Decomposition\n",
    "\n",
    "G-code tokens are hierarchically decomposed into:\n",
    "- **Commands**: G0, G1, M3, etc.\n",
    "- **Parameter Types**: X, Y, Z, F, etc.\n",
    "- **Parameter Values**: Numeric values\n",
    "\n",
    "This allows the model to learn structure and relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and create TokenDecomposer\n",
    "from miracle.dataset.target_utils import TokenDecomposer\n",
    "\n",
    "vocab_path = project_root / 'data' / 'vocabulary.json'\n",
    "\n",
    "if vocab_path.exists():\n",
    "    decomposer = TokenDecomposer(str(vocab_path))\n",
    "    \n",
    "    print(f\"Vocabulary size: {decomposer.vocab_size}\")\n",
    "    print(f\"Number of commands: {decomposer.n_commands}\")\n",
    "    print(f\"Number of parameter types: {decomposer.n_param_types}\")\n",
    "    print(f\"Number of parameter values: {decomposer.n_param_values}\")\nelse:\n",
    "    print(\"Vocabulary not found. Run preprocessing first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Decompose a sample sequence\n",
    "if vocab_path.exists():\n",
    "    # Sample G-code tokens\n",
    "    sample_tokens = ['G0', 'X10.5', 'Y20.3', 'G1', 'F1500']\n",
    "    \n",
    "    print(\"Sample G-code sequence:\")\n",
    "    print(f\"  {' '.join(sample_tokens)}\")\n",
    "    \n",
    "    # Convert tokens to IDs\n",
    "    token_ids = [decomposer.token_to_id.get(t, decomposer.token_to_id['<UNK>']) for t in sample_tokens]\n",
    "    \n",
    "    print(f\"\\nToken IDs: {token_ids}\")\n",
    "    \n",
    "    # Decompose\n",
    "    decomposed = decomposer.decompose_batch([token_ids])\n",
    "    \n",
    "    print(f\"\\nDecomposed structure:\")\n",
    "    print(f\"  Commands: {decomposed['commands'][0]}\")\n",
    "    print(f\"  Param types: {decomposed['param_types'][0]}\")\n",
    "    print(f\"  Param values: {decomposed['param_values'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Architecture Overview\n",
    "\n",
    "### MM-DTAE-LSTM Backbone\n",
    "- Processes multi-modal sensor data (continuous + categorical)\n",
    "- Uses LSTM for temporal modeling\n",
    "- Outputs embeddings for each timestep\n",
    "\n",
    "### Multi-Head Language Model\n",
    "- Three prediction heads:\n",
    "  1. **Command Head**: Predicts G/M commands\n",
    "  2. **Parameter Type Head**: Predicts X, Y, Z, F, etc.\n",
    "  3. **Parameter Value Head**: Predicts numeric values\n",
    "- Uses transformer architecture for autoregressive generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model configuration\n",
    "import json\n",
    "\n",
    "config_path = project_root / 'configs' / 'config.json'\n",
    "\n",
    "if config_path.exists():\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    print(\"Model Configuration:\")\n",
    "    print(f\"  Hidden dimension: {config.get('hidden_dim', 128)}\")\n",
    "    print(f\"  Number of layers: {config.get('num_layers', 2)}\")\n",
    "    print(f\"  Attention heads: {config.get('num_heads', 4)}\")\n",
    "    print(f\"  Dropout: {config.get('dropout', 0.1)}\")\n",
    "else:\n",
    "    print(\"Config file not found at\", config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quick Inference Demo\n",
    "\n",
    "Let's load a checkpoint and run inference on sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for available checkpoints\n",
    "import glob\n",
    "\n",
    "checkpoint_patterns = [\n",
    "    'outputs/*/checkpoint_best.pt',\n",
    "    'outputs/training_*/checkpoint_best.pt',\n",
    "]\n",
    "\n",
    "checkpoints = []\n",
    "for pattern in checkpoint_patterns:\n",
    "    checkpoints.extend(glob.glob(str(project_root / pattern)))\n",
    "\n",
    "if checkpoints:\n",
    "    print(f\"Found {len(checkpoints)} checkpoint(s):\")\n",
    "    for cp in checkpoints[:5]:\n",
    "        print(f\"  - {Path(cp).relative_to(project_root)}\")\n",
    "    \n",
    "    checkpoint_path = checkpoints[0]\n",
    "    print(f\"\\nUsing: {Path(checkpoint_path).relative_to(project_root)}\")\n",
    "else:\n",
    "    print(\"No checkpoints found. Train a model first.\")\n",
    "    checkpoint_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint and run inference\n",
    "if checkpoint_path and vocab_path.exists():\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    from miracle.model.model import MM_DTAE_LSTM, ModelConfig\n",
    "    from miracle.model.multihead_lm import MultiHeadGCodeLM\n",
    "    from miracle.utilities.device import get_device\n",
    "    \n",
    "    device = get_device()\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    config_dict = checkpoint.get('config', {})\n",
    "    \n",
    "    # Create models\n",
    "    backbone_config = ModelConfig(\n",
    "        sensor_dims=[155, 4],  # From preprocessed data\n",
    "        d_model=config_dict.get('hidden_dim', 128),\n",
    "        lstm_layers=config_dict.get('num_layers', 2),\n",
    "        gcode_vocab=decomposer.vocab_size,\n",
    "        n_heads=config_dict.get('num_heads', 4),\n",
    "    )\n",
    "    \n",
    "    backbone = MM_DTAE_LSTM(backbone_config).to(device)\n",
    "    backbone.load_state_dict(checkpoint['backbone_state_dict'])\n",
    "    backbone.eval()\n",
    "    \n",
    "    print(\"✓ Model loaded successfully!\")\n",
    "    print(f\"  Epoch: {checkpoint.get('epoch', 'unknown')}\")\n",
    "    print(f\"  Val Accuracy: {checkpoint.get('val_acc', 'unknown')}\")\n",
    "else:\n",
    "    print(\"Skipping inference demo - missing checkpoint or vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Environment Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Python version\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Check key packages\n",
    "packages_to_check = [\n",
    "    'torch',\n",
    "    'numpy',\n",
    "    'pandas',\n",
    "    'matplotlib',\n",
    "    'wandb',\n",
    "    'fastapi',\n",
    "]\n",
    "\n",
    "print(\"\\nPackage versions:\")\n",
    "for pkg in packages_to_check:\n",
    "    try:\n",
    "        module = __import__(pkg)\n",
    "        version = getattr(module, '__version__', 'unknown')\n",
    "        print(f\"  ✓ {pkg}: {version}\")\n",
    "    except ImportError:\n",
    "        print(f\"  ✗ {pkg}: NOT INSTALLED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Next Steps\n",
    "\n",
    "### Tutorial Sequence:\n",
    "1. ✓ **00_raw_data_analysis.ipynb** - Explore raw data\n",
    "2. ✓ **01_getting_started.ipynb** - You are here!\n",
    "3. **02_data_preprocessing.ipynb** - Prepare data for training\n",
    "4. **03_training_models.ipynb** - Train models\n",
    "5. **04_inference_prediction.ipynb** - Run predictions\n",
    "6. **05_api_usage.ipynb** - Use the FastAPI server\n",
    "7. **06_dashboard_usage.ipynb** - Interactive dashboard\n",
    "8. **07_hyperparameter_sweeps.ipynb** - Optimize with W&B\n",
    "9. **08_model_evaluation.ipynb** - Evaluate and compare models\n",
    "\n",
    "### Quick Commands:\n",
    "\n",
    "```bash\n",
    "# Preprocess data\n",
    "PYTHONPATH=src .venv/bin/python -m miracle.dataset.preprocessing \\\n",
    "    --data-dir data --output-dir outputs/processed --vocab-path data/vocabulary.json\n",
    "\n",
    "# Train a model\n",
    "PYTHONPATH=src .venv/bin/python scripts/train_multihead.py \\\n",
    "    --data-dir outputs/processed --vocab-path data/vocabulary.json \\\n",
    "    --output-dir outputs/training --max-epochs 10\n",
    "\n",
    "# Start API server\n",
    "PYTHONPATH=src .venv/bin/python src/miracle/api/server.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You now understand:\n",
    "- Project architecture and structure\n",
    "- Token decomposition system\n",
    "- Model components (backbone + multi-head LM)\n",
    "- How to verify your environment\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "- **Import errors**: Ensure virtual environment is activated and `PYTHONPATH` includes `src/`\n",
    "- **No checkpoints**: Train a model first using `scripts/train_multihead.py`\n",
    "- **Vocabulary missing**: Run preprocessing first\n",
    "- **CUDA errors**: Use CPU by setting `device='cpu'` or enable MPS fallback"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
