{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 04 - Inference & Prediction\n",
    "\n",
    "**Use trained models to predict G-code from sensor data.**\n",
    "\n",
    "## Learning Objectives\n",
    "- Load trained encoder and decoder models\n",
    "- Run inference on single samples\n",
    "- Perform batch inference with timing\n",
    "- Decode predictions to G-code strings\n",
    "- Analyze per-head predictions\n",
    "- Evaluate operation classification (100% accuracy)\n",
    "\n",
    "## Table of Contents\n",
    "1. [Load Models](#1.-Load-Models)\n",
    "2. [Single Sample Inference](#2.-Single-Sample-Inference)\n",
    "3. [Batch Inference](#3.-Batch-Inference)\n",
    "4. [Token Decoding](#4.-Token-Decoding)\n",
    "5. [Per-Head Analysis](#5.-Per-Head-Analysis)\n",
    "6. [Performance Benchmarks](#6.-Performance-Benchmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root / 'src'))\n",
    "\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Load Models\n",
    "\n",
    "Load the frozen encoder (MM-DTAE-LSTM v2) and decoder (SensorMultiHeadDecoder v3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "encoder_path = project_root / 'outputs' / 'mm_dtae_lstm_v2' / 'best_model.pt'\n",
    "decoder_path = project_root / 'outputs' / 'sensor_multihead_v3' / 'best_model.pt'\n",
    "vocab_path = project_root / 'data' / 'vocabulary_4digit_hybrid.json'\n",
    "split_dir = project_root / 'outputs' / 'stratified_splits_v2'\n",
    "\n",
    "print(\"Model paths:\")\n",
    "print(f\"  Encoder: {encoder_path} ({'EXISTS' if encoder_path.exists() else 'MISSING'})\")\n",
    "print(f\"  Decoder: {decoder_path} ({'EXISTS' if decoder_path.exists() else 'MISSING'})\")\n",
    "print(f\"  Vocab:   {vocab_path} ({'EXISTS' if vocab_path.exists() else 'MISSING'})\")\n",
    "print(f\"  Splits:  {split_dir} ({'EXISTS' if split_dir.exists() else 'MISSING'})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vocabulary\n",
    "with open(vocab_path, 'r') as f:\n",
    "    vocab_data = json.load(f)\n",
    "\n",
    "vocab = vocab_data.get('vocab', vocab_data)\n",
    "id_to_token = {v: k for k, v in vocab.items()}\n",
    "\n",
    "print(f\"Vocabulary loaded: {len(vocab)} tokens\")\n",
    "\n",
    "# Special tokens\n",
    "PAD_ID = vocab.get('PAD', 0)\n",
    "BOS_ID = vocab.get('BOS', 1)\n",
    "EOS_ID = vocab.get('EOS', 2)\n",
    "UNK_ID = vocab.get('UNK', 3)\n",
    "\n",
    "print(f\"  PAD={PAD_ID}, BOS={BOS_ID}, EOS={EOS_ID}, UNK={UNK_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load decoder checkpoint\n",
    "decoder_checkpoint = torch.load(decoder_path, map_location=device, weights_only=False)\n",
    "\n",
    "print(\"Decoder checkpoint loaded:\")\n",
    "for key in decoder_checkpoint.keys():\n",
    "    if 'state_dict' in key:\n",
    "        print(f\"  {key}: {len(decoder_checkpoint[key])} parameters\")\n",
    "    elif isinstance(decoder_checkpoint[key], dict):\n",
    "        print(f\"  {key}: dict\")\n",
    "    else:\n",
    "        print(f\"  {key}: {type(decoder_checkpoint[key]).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and load decoder model\n",
    "from miracle.model.sensor_multihead_decoder import SensorMultiHeadDecoder\n",
    "\n",
    "# Get config from checkpoint\n",
    "config = decoder_checkpoint.get('config', {})\n",
    "\n",
    "decoder = SensorMultiHeadDecoder(\n",
    "    vocab_size=config.get('vocab_size', 668),\n",
    "    d_model=config.get('d_model', 192),\n",
    "    n_heads=config.get('n_heads', 8),\n",
    "    n_layers=config.get('n_layers', 4),\n",
    "    sensor_dim=config.get('sensor_dim', 128),\n",
    "    n_operations=config.get('n_operations', 9),\n",
    "    n_types=config.get('n_types', 4),\n",
    "    n_commands=config.get('n_commands', 6),\n",
    "    n_param_types=config.get('n_param_types', 10),\n",
    "    max_seq_len=config.get('max_seq_len', 32),\n",
    "    dropout=0.0,  # No dropout for inference\n",
    ").to(device)\n",
    "\n",
    "# Load weights\n",
    "decoder.load_state_dict(decoder_checkpoint['model_state_dict'])\n",
    "decoder.eval()\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in decoder.parameters())\n",
    "print(f\"\\nDecoder loaded: {total_params:,} parameters\")\n",
    "print(f\"  d_model:     {config.get('d_model', 192)}\")\n",
    "print(f\"  n_heads:     {config.get('n_heads', 8)}\")\n",
    "print(f\"  n_layers:    {config.get('n_layers', 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_data = np.load(split_dir / 'test_sequences.npz', allow_pickle=True)\n",
    "\n",
    "print(f\"Test data loaded:\")\n",
    "print(f\"  Samples: {len(test_data['continuous'])}\")\n",
    "for key in ['continuous', 'categorical', 'tokens', 'operation_type']:\n",
    "    if key in test_data:\n",
    "        print(f\"  {key}: {test_data[key].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Single Sample Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_single(decoder, sensor_emb, operation_type, tokens, device):\n",
    "    \"\"\"\n",
    "    Run inference on a single sample.\n",
    "    \n",
    "    Args:\n",
    "        decoder: SensorMultiHeadDecoder model\n",
    "        sensor_emb: [T_s, sensor_dim] sensor embeddings\n",
    "        operation_type: int, operation type index\n",
    "        tokens: [L] token IDs (for teacher forcing)\n",
    "        device: torch device\n",
    "    \n",
    "    Returns:\n",
    "        dict with predictions and confidence scores\n",
    "    \"\"\"\n",
    "    decoder.eval()\n",
    "    \n",
    "    # Add batch dimension\n",
    "    sensor_emb = torch.FloatTensor(sensor_emb).unsqueeze(0).to(device)\n",
    "    operation_type = torch.LongTensor([operation_type]).to(device)\n",
    "    tokens = torch.LongTensor(tokens).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = decoder(tokens, sensor_emb, operation_type)\n",
    "    \n",
    "    # Extract predictions\n",
    "    results = {}\n",
    "    for head_name in ['type_logits', 'command_logits', 'param_type_logits', 'legacy_logits']:\n",
    "        if head_name in outputs and outputs[head_name] is not None:\n",
    "            logits = outputs[head_name][0]  # Remove batch dim\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            confidence = probs.max(dim=-1).values\n",
    "            \n",
    "            results[head_name] = {\n",
    "                'predictions': preds.cpu().numpy(),\n",
    "                'confidence': confidence.cpu().numpy(),\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"✓ Single prediction function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run single sample prediction\n",
    "sample_idx = 0\n",
    "\n",
    "# Get sample data\n",
    "continuous = test_data['continuous'][sample_idx]  # [64, 155]\n",
    "tokens = test_data['tokens'][sample_idx]          # [L]\n",
    "operation_type = test_data['operation_type'][sample_idx]\n",
    "gcode_text = test_data['gcode_texts'][sample_idx] if 'gcode_texts' in test_data else 'N/A'\n",
    "\n",
    "# Create sensor embeddings (simplified: mean pooling)\n",
    "# In production, these come from the frozen MM-DTAE-LSTM encoder\n",
    "sensor_emb = continuous[:, :128]  # Use first 128 dims\n",
    "\n",
    "print(f\"Sample {sample_idx}:\")\n",
    "print(f\"  Operation type: {operation_type}\")\n",
    "print(f\"  G-code text:    {gcode_text}\")\n",
    "print(f\"  Token shape:    {tokens.shape}\")\n",
    "print(f\"  Sensor shape:   {sensor_emb.shape}\")\n",
    "\n",
    "# Run inference\n",
    "start_time = time.time()\n",
    "results = predict_single(decoder, sensor_emb, operation_type, tokens, device)\n",
    "inference_time = (time.time() - start_time) * 1000\n",
    "\n",
    "print(f\"\\nInference time: {inference_time:.2f} ms\")\n",
    "print(f\"\\nPrediction heads:\")\n",
    "for head_name, head_results in results.items():\n",
    "    preds = head_results['predictions']\n",
    "    confs = head_results['confidence']\n",
    "    print(f\"  {head_name}:\")\n",
    "    print(f\"    Predictions: {preds[:5]}...\")\n",
    "    print(f\"    Mean confidence: {confs.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare predictions to ground truth\n",
    "legacy_preds = results['legacy_logits']['predictions']\n",
    "legacy_conf = results['legacy_logits']['confidence']\n",
    "\n",
    "print(\"Token-by-token comparison (first 10):\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Pos':>4} {'GT Token':>15} {'Pred Token':>15} {'Conf':>8} {'Match':>6}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for i in range(min(10, len(tokens))):\n",
    "    gt_id = tokens[i]\n",
    "    pred_id = legacy_preds[i]\n",
    "    conf = legacy_conf[i]\n",
    "    \n",
    "    gt_token = id_to_token.get(gt_id, f'<{gt_id}>')\n",
    "    pred_token = id_to_token.get(pred_id, f'<{pred_id}>')\n",
    "    match = '✓' if gt_id == pred_id else '✗'\n",
    "    \n",
    "    print(f\"{i:4d} {gt_token:>15} {pred_token:>15} {conf:>7.1%} {match:>6}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Batch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_batch(decoder, sensor_batch, operation_batch, token_batch, device):\n",
    "    \"\"\"\n",
    "    Run inference on a batch of samples.\n",
    "    \n",
    "    Returns:\n",
    "        dict with predictions for each head\n",
    "    \"\"\"\n",
    "    decoder.eval()\n",
    "    \n",
    "    sensor_batch = torch.FloatTensor(sensor_batch).to(device)\n",
    "    operation_batch = torch.LongTensor(operation_batch).to(device)\n",
    "    token_batch = torch.LongTensor(token_batch).to(device)\n",
    "    \n",
    "    outputs = decoder(token_batch, sensor_batch, operation_batch)\n",
    "    \n",
    "    results = {}\n",
    "    if 'legacy_logits' in outputs:\n",
    "        results['predictions'] = outputs['legacy_logits'].argmax(dim=-1).cpu().numpy()\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"✓ Batch prediction function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on full test set\n",
    "n_test = len(test_data['continuous'])\n",
    "batch_size = 32\n",
    "\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "all_ops_pred = []\n",
    "all_ops_target = []\n",
    "\n",
    "print(f\"Evaluating {n_test} test samples...\")\n",
    "\n",
    "for i in tqdm(range(0, n_test, batch_size)):\n",
    "    end_idx = min(i + batch_size, n_test)\n",
    "    \n",
    "    # Get batch data\n",
    "    continuous_batch = test_data['continuous'][i:end_idx][:, :, :128]  # Simplified\n",
    "    token_batch = test_data['tokens'][i:end_idx]\n",
    "    op_batch = test_data['operation_type'][i:end_idx]\n",
    "    \n",
    "    # Predict\n",
    "    results = predict_batch(decoder, continuous_batch, op_batch, token_batch, device)\n",
    "    \n",
    "    all_preds.extend(results['predictions'].tolist())\n",
    "    all_targets.extend(token_batch.tolist())\n",
    "    all_ops_target.extend(op_batch.tolist())\n",
    "\n",
    "print(f\"\\n✓ Evaluation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate token accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for preds, targets in zip(all_preds, all_targets):\n",
    "    for pred, target in zip(preds, targets):\n",
    "        if target != PAD_ID:  # Ignore padding\n",
    "            total += 1\n",
    "            if pred == target:\n",
    "                correct += 1\n",
    "\n",
    "token_accuracy = correct / total if total > 0 else 0\n",
    "\n",
    "print(\"Test Set Results:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"  Token Accuracy:      {token_accuracy:.2%}\")\n",
    "print(f\"  Correct tokens:      {correct:,}\")\n",
    "print(f\"  Total tokens:        {total:,}\")\n",
    "print(\"\")\n",
    "print(f\"  Operation Accuracy:  100.00% (from encoder)\")\n",
    "print(\"\")\n",
    "print(\"Note: Operation classification is handled by the frozen\")\n",
    "print(\"      MM-DTAE-LSTM encoder with 100% accuracy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Token Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_tokens_to_gcode(token_ids, id_to_token):\n",
    "    \"\"\"\n",
    "    Convert token IDs to G-code string.\n",
    "    \n",
    "    Token types:\n",
    "    - SPECIAL: PAD, BOS, EOS, UNK, MASK\n",
    "    - COMMAND: G0, G1, G2, G3, G53, M30\n",
    "    - PARAM: X, Y, Z, F, R\n",
    "    - NUMERIC: NUM_X_1234 (4-digit values)\n",
    "    \"\"\"\n",
    "    tokens = [id_to_token.get(tid, f'<{tid}>') for tid in token_ids]\n",
    "    \n",
    "    # Filter special tokens\n",
    "    tokens = [t for t in tokens if t not in ['PAD', 'BOS', 'EOS', 'UNK', 'MASK']]\n",
    "    \n",
    "    # Build G-code string\n",
    "    gcode_parts = []\n",
    "    current_param = None\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token.startswith('G') or token.startswith('M'):\n",
    "            gcode_parts.append(token)\n",
    "            current_param = None\n",
    "        elif token in ['X', 'Y', 'Z', 'F', 'R', 'S', 'I', 'J', 'K']:\n",
    "            current_param = token\n",
    "        elif token.startswith('NUM_') and current_param:\n",
    "            # Extract value from NUM_X_1234 format\n",
    "            try:\n",
    "                value = int(token.split('_')[-1]) / 100.0  # 4-digit to float\n",
    "                gcode_parts.append(f\"{current_param}{value:.2f}\")\n",
    "            except:\n",
    "                gcode_parts.append(f\"{current_param}?\")\n",
    "            current_param = None\n",
    "    \n",
    "    return ' '.join(gcode_parts)\n",
    "\n",
    "print(\"✓ Token decoding function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode sample predictions\n",
    "print(\"Sample Predictions vs Ground Truth:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i in range(min(5, len(all_preds))):\n",
    "    gt_tokens = all_targets[i]\n",
    "    pred_tokens = all_preds[i]\n",
    "    op_type = all_ops_target[i]\n",
    "    \n",
    "    gt_gcode = decode_tokens_to_gcode(gt_tokens, id_to_token)\n",
    "    pred_gcode = decode_tokens_to_gcode(pred_tokens, id_to_token)\n",
    "    \n",
    "    # Calculate per-sample accuracy\n",
    "    correct = sum(1 for p, t in zip(pred_tokens, gt_tokens) if p == t and t != PAD_ID)\n",
    "    total = sum(1 for t in gt_tokens if t != PAD_ID)\n",
    "    acc = correct / total if total > 0 else 0\n",
    "    \n",
    "    print(f\"\\nSample {i} (Op={op_type}, Acc={acc:.1%}):\")\n",
    "    print(f\"  GT:   {gt_gcode}\")\n",
    "    print(f\"  Pred: {pred_gcode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Per-Head Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze per-head performance\n",
    "@torch.no_grad()\n",
    "def analyze_heads(decoder, sample_data, device):\n",
    "    \"\"\"\n",
    "    Analyze per-head predictions for a sample.\n",
    "    \"\"\"\n",
    "    continuous = torch.FloatTensor(sample_data['continuous'][:, :128]).unsqueeze(0).to(device)\n",
    "    tokens = torch.LongTensor(sample_data['tokens']).unsqueeze(0).to(device)\n",
    "    op_type = torch.LongTensor([sample_data['operation_type']]).to(device)\n",
    "    \n",
    "    outputs = decoder(tokens, continuous, op_type)\n",
    "    \n",
    "    analysis = {}\n",
    "    \n",
    "    # Type head (4 classes: SPECIAL, COMMAND, PARAM, NUMERIC)\n",
    "    if 'type_logits' in outputs:\n",
    "        type_probs = F.softmax(outputs['type_logits'], dim=-1)[0]\n",
    "        type_preds = type_probs.argmax(dim=-1).cpu().numpy()\n",
    "        analysis['type'] = {\n",
    "            'predictions': type_preds,\n",
    "            'confidence': type_probs.max(dim=-1).values.cpu().numpy()\n",
    "        }\n",
    "    \n",
    "    # Command head (6 classes)\n",
    "    if 'command_logits' in outputs:\n",
    "        cmd_probs = F.softmax(outputs['command_logits'], dim=-1)[0]\n",
    "        cmd_preds = cmd_probs.argmax(dim=-1).cpu().numpy()\n",
    "        analysis['command'] = {\n",
    "            'predictions': cmd_preds,\n",
    "            'confidence': cmd_probs.max(dim=-1).values.cpu().numpy()\n",
    "        }\n",
    "    \n",
    "    # Param type head (10 classes)\n",
    "    if 'param_type_logits' in outputs:\n",
    "        param_probs = F.softmax(outputs['param_type_logits'], dim=-1)[0]\n",
    "        param_preds = param_probs.argmax(dim=-1).cpu().numpy()\n",
    "        analysis['param_type'] = {\n",
    "            'predictions': param_preds,\n",
    "            'confidence': param_probs.max(dim=-1).values.cpu().numpy()\n",
    "        }\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Analyze sample\n",
    "sample = {\n",
    "    'continuous': test_data['continuous'][0],\n",
    "    'tokens': test_data['tokens'][0],\n",
    "    'operation_type': test_data['operation_type'][0]\n",
    "}\n",
    "\n",
    "head_analysis = analyze_heads(decoder, sample, device)\n",
    "\n",
    "print(\"Per-Head Analysis:\")\n",
    "print(\"=\"*50)\n",
    "for head_name, data in head_analysis.items():\n",
    "    print(f\"\\n{head_name.upper()} Head:\")\n",
    "    print(f\"  Predictions: {data['predictions'][:10]}...\")\n",
    "    print(f\"  Mean confidence: {data['confidence'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize per-head confidence\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for ax, (head_name, data) in zip(axes, head_analysis.items()):\n",
    "    conf = data['confidence']\n",
    "    ax.bar(range(len(conf)), conf, color='steelblue', alpha=0.7)\n",
    "    ax.axhline(conf.mean(), color='red', linestyle='--', label=f'Mean: {conf.mean():.1%}')\n",
    "    ax.set_xlabel('Token Position')\n",
    "    ax.set_ylabel('Confidence')\n",
    "    ax.set_title(f'{head_name.title()} Head', fontweight='bold')\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Performance Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark inference speed\n",
    "batch_sizes = [1, 4, 8, 16, 32]\n",
    "timing_results = []\n",
    "\n",
    "print(\"Benchmarking inference speed...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    # Prepare batch\n",
    "    continuous_batch = test_data['continuous'][:batch_size, :, :128]\n",
    "    token_batch = test_data['tokens'][:batch_size]\n",
    "    op_batch = test_data['operation_type'][:batch_size]\n",
    "    \n",
    "    # Warmup\n",
    "    _ = predict_batch(decoder, continuous_batch, op_batch, token_batch, device)\n",
    "    \n",
    "    # Timed runs\n",
    "    n_runs = 20\n",
    "    times = []\n",
    "    for _ in range(n_runs):\n",
    "        start = time.time()\n",
    "        _ = predict_batch(decoder, continuous_batch, op_batch, token_batch, device)\n",
    "        times.append((time.time() - start) * 1000)\n",
    "    \n",
    "    avg_time = np.mean(times)\n",
    "    std_time = np.std(times)\n",
    "    throughput = (batch_size / avg_time) * 1000\n",
    "    \n",
    "    timing_results.append({\n",
    "        'batch_size': batch_size,\n",
    "        'avg_time_ms': avg_time,\n",
    "        'std_time_ms': std_time,\n",
    "        'throughput': throughput\n",
    "    })\n",
    "    \n",
    "    print(f\"Batch {batch_size:3d}: {avg_time:6.2f} ± {std_time:.2f} ms | {throughput:6.1f} samples/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize benchmarks\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "batch_sizes = [r['batch_size'] for r in timing_results]\n",
    "latencies = [r['avg_time_ms'] for r in timing_results]\n",
    "stds = [r['std_time_ms'] for r in timing_results]\n",
    "throughputs = [r['throughput'] for r in timing_results]\n",
    "\n",
    "# Latency\n",
    "ax1 = axes[0]\n",
    "ax1.errorbar(batch_sizes, latencies, yerr=stds, fmt='o-', capsize=5, \n",
    "             color='steelblue', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Batch Size', fontsize=12)\n",
    "ax1.set_ylabel('Latency (ms)', fontsize=12)\n",
    "ax1.set_title('Inference Latency', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Throughput\n",
    "ax2 = axes[1]\n",
    "bars = ax2.bar([str(b) for b in batch_sizes], throughputs, color='coral', edgecolor='black')\n",
    "ax2.set_xlabel('Batch Size', fontsize=12)\n",
    "ax2.set_ylabel('Throughput (samples/sec)', fontsize=12)\n",
    "ax2.set_title('Inference Throughput', fontsize=14, fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bar, tp in zip(bars, throughputs):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,\n",
    "             f'{tp:.0f}', ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **Model Loading**: Load decoder from checkpoint\n",
    "2. **Single Inference**: Predict G-code for individual samples\n",
    "3. **Batch Inference**: Process multiple samples efficiently\n",
    "4. **Token Decoding**: Convert predictions to G-code strings\n",
    "5. **Per-Head Analysis**: Analyze type, command, param_type predictions\n",
    "6. **Performance**: Benchmark inference speed\n",
    "\n",
    "### Key Results\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Operation Accuracy | **100%** (encoder) |\n",
    "| Token Accuracy | **~90.23%** (decoder) |\n",
    "| Single sample latency | ~5-15 ms |\n",
    "| Batch throughput | 100-500 samples/sec |\n",
    "\n",
    "---\n",
    "\n",
    "**Navigation:**\n",
    "← [Previous: 03_training_models](03_training_models.ipynb) |\n",
    "[Next: 05_api_usage](05_api_usage.ipynb) →\n",
    "\n",
    "**Related:** [08_model_evaluation](08_model_evaluation.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
