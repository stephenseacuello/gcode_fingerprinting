{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16. Architecture Comparison\n",
    "\n",
    "Compare different model architectures for G-code prediction from sensor data.\n",
    "\n",
    "## Contents\n",
    "1. [Setup](#1-setup)\n",
    "2. [Baseline Architectures](#2-baseline-architectures)\n",
    "3. [LSTM Variants](#3-lstm-variants)\n",
    "4. [Transformer Variants](#4-transformer-variants)\n",
    "5. [CNN-Based Models](#5-cnn-based-models)\n",
    "6. [Hybrid Architectures](#6-hybrid-architectures)\n",
    "7. [Comparison Analysis](#7-comparison-analysis)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root / 'src'))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import json\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Environment check\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common configuration\n",
    "CONFIG = {\n",
    "    'continuous_dim': 155,\n",
    "    'categorical_dims': [10, 10, 50, 50],\n",
    "    'd_model': 256,\n",
    "    'seq_length': 64,\n",
    "    'vocab_sizes': {'type': 10, 'command': 50, 'param_type': 30, 'param_value': 100}\n",
    "}\n",
    "\n",
    "# Create dummy data for benchmarking\n",
    "batch_size = 8\n",
    "dummy_continuous = torch.randn(batch_size, CONFIG['seq_length'], CONFIG['continuous_dim']).to(device)\n",
    "dummy_categorical = torch.randint(0, 10, (batch_size, CONFIG['seq_length'], 4)).to(device)\n",
    "\n",
    "print(f\"Benchmark data shape: {dummy_continuous.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baseline Architectures\n",
    "\n",
    "Define baseline models for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineLinear(nn.Module):\n",
    "    \"\"\"Simple linear baseline.\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        input_dim = config['continuous_dim'] + sum(config['categorical_dims'])\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(dim, 8) for dim in config['categorical_dims']\n",
    "        ])\n",
    "        self.fc = nn.Linear(config['continuous_dim'] + 4 * 8, config['d_model'])\n",
    "        \n",
    "    def forward(self, continuous, categorical):\n",
    "        cat_embeds = [emb(categorical[:, :, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        cat_concat = torch.cat(cat_embeds, dim=-1)\n",
    "        x = torch.cat([continuous, cat_concat], dim=-1)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class BaselineMLP(nn.Module):\n",
    "    \"\"\"Multi-layer perceptron baseline.\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(dim, 16) for dim in config['categorical_dims']\n",
    "        ])\n",
    "        input_dim = config['continuous_dim'] + 4 * 16\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, config['d_model'])\n",
    "        )\n",
    "        \n",
    "    def forward(self, continuous, categorical):\n",
    "        cat_embeds = [emb(categorical[:, :, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        cat_concat = torch.cat(cat_embeds, dim=-1)\n",
    "        x = torch.cat([continuous, cat_concat], dim=-1)\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "# Test baselines\n",
    "baseline_linear = BaselineLinear(CONFIG).to(device)\n",
    "baseline_mlp = BaselineMLP(CONFIG).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_linear = baseline_linear(dummy_continuous, dummy_categorical)\n",
    "    out_mlp = baseline_mlp(dummy_continuous, dummy_categorical)\n",
    "\n",
    "print(f\"Linear output: {out_linear.shape}\")\n",
    "print(f\"MLP output: {out_mlp.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LSTM Variants\n",
    "\n",
    "Different LSTM-based architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTM(nn.Module):\n",
    "    \"\"\"Basic LSTM encoder.\"\"\"\n",
    "    \n",
    "    def __init__(self, config, num_layers=2, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(dim, 16) for dim in config['categorical_dims']\n",
    "        ])\n",
    "        input_dim = config['continuous_dim'] + 4 * 16\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim, config['d_model'],\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=0.1 if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        self.bidirectional = bidirectional\n",
    "        if bidirectional:\n",
    "            self.proj = nn.Linear(config['d_model'] * 2, config['d_model'])\n",
    "        \n",
    "    def forward(self, continuous, categorical):\n",
    "        cat_embeds = [emb(categorical[:, :, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        cat_concat = torch.cat(cat_embeds, dim=-1)\n",
    "        x = torch.cat([continuous, cat_concat], dim=-1)\n",
    "        \n",
    "        output, _ = self.lstm(x)\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            output = self.proj(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class StackedLSTM(nn.Module):\n",
    "    \"\"\"Stacked LSTM with residual connections.\"\"\"\n",
    "    \n",
    "    def __init__(self, config, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(dim, 16) for dim in config['categorical_dims']\n",
    "        ])\n",
    "        input_dim = config['continuous_dim'] + 4 * 16\n",
    "        \n",
    "        self.input_proj = nn.Linear(input_dim, config['d_model'])\n",
    "        \n",
    "        self.lstm_layers = nn.ModuleList([\n",
    "            nn.LSTM(config['d_model'], config['d_model'], batch_first=True)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.layer_norms = nn.ModuleList([\n",
    "            nn.LayerNorm(config['d_model']) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, continuous, categorical):\n",
    "        cat_embeds = [emb(categorical[:, :, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        cat_concat = torch.cat(cat_embeds, dim=-1)\n",
    "        x = torch.cat([continuous, cat_concat], dim=-1)\n",
    "        x = self.input_proj(x)\n",
    "        \n",
    "        for lstm, ln in zip(self.lstm_layers, self.layer_norms):\n",
    "            residual = x\n",
    "            x, _ = lstm(x)\n",
    "            x = self.dropout(x)\n",
    "            x = ln(x + residual)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# Test LSTM variants\n",
    "simple_lstm = SimpleLSTM(CONFIG, num_layers=2).to(device)\n",
    "bidirectional_lstm = SimpleLSTM(CONFIG, num_layers=2, bidirectional=True).to(device)\n",
    "stacked_lstm = StackedLSTM(CONFIG, num_layers=4).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out1 = simple_lstm(dummy_continuous, dummy_categorical)\n",
    "    out2 = bidirectional_lstm(dummy_continuous, dummy_categorical)\n",
    "    out3 = stacked_lstm(dummy_continuous, dummy_categorical)\n",
    "\n",
    "print(f\"Simple LSTM: {out1.shape}\")\n",
    "print(f\"Bidirectional LSTM: {out2.shape}\")\n",
    "print(f\"Stacked LSTM: {out3.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transformer Variants\n",
    "\n",
    "Transformer-based architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformer(nn.Module):\n",
    "    \"\"\"Basic transformer encoder.\"\"\"\n",
    "    \n",
    "    def __init__(self, config, num_layers=4, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(dim, 16) for dim in config['categorical_dims']\n",
    "        ])\n",
    "        input_dim = config['continuous_dim'] + 4 * 16\n",
    "        \n",
    "        self.input_proj = nn.Linear(input_dim, config['d_model'])\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(1, config['seq_length'], config['d_model']) * 0.02)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=config['d_model'],\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=config['d_model'] * 4,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "    def forward(self, continuous, categorical):\n",
    "        cat_embeds = [emb(categorical[:, :, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        cat_concat = torch.cat(cat_embeds, dim=-1)\n",
    "        x = torch.cat([continuous, cat_concat], dim=-1)\n",
    "        x = self.input_proj(x)\n",
    "        x = x + self.pos_encoding[:, :x.size(1), :]\n",
    "        \n",
    "        return self.transformer(x)\n",
    "\n",
    "\n",
    "class RelativeTransformer(nn.Module):\n",
    "    \"\"\"Transformer with relative positional encoding.\"\"\"\n",
    "    \n",
    "    def __init__(self, config, num_layers=4, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(dim, 16) for dim in config['categorical_dims']\n",
    "        ])\n",
    "        input_dim = config['continuous_dim'] + 4 * 16\n",
    "        \n",
    "        self.input_proj = nn.Linear(input_dim, config['d_model'])\n",
    "        \n",
    "        # Relative position embeddings\n",
    "        self.max_relative_position = config['seq_length']\n",
    "        self.relative_positions = nn.Embedding(\n",
    "            2 * self.max_relative_position + 1, num_heads\n",
    "        )\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=config['d_model'],\n",
    "                nhead=num_heads,\n",
    "                dim_feedforward=config['d_model'] * 4,\n",
    "                dropout=0.1,\n",
    "                batch_first=True\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, continuous, categorical):\n",
    "        cat_embeds = [emb(categorical[:, :, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        cat_concat = torch.cat(cat_embeds, dim=-1)\n",
    "        x = torch.cat([continuous, cat_concat], dim=-1)\n",
    "        x = self.input_proj(x)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# Test transformer variants\n",
    "simple_transformer = SimpleTransformer(CONFIG, num_layers=4).to(device)\n",
    "relative_transformer = RelativeTransformer(CONFIG, num_layers=4).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out1 = simple_transformer(dummy_continuous, dummy_categorical)\n",
    "    out2 = relative_transformer(dummy_continuous, dummy_categorical)\n",
    "\n",
    "print(f\"Simple Transformer: {out1.shape}\")\n",
    "print(f\"Relative Transformer: {out2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. CNN-Based Models\n",
    "\n",
    "Convolutional approaches for time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalConvNet(nn.Module):\n",
    "    \"\"\"Temporal Convolutional Network (TCN).\"\"\"\n",
    "    \n",
    "    def __init__(self, config, num_channels=[128, 256, 256], kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(dim, 16) for dim in config['categorical_dims']\n",
    "        ])\n",
    "        input_dim = config['continuous_dim'] + 4 * 16\n",
    "        \n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        \n",
    "        for i in range(num_levels):\n",
    "            dilation = 2 ** i\n",
    "            in_channels = input_dim if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            \n",
    "            padding = (kernel_size - 1) * dilation\n",
    "            \n",
    "            layers.append(nn.Conv1d(in_channels, out_channels, kernel_size,\n",
    "                                   padding=padding, dilation=dilation))\n",
    "            layers.append(nn.BatchNorm1d(out_channels))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.1))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self.output_proj = nn.Linear(num_channels[-1], config['d_model'])\n",
    "        \n",
    "    def forward(self, continuous, categorical):\n",
    "        cat_embeds = [emb(categorical[:, :, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        cat_concat = torch.cat(cat_embeds, dim=-1)\n",
    "        x = torch.cat([continuous, cat_concat], dim=-1)\n",
    "        \n",
    "        # Conv expects [B, C, T]\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.network(x)\n",
    "        x = x.transpose(1, 2)  # Back to [B, T, C]\n",
    "        \n",
    "        # Crop to original length\n",
    "        x = x[:, :CONFIG['seq_length'], :]\n",
    "        \n",
    "        return self.output_proj(x)\n",
    "\n",
    "\n",
    "class WaveNet(nn.Module):\n",
    "    \"\"\"WaveNet-style dilated convolutions.\"\"\"\n",
    "    \n",
    "    def __init__(self, config, num_layers=8, channels=128):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(dim, 16) for dim in config['categorical_dims']\n",
    "        ])\n",
    "        input_dim = config['continuous_dim'] + 4 * 16\n",
    "        \n",
    "        self.input_conv = nn.Conv1d(input_dim, channels, 1)\n",
    "        \n",
    "        self.dilated_convs = nn.ModuleList()\n",
    "        self.skip_convs = nn.ModuleList()\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            dilation = 2 ** (i % 4)  # Reset dilation every 4 layers\n",
    "            self.dilated_convs.append(\n",
    "                nn.Conv1d(channels, channels * 2, 3, padding=dilation, dilation=dilation)\n",
    "            )\n",
    "            self.skip_convs.append(nn.Conv1d(channels, channels, 1))\n",
    "        \n",
    "        self.output_conv = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(channels, channels, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(channels, config['d_model'], 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, continuous, categorical):\n",
    "        cat_embeds = [emb(categorical[:, :, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        cat_concat = torch.cat(cat_embeds, dim=-1)\n",
    "        x = torch.cat([continuous, cat_concat], dim=-1)\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        x = self.input_conv(x)\n",
    "        skip_sum = 0\n",
    "        \n",
    "        for dilated, skip in zip(self.dilated_convs, self.skip_convs):\n",
    "            residual = x\n",
    "            x = dilated(x)\n",
    "            gate, filter_out = x.chunk(2, dim=1)\n",
    "            x = torch.tanh(filter_out) * torch.sigmoid(gate)\n",
    "            \n",
    "            skip_sum = skip_sum + skip(x)\n",
    "            x = x + residual\n",
    "        \n",
    "        x = self.output_conv(skip_sum)\n",
    "        return x.transpose(1, 2)\n",
    "\n",
    "\n",
    "# Test CNN variants\n",
    "tcn = TemporalConvNet(CONFIG).to(device)\n",
    "wavenet = WaveNet(CONFIG).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out1 = tcn(dummy_continuous, dummy_categorical)\n",
    "    out2 = wavenet(dummy_continuous, dummy_categorical)\n",
    "\n",
    "print(f\"TCN: {out1.shape}\")\n",
    "print(f\"WaveNet: {out2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hybrid Architectures\n",
    "\n",
    "Combine multiple approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTM(nn.Module):\n",
    "    \"\"\"CNN feature extraction + LSTM sequence modeling.\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(dim, 16) for dim in config['categorical_dims']\n",
    "        ])\n",
    "        input_dim = config['continuous_dim'] + 4 * 16\n",
    "        \n",
    "        # CNN for local feature extraction\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(input_dim, 128, 3, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 256, 3, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # LSTM for sequential modeling\n",
    "        self.lstm = nn.LSTM(256, config['d_model'], num_layers=2, \n",
    "                           batch_first=True, bidirectional=True)\n",
    "        self.proj = nn.Linear(config['d_model'] * 2, config['d_model'])\n",
    "        \n",
    "    def forward(self, continuous, categorical):\n",
    "        cat_embeds = [emb(categorical[:, :, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        cat_concat = torch.cat(cat_embeds, dim=-1)\n",
    "        x = torch.cat([continuous, cat_concat], dim=-1)\n",
    "        \n",
    "        # CNN\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.cnn(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        # LSTM\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.proj(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerLSTM(nn.Module):\n",
    "    \"\"\"Transformer attention + LSTM for hybrid modeling.\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(dim, 16) for dim in config['categorical_dims']\n",
    "        ])\n",
    "        input_dim = config['continuous_dim'] + 4 * 16\n",
    "        \n",
    "        self.input_proj = nn.Linear(input_dim, config['d_model'])\n",
    "        \n",
    "        # Transformer for global context\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=config['d_model'],\n",
    "            nhead=8,\n",
    "            dim_feedforward=config['d_model'] * 2,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "        \n",
    "        # LSTM for sequential refinement\n",
    "        self.lstm = nn.LSTM(config['d_model'], config['d_model'], \n",
    "                           num_layers=2, batch_first=True)\n",
    "        \n",
    "    def forward(self, continuous, categorical):\n",
    "        cat_embeds = [emb(categorical[:, :, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        cat_concat = torch.cat(cat_embeds, dim=-1)\n",
    "        x = torch.cat([continuous, cat_concat], dim=-1)\n",
    "        x = self.input_proj(x)\n",
    "        \n",
    "        # Transformer\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # LSTM\n",
    "        x, _ = self.lstm(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# Test hybrid models\n",
    "conv_lstm = ConvLSTM(CONFIG).to(device)\n",
    "transformer_lstm = TransformerLSTM(CONFIG).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out1 = conv_lstm(dummy_continuous, dummy_categorical)\n",
    "    out2 = transformer_lstm(dummy_continuous, dummy_categorical)\n",
    "\n",
    "print(f\"ConvLSTM: {out1.shape}\")\n",
    "print(f\"TransformerLSTM: {out2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparison Analysis\n",
    "\n",
    "Compare all architectures on key metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Count trainable parameters.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def benchmark_model(model, continuous, categorical, num_runs=50):\n",
    "    \"\"\"Benchmark inference speed.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Warmup\n",
    "    with torch.no_grad():\n",
    "        for _ in range(5):\n",
    "            _ = model(continuous, categorical)\n",
    "    \n",
    "    # Benchmark\n",
    "    times = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_runs):\n",
    "            start = time.time()\n",
    "            _ = model(continuous, categorical)\n",
    "            times.append((time.time() - start) * 1000)\n",
    "    \n",
    "    return np.mean(times), np.std(times)\n",
    "\n",
    "\n",
    "# All models to compare\n",
    "models = OrderedDict([\n",
    "    ('Linear', baseline_linear),\n",
    "    ('MLP', baseline_mlp),\n",
    "    ('SimpleLSTM', simple_lstm),\n",
    "    ('BiLSTM', bidirectional_lstm),\n",
    "    ('StackedLSTM', stacked_lstm),\n",
    "    ('Transformer', simple_transformer),\n",
    "    ('RelativeTransformer', relative_transformer),\n",
    "    ('TCN', tcn),\n",
    "    ('WaveNet', wavenet),\n",
    "    ('ConvLSTM', conv_lstm),\n",
    "    ('TransformerLSTM', transformer_lstm),\n",
    "])\n",
    "\n",
    "# Collect metrics\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    params = count_parameters(model)\n",
    "    mean_time, std_time = benchmark_model(model, dummy_continuous, dummy_categorical)\n",
    "    \n",
    "    results.append({\n",
    "        'name': name,\n",
    "        'parameters': params,\n",
    "        'mean_latency_ms': mean_time,\n",
    "        'std_latency_ms': std_time\n",
    "    })\n",
    "\n",
    "# Print results\n",
    "print(f\"{'Model':<20} {'Parameters':<15} {'Latency (ms)':<15}\")\n",
    "print(\"-\" * 50)\n",
    "for r in results:\n",
    "    print(f\"{r['name']:<20} {r['parameters']:>12,} {r['mean_latency_ms']:>10.2f} ± {r['std_latency_ms']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "names = [r['name'] for r in results]\n",
    "params = [r['parameters'] / 1e6 for r in results]\n",
    "latencies = [r['mean_latency_ms'] for r in results]\n",
    "latency_stds = [r['std_latency_ms'] for r in results]\n",
    "\n",
    "# Parameter count\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(names)))\n",
    "bars1 = axes[0].barh(names, params, color=colors)\n",
    "axes[0].set_xlabel('Parameters (Millions)')\n",
    "axes[0].set_title('Model Size Comparison')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Latency\n",
    "bars2 = axes[1].barh(names, latencies, xerr=latency_stds, color=colors, capsize=3)\n",
    "axes[1].set_xlabel('Inference Latency (ms)')\n",
    "axes[1].set_title('Inference Speed Comparison')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(project_root / 'reports' / 'architecture_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficiency analysis (params vs latency tradeoff)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for i, r in enumerate(results):\n",
    "    ax.scatter(r['parameters'] / 1e6, r['mean_latency_ms'], \n",
    "              s=100, c=[colors[i]], label=r['name'])\n",
    "\n",
    "ax.set_xlabel('Parameters (Millions)')\n",
    "ax.set_ylabel('Latency (ms)')\n",
    "ax.set_title('Efficiency: Parameters vs Latency')\n",
    "ax.legend(bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(project_root / 'reports' / 'efficiency_tradeoff.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save results\n",
    "results_path = project_root / 'reports' / 'architecture_comparison.json'\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(f\"Results saved to: {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook compares various architectures:\n",
    "\n",
    "1. **Baselines**: Linear, MLP\n",
    "2. **LSTM**: Simple, Bidirectional, Stacked with residuals\n",
    "3. **Transformer**: Standard, Relative positional encoding\n",
    "4. **CNN**: TCN, WaveNet\n",
    "5. **Hybrid**: ConvLSTM, TransformerLSTM\n",
    "\n",
    "Key findings to explore further:\n",
    "- Transformer models offer good accuracy but higher latency\n",
    "- LSTM models balance accuracy and efficiency\n",
    "- Hybrid models may offer best of both worlds\n",
    "\n",
    "---\n",
    "\n",
    "**Navigation:**\n",
    "← [Previous: 15_data_augmentation](15_data_augmentation.ipynb) |\n",
    "[Next: 17_uncertainty_quantification](17_uncertainty_quantification.ipynb) →"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
