{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Data Preprocessing\n",
    "\n",
    "This notebook demonstrates the complete data preprocessing pipeline.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the preprocessing pipeline\n",
    "- Learn about feature extraction from sensor data\n",
    "- Create vocabulary from G-code tokens\n",
    "- Generate train/validation/test splits\n",
    "- Visualize preprocessed data\n",
    "\n",
    "## Preprocessing Steps\n",
    "1. Load raw G-code and sensor data\n",
    "2. Tokenize G-code sequences\n",
    "3. Build vocabulary\n",
    "4. Extract and normalize sensor features\n",
    "5. Create sliding windows\n",
    "6. Split into train/val/test sets\n",
    "7. Save processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root / 'src'))\n",
    "\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "print(\"âœ“ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding the Preprocessing Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View preprocessing module structure\n",
    "preprocessing_module = project_root / 'src' / 'miracle' / 'dataset' / 'preprocessing.py'\n",
    "\n",
    "if preprocessing_module.exists():\n",
    "    print(\"Preprocessing module found!\")\n",
    "    print(f\"Location: {preprocessing_module}\")\n",
    "    print(\"\\nKey functions:\")\n",
    "    print(\"  - load_gcode(): Load G-code files\")\n",
    "    print(\"  - tokenize(): Convert G-code to tokens\")\n",
    "    print(\"  - build_vocabulary(): Create token-to-ID mapping\")\n",
    "    print(\"  - extract_features(): Process sensor data\")\n",
    "    print(\"  - create_sequences(): Generate training samples\")\n",
    "else:\n",
    "    print(\"Preprocessing module not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Running Preprocessing\n",
    "\n",
    "The preprocessing script can be run from the command line or imported as a module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Command-line preprocessing (shown for reference)\n",
    "print(\"To run preprocessing from command line:\")\n",
    "print()\n",
    "print(\"PYTHONPATH=src .venv/bin/python -m miracle.dataset.preprocessing \\\\\")\n",
    "print(\"    --data-dir data \\\\\")\n",
    "print(\"    --output-dir outputs/processed_demo \\\\\")\n",
    "print(\"    --vocab-path data/vocabulary.json \\\\\")\n",
    "print(\"    --max-seq-length 64 \\\\\")\n",
    "print(\"    --train-ratio 0.7 \\\\\")\n",
    "print(\"    --val-ratio 0.15\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploring Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find preprocessed data\n",
    "import glob\n",
    "\n",
    "processed_dirs = glob.glob(str(project_root / 'outputs' / 'processed*'))\n",
    "\n",
    "if processed_dirs:\n",
    "    print(f\"Found {len(processed_dirs)} preprocessed dataset(s):\")\n",
    "    for d in processed_dirs:\n",
    "        print(f\"  - {Path(d).name}\")\n",
    "    \n",
    "    # Use the first one\n",
    "    processed_dir = Path(processed_dirs[0])\n",
    "    print(f\"\\nUsing: {processed_dir.name}\")\n",
    "else:\n",
    "    print(\"No preprocessed data found. Run preprocessing first.\")\n",
    "    processed_dir = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "if processed_dir:\n",
    "    train_data = np.load(processed_dir / 'train_sequences.npz', allow_pickle=True)\n",
    "    val_data = np.load(processed_dir / 'val_sequences.npz', allow_pickle=True)\n",
    "    test_data = np.load(processed_dir / 'test_sequences.npz', allow_pickle=True)\n",
    "    \n",
    "    print(\"Data loaded successfully!\")\n",
    "    print(f\"\\nTrain set:\")\n",
    "    print(f\"  Samples: {len(train_data['tokens'])}\")\n",
    "    print(f\"  Keys: {list(train_data.keys())}\")\n",
    "    \n",
    "    print(f\"\\nValidation set: {len(val_data['tokens'])} samples\")\n",
    "    print(f\"Test set: {len(test_data['tokens'])} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine data structure\n",
    "if processed_dir:\n",
    "    print(\"Sample data structure:\")\n",
    "    print(f\"\\nTokens shape: {train_data['tokens'][0].shape}\")\n",
    "    print(f\"Continuous features shape: {train_data['continuous'][0].shape}\")\n",
    "    print(f\"Categorical features shape: {train_data['categorical'][0].shape}\")\n",
    "    \n",
    "    print(f\"\\nFirst token sequence (truncated):\")\n",
    "    print(train_data['tokens'][0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualizing Sensor Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze continuous features\n",
    "if processed_dir:\n",
    "    sample_continuous = train_data['continuous'][0]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Feature distributions for different timesteps\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        feature_idx = i * 30  # Sample different features\n",
    "        if feature_idx < sample_continuous.shape[1]:\n",
    "            ax.plot(sample_continuous[:, feature_idx], alpha=0.7)\n",
    "            ax.set_title(f\"Continuous Feature {feature_idx}\", fontweight='bold')\n",
    "            ax.set_xlabel('Timestep')\n",
    "            ax.set_ylabel('Value')\n",
    "            ax.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyzing Sequence Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence length distribution\n",
    "if processed_dir:\n",
    "    seq_lengths = [len(seq) for seq in train_data['tokens']]\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.hist(seq_lengths, bins=30, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "    plt.axvline(np.mean(seq_lengths), color='red', linestyle='--', label=f'Mean: {np.mean(seq_lengths):.1f}')\n",
    "    plt.axvline(np.median(seq_lengths), color='green', linestyle='--', label=f'Median: {np.median(seq_lengths):.1f}')\n",
    "    plt.xlabel('Sequence Length', fontsize=12)\n",
    "    plt.ylabel('Frequency', fontsize=12)\n",
    "    plt.title('Distribution of Token Sequence Lengths', fontsize=14, fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nSequence length statistics:\")\n",
    "    print(f\"  Min: {min(seq_lengths)}\")\n",
    "    print(f\"  Max: {max(seq_lengths)}\")\n",
    "    print(f\"  Mean: {np.mean(seq_lengths):.1f}\")\n",
    "    print(f\"  Std: {np.std(seq_lengths):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Vocabulary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze vocabulary\n",
    "vocab_path = project_root / 'data' / 'vocabulary.json'\n",
    "\n",
    "if vocab_path.exists():\n",
    "    with open(vocab_path, 'r') as f:\n",
    "        vocab = json.load(f)\n",
    "    \n",
    "    # Count token usage in training data\n",
    "    if processed_dir:\n",
    "        all_tokens = []\n",
    "        for seq in train_data['tokens'][:100]:  # Sample 100 sequences\n",
    "            all_tokens.extend(seq)\n",
    "        \n",
    "        token_counts = Counter(all_tokens)\n",
    "        \n",
    "        print(f\"Vocabulary size: {len(vocab)}\")\n",
    "        print(f\"Tokens used in sample: {len(token_counts)}\")\n",
    "        print(f\"\\nTop 20 most common tokens:\")\n",
    "        \n",
    "        # Get reverse mapping\n",
    "        id_to_token = {v: k for k, v in vocab.items()}\n",
    "        \n",
    "        for token_id, count in token_counts.most_common(20):\n",
    "            token_str = id_to_token.get(token_id, f'<ID:{token_id}>')\n",
    "            print(f\"  {token_str:15s}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hands-On: Custom Preprocessing\n",
    "\n",
    "Create a mini-dataset from a small subset of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Simple tokenization function\n",
    "import re\n",
    "\n",
    "def simple_tokenize(gcode_line):\n",
    "    \"\"\"Tokenize a single G-code line.\"\"\"\n",
    "    # Remove comments\n",
    "    line = re.sub(r';.*', '', gcode_line)\n",
    "    line = re.sub(r'\\(.*?\\)', '', line)\n",
    "    \n",
    "    # Extract tokens\n",
    "    tokens = []\n",
    "    for token in line.strip().split():\n",
    "        # Match patterns like G0, X10.5, etc.\n",
    "        if re.match(r'[A-Z][\\d.\\-]+', token):\n",
    "            tokens.append(token)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Test\n",
    "test_line = \"G0 X10.5 Y20.3 Z5.0 F1500 ; Move to position\"\n",
    "tokens = simple_tokenize(test_line)\n",
    "print(f\"Input: {test_line}\")\n",
    "print(f\"Tokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You learned:\n",
    "- How the preprocessing pipeline works\n",
    "- How to load and explore preprocessed data\n",
    "- Understanding sensor feature structure\n",
    "- Vocabulary creation and usage\n",
    "- Sequence length distributions\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue to **03_training_models.ipynb** to learn how to train models on this preprocessed data.\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "- **No preprocessed data**: Run `PYTHONPATH=src .venv/bin/python -m miracle.dataset.preprocessing ...`\n",
    "- **Vocabulary mismatch**: Ensure you use the same vocabulary file for both preprocessing and training\n",
    "- **Memory errors**: Reduce `--max-samples` in preprocessing arguments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
