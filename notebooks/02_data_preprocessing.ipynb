{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Data Preprocessing\n",
    "\n",
    "**Complete walkthrough of the data preprocessing pipeline.**\n",
    "\n",
    "## Learning Objectives\n",
    "- Load and inspect raw sensor + G-code data\n",
    "- Understand the 4-digit hybrid G-code tokenization\n",
    "- Configure windowing and stride parameters\n",
    "- Create multilabel stratified train/validation/test splits\n",
    "- Validate data quality and coverage\n",
    "- Visualize the preprocessing pipeline\n",
    "\n",
    "## Table of Contents\n",
    "1. [Raw Data Loading](#1.-Raw-Data-Loading)\n",
    "2. [Sensor Feature Extraction](#2.-Sensor-Feature-Extraction)\n",
    "3. [G-code Tokenization (4-Digit Hybrid)](#3.-G-code-Tokenization)\n",
    "4. [Vocabulary Management](#4.-Vocabulary-Management)\n",
    "5. [Window Configuration](#5.-Window-Configuration)\n",
    "6. [Multilabel Stratified Splitting](#6.-Multilabel-Stratified-Splitting)\n",
    "7. [Data Quality Validation](#7.-Data-Quality-Validation)\n",
    "8. [Pipeline Visualization](#8.-Pipeline-Visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Environment Check\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root / 'src'))\n",
    "\n",
    "# Environment info\n",
    "print(f\"Python: {sys.version.split()[0]}\")\n",
    "print(f\"Project root: {project_root}\")\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "np.random.seed(SEED)\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Raw Data Loading\n",
    "\n",
    "Load and inspect the raw CSV files containing aligned sensor + G-code data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all raw data files\n",
    "data_dir = project_root / 'data'\n",
    "csv_files = sorted(data_dir.glob('*.csv'))\n",
    "\n",
    "print(f\"Found {len(csv_files)} data files in {data_dir}\")\n",
    "print(\"\\nFiles by operation type:\")\n",
    "\n",
    "# Categorize by operation\n",
    "files_by_op = defaultdict(list)\n",
    "for f in csv_files:\n",
    "    name = f.stem\n",
    "    if 'Face' in name:\n",
    "        files_by_op['Face'].append(f)\n",
    "    elif 'Pocket' in name:\n",
    "        files_by_op['Pocket'].append(f)\n",
    "    elif 'Adaptive' in name:\n",
    "        files_by_op['Adaptive'].append(f)\n",
    "    elif 'Damaged' in name:\n",
    "        files_by_op['Damaged'].append(f)\n",
    "    else:\n",
    "        files_by_op['Other'].append(f)\n",
    "\n",
    "for op, files in files_by_op.items():\n",
    "    print(f\"  {op}: {len(files)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample file to inspect structure\n",
    "if csv_files:\n",
    "    sample_file = csv_files[0]\n",
    "    df = pd.read_csv(sample_file)\n",
    "    \n",
    "    print(f\"Sample file: {sample_file.name}\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"\\nColumns ({len(df.columns)}):\")\n",
    "    \n",
    "    # Categorize columns\n",
    "    gcode_cols = [c for c in df.columns if 'gcode' in c.lower() or 'token' in c.lower()]\n",
    "    sensor_cols = [c for c in df.columns if c not in gcode_cols]\n",
    "    \n",
    "    print(f\"  G-code columns: {len(gcode_cols)}\")\n",
    "    print(f\"  Sensor columns: {len(sensor_cols)}\")\n",
    "    \n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect column data types\n",
    "if csv_files:\n",
    "    print(\"Column Data Types:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    string_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    print(f\"\\nNumeric columns ({len(numeric_cols)}):\")\n",
    "    for col in numeric_cols[:10]:\n",
    "        print(f\"  {col}: {df[col].dtype}\")\n",
    "    if len(numeric_cols) > 10:\n",
    "        print(f\"  ... and {len(numeric_cols) - 10} more\")\n",
    "    \n",
    "    print(f\"\\nString columns ({len(string_cols)}):\")\n",
    "    for col in string_cols:\n",
    "        print(f\"  {col}: {df[col].dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Sensor Feature Extraction\n",
    "\n",
    "Extract and normalize continuous and categorical sensor features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify sensor feature types\n",
    "if csv_files:\n",
    "    # Typical sensor columns\n",
    "    continuous_candidates = [\n",
    "        'SpindleLoad', 'ActFeedRate', 'Xact', 'Yact', 'Zact',\n",
    "        'ActSpindleSpeed', 'ToolLength', 'ToolRadius'\n",
    "    ]\n",
    "    \n",
    "    # Find actual columns\n",
    "    continuous_cols = [c for c in df.columns if any(cc in c for cc in continuous_candidates)]\n",
    "    \n",
    "    print(f\"Continuous sensor features ({len(continuous_cols)}):\")\n",
    "    for col in continuous_cols[:10]:\n",
    "        stats = df[col].describe()\n",
    "        print(f\"  {col}: mean={stats['mean']:.2f}, std={stats['std']:.2f}\")\n",
    "    \n",
    "    if len(continuous_cols) > 10:\n",
    "        print(f\"  ... and {len(continuous_cols) - 10} more features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sensor feature distributions\n",
    "if csv_files and continuous_cols:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "    \n",
    "    plot_cols = continuous_cols[:6]\n",
    "    \n",
    "    for ax, col in zip(axes.flat, plot_cols):\n",
    "        data = df[col].dropna()\n",
    "        ax.hist(data, bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "        ax.axvline(data.mean(), color='red', linestyle='--', label=f'Mean: {data.mean():.1f}')\n",
    "        ax.set_xlabel(col, fontsize=10)\n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax.legend(fontsize=8)\n",
    "        ax.grid(alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Sensor Feature Distributions', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature normalization demonstration\n",
    "def normalize_features(df, cols, method='zscore'):\n",
    "    \"\"\"Normalize features using z-score or min-max.\"\"\"\n",
    "    normalized = df[cols].copy()\n",
    "    \n",
    "    if method == 'zscore':\n",
    "        for col in cols:\n",
    "            mean = normalized[col].mean()\n",
    "            std = normalized[col].std()\n",
    "            normalized[col] = (normalized[col] - mean) / (std + 1e-8)\n",
    "    elif method == 'minmax':\n",
    "        for col in cols:\n",
    "            min_val = normalized[col].min()\n",
    "            max_val = normalized[col].max()\n",
    "            normalized[col] = (normalized[col] - min_val) / (max_val - min_val + 1e-8)\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "if csv_files and continuous_cols:\n",
    "    # Demonstrate normalization\n",
    "    sample_cols = continuous_cols[:3]\n",
    "    normalized_df = normalize_features(df, sample_cols, method='zscore')\n",
    "    \n",
    "    print(\"Before normalization:\")\n",
    "    print(df[sample_cols].describe().round(2))\n",
    "    \n",
    "    print(\"\\nAfter z-score normalization:\")\n",
    "    print(normalized_df.describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. G-code Tokenization (4-Digit Hybrid)\n",
    "\n",
    "The current system uses **4-digit hybrid tokenization** for better numeric precision.\n",
    "\n",
    "### Token Structure\n",
    "\n",
    "Each G-code sequence is tokenized into 7 positions:\n",
    "```\n",
    "[Type, Command, Param, Sign, Digit1, Digit2, Digit3]\n",
    "```\n",
    "\n",
    "- **Type**: SPECIAL, COMMAND, PARAM, NUMERIC (4 classes)\n",
    "- **Command**: G0, G1, G3, G53, M30, NONE (6 classes)\n",
    "- **Param Type**: X, Y, Z, F, R, NONE, etc. (10 classes)\n",
    "- **Sign**: +, -, NONE\n",
    "- **Digits**: 0-9 for each position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-digit hybrid tokenization (used by the model)\n",
    "def tokenize_gcode_4digit(gcode_str):\n",
    "    \"\"\"Tokenize G-code using 4-digit hybrid encoding.\n",
    "    \n",
    "    Returns a dict with token components:\n",
    "    - type: SPECIAL, COMMAND, PARAM, NUMERIC\n",
    "    - command: G0, G1, G3, G53, M30, NONE\n",
    "    - param_type: X, Y, Z, F, R, NONE\n",
    "    - sign: +, -, NONE\n",
    "    - digits: [d1, d2, d3] for numeric values\n",
    "    \"\"\"\n",
    "    if pd.isna(gcode_str) or not isinstance(gcode_str, str):\n",
    "        return {'type': 'SPECIAL', 'command': 'NONE', 'param_type': 'NONE', \n",
    "                'sign': 'NONE', 'digits': [0, 0, 0]}\n",
    "    \n",
    "    # Remove comments\n",
    "    gcode_str = re.sub(r';.*', '', gcode_str)\n",
    "    gcode_str = re.sub(r'\\(.*?\\)', '', gcode_str)\n",
    "    gcode_str = gcode_str.strip().upper()\n",
    "    \n",
    "    result = {\n",
    "        'type': 'SPECIAL',\n",
    "        'command': 'NONE',\n",
    "        'param_type': 'NONE',\n",
    "        'sign': 'NONE',\n",
    "        'digits': [0, 0, 0]\n",
    "    }\n",
    "    \n",
    "    # Match G/M commands\n",
    "    cmd_match = re.match(r'([GM])(\\d+)', gcode_str)\n",
    "    if cmd_match:\n",
    "        result['type'] = 'COMMAND'\n",
    "        cmd_num = int(cmd_match.group(2))\n",
    "        cmd_letter = cmd_match.group(1)\n",
    "        result['command'] = f\"{cmd_letter}{cmd_num}\"\n",
    "        return result\n",
    "    \n",
    "    # Match parameters with values\n",
    "    param_match = re.match(r'([XYZFRS])(-?\\d+\\.?\\d*)', gcode_str)\n",
    "    if param_match:\n",
    "        result['type'] = 'PARAM'\n",
    "        result['param_type'] = param_match.group(1)\n",
    "        \n",
    "        value_str = param_match.group(2)\n",
    "        value = float(value_str)\n",
    "        \n",
    "        # Sign\n",
    "        result['sign'] = '-' if value < 0 else '+'\n",
    "        \n",
    "        # Convert to 3 digits (MMM format for values up to 999)\n",
    "        abs_val = int(abs(value)) % 1000\n",
    "        result['digits'] = [\n",
    "            (abs_val // 100) % 10,\n",
    "            (abs_val // 10) % 10,\n",
    "            abs_val % 10\n",
    "        ]\n",
    "        return result\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test tokenization\n",
    "test_gcodes = [\n",
    "    \"G0\",\n",
    "    \"G1\",\n",
    "    \"X123.5\",\n",
    "    \"Y-45.2\",\n",
    "    \"Z5.0\",\n",
    "    \"F1500\",\n",
    "    \"M30\"\n",
    "]\n",
    "\n",
    "print(\"4-Digit Hybrid Tokenization Examples:\")\n",
    "print(\"=\"*70)\n",
    "for gcode in test_gcodes:\n",
    "    tokens = tokenize_gcode_4digit(gcode)\n",
    "    print(f\"\\nInput: {gcode:12s}\")\n",
    "    print(f\"  Type: {tokens['type']}, Command: {tokens['command']}, \"\n",
    "          f\"Param: {tokens['param_type']}, Sign: {tokens['sign']}, Digits: {tokens['digits']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Vocabulary Management\n",
    "\n",
    "The current vocabulary uses **4-digit hybrid encoding** (`vocabulary_4digit_hybrid.json`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 4-digit hybrid vocabulary\n",
    "vocab_path = project_root / 'data' / 'vocabulary_4digit_hybrid.json'\n",
    "\n",
    "if vocab_path.exists():\n",
    "    with open(vocab_path, 'r') as f:\n",
    "        vocab = json.load(f)\n",
    "    \n",
    "    print(f\"Loaded vocabulary: {vocab_path.name}\")\n",
    "    print(f\"Total entries: {len(vocab)}\")\n",
    "    \n",
    "    # The vocabulary contains mappings for:\n",
    "    # - Types (4): SPECIAL, COMMAND, PARAM, NUMERIC\n",
    "    # - Commands (6): G0, G1, G3, G53, M30, NONE\n",
    "    # - Param types (10+): X, Y, Z, F, R, NONE, etc.\n",
    "    # - Digits (10): 0-9\n",
    "    # - Signs (3): +, -, NONE\n",
    "    \n",
    "    print(\"\\nVocabulary structure:\")\n",
    "    for key, value in list(vocab.items())[:20]:\n",
    "        print(f\"  {key}: {value}\")\n",
    "else:\n",
    "    print(f\"Vocabulary not found: {vocab_path}\")\n",
    "    # Try alternative paths\n",
    "    alt_paths = list(project_root.glob('data/*.json'))\n",
    "    print(f\"Available vocab files: {[p.name for p in alt_paths]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model output head dimensions\n",
    "print(\"Multi-Head Model Output Dimensions:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"  Token Types:   4 classes (SPECIAL, COMMAND, PARAM, NUMERIC)\")\n",
    "print(f\"  Commands:      6 classes (G0, G1, G3, G53, M30, NONE)\")\n",
    "print(f\"  Param Types:  10 classes (X, Y, Z, F, R, NONE, ...)\")\n",
    "print(f\"  Signs:         3 classes (+, -, NONE)\")\n",
    "print(f\"  Digits:       10 classes (0-9) x 3 positions\")\n",
    "print(f\"\\nOperation Types: 9 classes (from frozen encoder)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Window Configuration\n",
    "\n",
    "Configure sliding window parameters for sequence creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window/stride configuration\n",
    "WINDOW_SIZE = 64   # Number of timesteps per sample\n",
    "STRIDE = 16        # Step between windows\n",
    "\n",
    "print(\"Window Configuration:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"  Window size: {WINDOW_SIZE} timesteps\")\n",
    "print(f\"  Stride: {STRIDE} timesteps\")\n",
    "print(f\"  Overlap: {WINDOW_SIZE - STRIDE} timesteps ({(WINDOW_SIZE - STRIDE)/WINDOW_SIZE*100:.0f}%)\")\n",
    "\n",
    "# Calculate samples from a sequence\n",
    "if csv_files:\n",
    "    seq_len = len(df)\n",
    "    n_windows = max(1, (seq_len - WINDOW_SIZE) // STRIDE + 1)\n",
    "    \n",
    "    print(f\"\\nFor sample file ({seq_len} rows):\")\n",
    "    print(f\"  Number of windows: {n_windows}\")\n",
    "    print(f\"  Total samples: ~{n_windows} per file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize windowing\n",
    "def create_windows(data, window_size, stride):\n",
    "    \"\"\"Create sliding windows from data.\"\"\"\n",
    "    windows = []\n",
    "    for start in range(0, len(data) - window_size + 1, stride):\n",
    "        windows.append(data[start:start + window_size])\n",
    "    return windows\n",
    "\n",
    "# Visualize window creation\n",
    "if csv_files and continuous_cols:\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "    \n",
    "    # Get sample sensor data\n",
    "    sensor_col = continuous_cols[0]\n",
    "    full_signal = df[sensor_col].values[:500]  # First 500 points\n",
    "    \n",
    "    # Plot full signal\n",
    "    ax1 = axes[0]\n",
    "    ax1.plot(full_signal, 'b-', linewidth=1, alpha=0.7)\n",
    "    ax1.set_xlabel('Timestep')\n",
    "    ax1.set_ylabel(sensor_col)\n",
    "    ax1.set_title('Full Sensor Signal with Window Positions', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Highlight windows\n",
    "    colors = plt.cm.Set2(np.linspace(0, 1, 5))\n",
    "    for i in range(5):\n",
    "        start = i * STRIDE\n",
    "        end = start + WINDOW_SIZE\n",
    "        if end <= len(full_signal):\n",
    "            ax1.axvspan(start, end, alpha=0.2, color=colors[i], label=f'Window {i+1}')\n",
    "    ax1.legend(loc='upper right', fontsize=9)\n",
    "    \n",
    "    # Plot individual windows\n",
    "    ax2 = axes[1]\n",
    "    windows = create_windows(full_signal, WINDOW_SIZE, STRIDE)[:5]\n",
    "    \n",
    "    for i, window in enumerate(windows):\n",
    "        ax2.plot(window + i*20, color=colors[i], linewidth=1.5, label=f'Window {i+1}')\n",
    "    \n",
    "    ax2.set_xlabel('Position in Window')\n",
    "    ax2.set_ylabel('Value (offset for visibility)')\n",
    "    ax2.set_title('Individual Windows (Offset for Clarity)', fontsize=14, fontweight='bold')\n",
    "    ax2.legend(loc='upper right', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Multilabel Stratified Splitting\n",
    "\n",
    "The current preprocessing uses **multilabel stratified splitting** to ensure proper class coverage across train/val/test sets.\n",
    "\n",
    "### Why Multilabel Stratification?\n",
    "\n",
    "Standard stratification on a single label may not ensure coverage of:\n",
    "- All 9 operation types in each split\n",
    "- All 6 G-code commands in each split\n",
    "- All parameter types in each split\n",
    "\n",
    "Multilabel stratification treats each sample as having multiple labels and ensures balanced distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load split info from preprocessed data\n",
    "split_dirs = [\n",
    "    project_root / 'outputs' / 'multilabel_stratified_splits',\n",
    "    project_root / 'outputs' / 'stratified_splits_v2'\n",
    "]\n",
    "\n",
    "split_info_path = None\n",
    "for sd in split_dirs:\n",
    "    si_path = sd / 'split_info.json'\n",
    "    if si_path.exists():\n",
    "        split_info_path = si_path\n",
    "        break\n",
    "\n",
    "if split_info_path:\n",
    "    with open(split_info_path, 'r') as f:\n",
    "        split_info = json.load(f)\n",
    "    \n",
    "    print(f\"Split Information ({split_info_path.parent.name}):\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Method: {split_info.get('method', 'unknown')}\")\n",
    "    print(f\"\\nSample Counts:\")\n",
    "    print(f\"  Train: {split_info['train_samples']:,} samples ({split_info['train_samples']/(split_info['train_samples']+split_info['val_samples']+split_info['test_samples'])*100:.1f}%)\")\n",
    "    print(f\"  Val:   {split_info['val_samples']:,} samples ({split_info['val_samples']/(split_info['train_samples']+split_info['val_samples']+split_info['test_samples'])*100:.1f}%)\")\n",
    "    print(f\"  Test:  {split_info['test_samples']:,} samples ({split_info['test_samples']/(split_info['train_samples']+split_info['val_samples']+split_info['test_samples'])*100:.1f}%)\")\n",
    "    print(f\"  Total: {split_info['train_samples'] + split_info['val_samples'] + split_info['test_samples']:,} samples\")\n",
    "else:\n",
    "    print(\"No split_info.json found. Run preprocessing first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class coverage across splits\n",
    "if split_info_path and 'coverage_results' in split_info:\n",
    "    coverage = split_info['coverage_results']\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "    \n",
    "    # Operation type distribution\n",
    "    ax1 = axes[0]\n",
    "    splits_data = ['train', 'val', 'test']\n",
    "    x = np.arange(9)  # 9 operation types\n",
    "    width = 0.25\n",
    "    \n",
    "    for i, split in enumerate(splits_data):\n",
    "        op_counts = coverage[split]['operation_counts']\n",
    "        counts = [op_counts.get(str(j), 0) for j in range(9)]\n",
    "        ax1.bar(x + i*width, counts, width, label=split.capitalize())\n",
    "    \n",
    "    ax1.set_xlabel('Operation Type')\n",
    "    ax1.set_ylabel('Count')\n",
    "    ax1.set_title('Operation Type Distribution', fontsize=12, fontweight='bold')\n",
    "    ax1.set_xticks(x + width)\n",
    "    ax1.set_xticklabels([f'Op{i}' for i in range(9)])\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Command distribution\n",
    "    ax2 = axes[1]\n",
    "    commands = ['G0', 'G1', 'G3', 'G53', 'M30', 'NONE']\n",
    "    x = np.arange(len(commands))\n",
    "    \n",
    "    for i, split in enumerate(splits_data):\n",
    "        cmd_counts = coverage[split]['command_counts']\n",
    "        counts = [cmd_counts.get(c, 0) for c in commands]\n",
    "        ax2.bar(x + i*width, counts, width, label=split.capitalize())\n",
    "    \n",
    "    ax2.set_xlabel('Command')\n",
    "    ax2.set_ylabel('Count')\n",
    "    ax2.set_title('Command Distribution', fontsize=12, fontweight='bold')\n",
    "    ax2.set_xticks(x + width)\n",
    "    ax2.set_xticklabels(commands)\n",
    "    ax2.legend()\n",
    "    \n",
    "    # Parameter type distribution\n",
    "    ax3 = axes[2]\n",
    "    params = ['X', 'Y', 'Z', 'F', 'R', 'NONE']\n",
    "    x = np.arange(len(params))\n",
    "    \n",
    "    for i, split in enumerate(splits_data):\n",
    "        param_counts = coverage[split]['param_counts']\n",
    "        counts = [param_counts.get(p, 0) for p in params]\n",
    "        ax3.bar(x + i*width, counts, width, label=split.capitalize())\n",
    "    \n",
    "    ax3.set_xlabel('Parameter Type')\n",
    "    ax3.set_ylabel('Count')\n",
    "    ax3.set_title('Parameter Type Distribution', fontsize=12, fontweight='bold')\n",
    "    ax3.set_xticks(x + width)\n",
    "    ax3.set_xticklabels(params)\n",
    "    ax3.legend()\n",
    "    \n",
    "    plt.suptitle('Multilabel Stratified Split Coverage', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print coverage validation\n",
    "    print(\"\\nCoverage Validation:\")\n",
    "    for split in splits_data:\n",
    "        passed = coverage[split].get('passed', 'Unknown')\n",
    "        issues = coverage[split].get('issues', [])\n",
    "        status = '✓ PASS' if passed else '✗ FAIL'\n",
    "        print(f\"  {split.capitalize()}: {status}\")\n",
    "        if issues:\n",
    "            for issue in issues:\n",
    "                print(f\"    - {issue}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and inspect preprocessed data (NPZ format)\n",
    "split_dir = None\n",
    "for sd in split_dirs:\n",
    "    if (sd / 'train_sequences.npz').exists():\n",
    "        split_dir = sd\n",
    "        break\n",
    "\n",
    "if split_dir:\n",
    "    print(f\"Loading preprocessed data from: {split_dir.name}\")\n",
    "    \n",
    "    # Load train data to inspect structure\n",
    "    train_data = np.load(split_dir / 'train_sequences.npz', allow_pickle=True)\n",
    "    \n",
    "    print(f\"\\nData Structure (NPZ format):\")\n",
    "    print(\"=\"*60)\n",
    "    for key in train_data.files:\n",
    "        arr = train_data[key]\n",
    "        print(f\"  {key:20s}: shape={str(arr.shape):15s} dtype={arr.dtype}\")\n",
    "    \n",
    "    # Key fields:\n",
    "    # - continuous: [N, 64, 155] - continuous sensor features\n",
    "    # - categorical: [N, 64, 4] - categorical sensor features\n",
    "    # - tokens: [N, 7] - tokenized G-code (type, cmd, param, sign, d1, d2, d3)\n",
    "    # - operation_type: [N] - operation class (0-8)\n",
    "    # - gcode_texts: [N] - original G-code strings\n",
    "else:\n",
    "    print(\"No preprocessed data found. Run preprocessing first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Data Quality Validation\n",
    "\n",
    "Validate the preprocessed data for common issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality checks on raw data\n",
    "def validate_data_quality(df):\n",
    "    \"\"\"Run quality checks on data.\"\"\"\n",
    "    issues = []\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_pct = (df.isnull().sum() / len(df) * 100).round(2)\n",
    "    high_missing = missing_pct[missing_pct > 5]\n",
    "    if len(high_missing) > 0:\n",
    "        issues.append(f\"High missing values in: {list(high_missing.index)}\")\n",
    "    \n",
    "    # Check for constant columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    constant_cols = [c for c in numeric_cols if df[c].std() < 1e-8]\n",
    "    if constant_cols:\n",
    "        issues.append(f\"Constant columns: {constant_cols[:5]}\")\n",
    "    \n",
    "    # Check for outliers (>5 std from mean)\n",
    "    outlier_cols = []\n",
    "    for col in numeric_cols:\n",
    "        z_scores = np.abs((df[col] - df[col].mean()) / (df[col].std() + 1e-8))\n",
    "        if (z_scores > 5).sum() > len(df) * 0.01:  # >1% outliers\n",
    "            outlier_cols.append(col)\n",
    "    if outlier_cols:\n",
    "        issues.append(f\"Columns with many outliers: {outlier_cols[:5]}\")\n",
    "    \n",
    "    return issues\n",
    "\n",
    "if csv_files:\n",
    "    print(\"Data Quality Report\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    issues = validate_data_quality(df)\n",
    "    \n",
    "    if issues:\n",
    "        print(\"\\n⚠️  Issues found:\")\n",
    "        for issue in issues:\n",
    "            print(f\"  - {issue}\")\n",
    "    else:\n",
    "        print(\"\\n✓ No major issues found\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\nData Summary:\")\n",
    "    print(f\"  Total rows: {len(df):,}\")\n",
    "    print(f\"  Total columns: {len(df.columns)}\")\n",
    "    print(f\"  Missing values: {df.isnull().sum().sum():,} ({df.isnull().sum().sum()/df.size*100:.2f}%)\")\n",
    "    print(f\"  Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate preprocessed NPZ data\n",
    "if split_dir:\n",
    "    print(\"Preprocessed Data Validation:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for split_name in ['train', 'val', 'test']:\n",
    "        npz_path = split_dir / f'{split_name}_sequences.npz'\n",
    "        if npz_path.exists():\n",
    "            data = np.load(npz_path, allow_pickle=True)\n",
    "            n_samples = len(data['operation_type'])\n",
    "            \n",
    "            # Check for NaN values\n",
    "            has_nan = any(np.isnan(data[k]).any() for k in ['continuous'] if k in data.files)\n",
    "            \n",
    "            # Check operation type distribution\n",
    "            op_types = data['operation_type']\n",
    "            unique_ops = len(np.unique(op_types))\n",
    "            \n",
    "            status = '✓' if not has_nan and unique_ops == 9 else '⚠️'\n",
    "            print(f\"  {split_name:5s}: {n_samples:5,} samples, {unique_ops} operation types {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Pipeline Visualization\n",
    "\n",
    "Visualize the complete preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline diagram\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "ax.set_xlim(0, 16)\n",
    "ax.set_ylim(0, 8)\n",
    "ax.axis('off')\n",
    "\n",
    "# Colors\n",
    "colors = {\n",
    "    'input': '#E8F4F8',\n",
    "    'process': '#D4E6F1',\n",
    "    'output': '#D5F5E3',\n",
    "    'arrow': '#555555'\n",
    "}\n",
    "\n",
    "# Draw pipeline stages\n",
    "stages = [\n",
    "    (1, 6, 'Raw CSV\\nFiles', colors['input']),\n",
    "    (4, 6, 'Load &\\nParse', colors['process']),\n",
    "    (7, 7, 'Sensor\\nFeatures', colors['process']),\n",
    "    (7, 5, '4-Digit\\nTokens', colors['process']),\n",
    "    (10, 6, 'Window\\nCreation', colors['process']),\n",
    "    (13, 6, 'Stratified\\nSplits (NPZ)', colors['output']),\n",
    "]\n",
    "\n",
    "for x, y, text, color in stages:\n",
    "    rect = plt.Rectangle((x-0.8, y-0.6), 1.6, 1.2, facecolor=color, \n",
    "                          edgecolor='black', linewidth=2, zorder=2)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x, y, text, ha='center', va='center', fontsize=10, fontweight='bold', zorder=3)\n",
    "\n",
    "# Draw arrows\n",
    "arrows = [\n",
    "    (1.8, 6, 3.2, 6),\n",
    "    (4.8, 6, 6.2, 7),\n",
    "    (4.8, 6, 6.2, 5),\n",
    "    (7.8, 7, 9.2, 6),\n",
    "    (7.8, 5, 9.2, 6),\n",
    "    (10.8, 6, 12.2, 6),\n",
    "]\n",
    "\n",
    "for x1, y1, x2, y2 in arrows:\n",
    "    ax.annotate('', xy=(x2, y2), xytext=(x1, y1),\n",
    "                arrowprops=dict(arrowstyle='->', color=colors['arrow'], lw=2))\n",
    "\n",
    "# Add details below\n",
    "details = [\n",
    "    (1, 4, '145 CSV files\\n9 operation types'),\n",
    "    (4, 4, 'Pandas\\nDataFrame'),\n",
    "    (7, 3.5, '155 continuous\\n4 categorical'),\n",
    "    (10, 4, f'Window: {WINDOW_SIZE}\\nStride: {STRIDE}'),\n",
    "    (13, 4, '70% / 15% / 15%\\nMultilabel'),\n",
    "]\n",
    "\n",
    "for x, y, text in details:\n",
    "    ax.text(x, y, text, ha='center', va='center', fontsize=9, style='italic', color='gray')\n",
    "\n",
    "ax.set_title('Data Preprocessing Pipeline (4-Digit Hybrid)', fontsize=16, fontweight='bold', y=0.95)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Command-line preprocessing reference\n",
    "print(\"=\"*70)\n",
    "print(\"PREPROCESSING COMMANDS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. Create multilabel stratified splits:\")\n",
    "print(\"-\"*50)\n",
    "print(\"\"\"\n",
    "PYTHONPATH=src .venv/bin/python scripts/create_multilabel_stratified_splits.py \\\\\n",
    "    --data-dir outputs/processed_v2 \\\\\n",
    "    --output-dir outputs/multilabel_stratified_splits \\\\\n",
    "    --val-size 0.15 \\\\\n",
    "    --test-size 0.15 \\\\\n",
    "    --seed 42\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n2. Alternative: Standard stratified splits:\")\n",
    "print(\"-\"*50)\n",
    "print(\"\"\"\n",
    "PYTHONPATH=src .venv/bin/python scripts/create_stratified_splits.py \\\\\n",
    "    --data-dir outputs/processed_v2 \\\\\n",
    "    --output-dir outputs/stratified_splits_v2 \\\\\n",
    "    --val-size 0.15 \\\\\n",
    "    --test-size 0.15\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n3. Output files:\")\n",
    "print(\"-\"*50)\n",
    "print(\"\"\"\n",
    "outputs/multilabel_stratified_splits/\n",
    "├── train_sequences.npz      # Training data\n",
    "├── val_sequences.npz        # Validation data  \n",
    "├── test_sequences.npz       # Test data\n",
    "├── split_info.json          # Split statistics\n",
    "└── split_indices.npz        # Original indices\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **Raw Data Loading**: CSV files with aligned sensor + G-code data\n",
    "2. **Sensor Features**: 155 continuous + 4 categorical features\n",
    "3. **G-code Tokenization**: 4-digit hybrid encoding for better precision\n",
    "4. **Vocabulary**: Multi-head output structure (types, commands, params, digits)\n",
    "5. **Windowing**: Sliding windows with configurable size and stride\n",
    "6. **Data Splits**: Multilabel stratified 70/15/15 train/val/test split\n",
    "7. **Quality Checks**: Coverage validation for all splits\n",
    "\n",
    "### Key Parameters\n",
    "\n",
    "| Parameter | Value | Description |\n",
    "|-----------|-------|-------------|\n",
    "| Window Size | 64 | Timesteps per sample |\n",
    "| Stride | 16 | Step between windows |\n",
    "| Vocabulary | 4-digit hybrid | Multi-head output structure |\n",
    "| Train/Val/Test | 70/15/15 | Multilabel stratified |\n",
    "| Operation Types | 9 | Classification target |\n",
    "| Commands | 6 | G0, G1, G3, G53, M30, NONE |\n",
    "| Param Types | 10 | X, Y, Z, F, R, NONE, etc. |\n",
    "\n",
    "### Output Format (NPZ)\n",
    "\n",
    "| Key | Shape | Description |\n",
    "|-----|-------|-------------|\n",
    "| `continuous` | [N, 64, 155] | Continuous sensor features |\n",
    "| `categorical` | [N, 64, 4] | Categorical sensor features |\n",
    "| `tokens` | [N, 7] | Tokenized G-code |\n",
    "| `operation_type` | [N] | Operation class (0-8) |\n",
    "| `gcode_texts` | [N] | Original G-code strings |\n",
    "\n",
    "---\n",
    "**Navigation:**\n",
    "← [Previous: 01_getting_started](01_getting_started.ipynb) |\n",
    "[Next: 03_training_models](03_training_models.ipynb) →\n",
    "\n",
    "**Related:** [00_raw_data_analysis](00_raw_data_analysis.ipynb) | [08_model_evaluation](08_model_evaluation.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
