{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: Basic Model Training\n",
    "\n",
    "This tutorial demonstrates how to train a G-code fingerprinting model from scratch.\n",
    "\n",
    "**What you'll learn:**\n",
    "- Load and prepare preprocessed data\n",
    "- Configure model hyperparameters\n",
    "- Train a multi-head transformer\n",
    "- Evaluate model performance\n",
    "- Save and load checkpoints\n",
    "\n",
    "**Prerequisites:**\n",
    "- Preprocessed data in `data/preprocessed/`\n",
    "- Python environment with all dependencies installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent / 'src'))\n",
    "\n",
    "from miracle.data.dataset import GCodeDataset\n",
    "from miracle.data.augmentation import DataAugmenter\n",
    "from miracle.model.multihead_transformer import MultiHeadGCodeTransformer\n",
    "from miracle.training.trainer import Trainer\n",
    "from miracle.utils.target_utils import TargetDecomposer\n",
    "from miracle.tokenization.gcode_tokenizer import GCodeTokenizer, TokenizerConfig\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"✓ Imports successful\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Training\n",
    "\n",
    "Set hyperparameters for training. These are reasonable defaults for quick experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "config = {\n",
    "    # Data\n",
    "    \"data_dir\": \"../data/preprocessed\",\n",
    "    \"batch_size\": 8,\n",
    "    \"num_workers\": 4,\n",
    "    \n",
    "    # Model architecture\n",
    "    \"d_model\": 128,\n",
    "    \"nhead\": 8,\n",
    "    \"num_encoder_layers\": 2,\n",
    "    \"num_decoder_layers\": 2,\n",
    "    \"dim_feedforward\": 512,\n",
    "    \"dropout\": 0.1,\n",
    "    \n",
    "    # Training\n",
    "    \"epochs\": 30,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"warmup_epochs\": 5,\n",
    "    \n",
    "    # Data augmentation\n",
    "    \"augmentation\": True,\n",
    "    \"oversampling_factor\": 3,\n",
    "    \"noise_level\": 0.02,\n",
    "    \n",
    "    # Loss weights\n",
    "    \"type_weight\": 1.0,\n",
    "    \"command_weight\": 3.0,\n",
    "    \"param_type_weight\": 2.0,\n",
    "    \"param_value_weight\": 2.0,\n",
    "    \n",
    "    # Output\n",
    "    \"output_dir\": \"outputs/tutorial_01\",\n",
    "    \"save_every\": 5,\n",
    "}\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(config[\"output_dir\"], exist_ok=True)\n",
    "\n",
    "# Save config\n",
    "with open(f\"{config['output_dir']}/config.json\", 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"✓ Configuration set\")\n",
    "print(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Tokenizer and Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer_config = TokenizerConfig(bucket_digits=2)  # 2-digit bucketing\n",
    "tokenizer = GCodeTokenizer(tokenizer_config)\n",
    "\n",
    "# Load vocabulary from preprocessed data\n",
    "vocab_path = Path(config[\"data_dir\"]) / \"vocabulary_v2.json\"\n",
    "tokenizer.load(vocab_path)\n",
    "\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "print(f\"✓ Loaded vocabulary: {vocab_size} tokens\")\n",
    "print(f\"  Bucket digits: {tokenizer.config.bucket_digits}\")\n",
    "print(f\"  Special tokens: PAD={tokenizer.pad_token_id}, UNK={tokenizer.unk_token_id}\")\n",
    "\n",
    "# Show sample tokens\n",
    "sample_tokens = list(tokenizer.vocab.keys())[:10]\n",
    "print(f\"\\nSample tokens: {sample_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize target decomposer\n",
    "decomposer = TargetDecomposer(tokenizer.vocab)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = GCodeDataset(\n",
    "    data_dir=config[\"data_dir\"],\n",
    "    split=\"train\",\n",
    "    vocab=tokenizer.vocab,\n",
    "    decomposer=decomposer,\n",
    ")\n",
    "\n",
    "val_dataset = GCodeDataset(\n",
    "    data_dir=config[\"data_dir\"],\n",
    "    split=\"val\",\n",
    "    vocab=tokenizer.vocab,\n",
    "    decomposer=decomposer,\n",
    ")\n",
    "\n",
    "test_dataset = GCodeDataset(\n",
    "    data_dir=config[\"data_dir\"],\n",
    "    split=\"test\",\n",
    "    vocab=tokenizer.vocab,\n",
    "    decomposer=decomposer,\n",
    ")\n",
    "\n",
    "print(f\"✓ Datasets loaded:\")\n",
    "print(f\"  Train: {len(train_dataset)} samples\")\n",
    "print(f\"  Val:   {len(val_dataset)} samples\")\n",
    "print(f\"  Test:  {len(test_dataset)} samples\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=config[\"num_workers\"],\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    num_workers=config[\"num_workers\"],\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    num_workers=config[\"num_workers\"],\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Data loaders created\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches:   {len(val_loader)}\")\n",
    "print(f\"  Test batches:  {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inspect Sample Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sample batch\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "print(\"Sample batch contents:\")\n",
    "print(f\"  continuous shape: {batch['continuous'].shape}\")\n",
    "print(f\"  categorical shape: {batch['categorical'].shape}\")\n",
    "print(f\"  tokens shape: {batch['tokens'].shape}\")\n",
    "print(f\"  type_ids shape: {batch['type_ids'].shape}\")\n",
    "print(f\"  command_ids shape: {batch['command_ids'].shape}\")\n",
    "print(f\"  param_type_ids shape: {batch['param_type_ids'].shape}\")\n",
    "print(f\"  param_value_ids shape: {batch['param_value_ids'].shape}\")\n",
    "\n",
    "# Visualize first sample's sensor data\n",
    "continuous_sample = batch['continuous'][0].numpy()  # Shape: [T, 135]\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 6))\n",
    "\n",
    "# Plot first 20 channels\n",
    "axes[0].plot(continuous_sample[:, :20])\n",
    "axes[0].set_title(\"Sample Sensor Data (First 20 Channels)\")\n",
    "axes[0].set_xlabel(\"Time Step\")\n",
    "axes[0].set_ylabel(\"Normalized Value\")\n",
    "axes[0].legend([f\"Ch{i}\" for i in range(20)], ncol=10, loc='upper right', fontsize=8)\n",
    "\n",
    "# Plot all channels as heatmap\n",
    "im = axes[1].imshow(continuous_sample.T, aspect='auto', cmap='viridis')\n",
    "axes[1].set_title(\"All Sensor Channels (Heatmap)\")\n",
    "axes[1].set_xlabel(\"Time Step\")\n",
    "axes[1].set_ylabel(\"Channel\")\n",
    "plt.colorbar(im, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Sample batch inspected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create model\n",
    "model = MultiHeadGCodeTransformer(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=config[\"d_model\"],\n",
    "    nhead=config[\"nhead\"],\n",
    "    num_encoder_layers=config[\"num_encoder_layers\"],\n",
    "    num_decoder_layers=config[\"num_decoder_layers\"],\n",
    "    dim_feedforward=config[\"dim_feedforward\"],\n",
    "    dropout=config[\"dropout\"],\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\n✓ Model created\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Model size: {total_params * 4 / (1024**2):.2f} MB (FP32)\")\n",
    "\n",
    "# Print model summary\n",
    "print(\"\\nModel architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Setup Optimizer and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config[\"learning_rate\"],\n",
    "    weight_decay=config[\"weight_decay\"],\n",
    ")\n",
    "\n",
    "# Learning rate scheduler (cosine annealing with warmup)\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "scheduler = CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=config[\"epochs\"] - config[\"warmup_epochs\"],\n",
    ")\n",
    "\n",
    "print(\"✓ Optimizer and scheduler created\")\n",
    "print(f\"  Optimizer: AdamW\")\n",
    "print(f\"  Learning rate: {config['learning_rate']}\")\n",
    "print(f\"  Weight decay: {config['weight_decay']}\")\n",
    "print(f\"  Scheduler: CosineAnnealing\")\n",
    "print(f\"  Warmup epochs: {config['warmup_epochs']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Define Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(logits, targets, weights):\n",
    "    \"\"\"\n",
    "    Compute weighted multi-head loss.\n",
    "    \n",
    "    Args:\n",
    "        logits: Tuple of (type_logits, command_logits, param_type_logits, param_value_logits)\n",
    "        targets: Tuple of (type_ids, command_ids, param_type_ids, param_value_ids)\n",
    "        weights: Dictionary of loss weights\n",
    "    \n",
    "    Returns:\n",
    "        Total loss, dict of individual losses\n",
    "    \"\"\"\n",
    "    type_logits, command_logits, param_type_logits, param_value_logits = logits\n",
    "    type_ids, command_ids, param_type_ids, param_value_ids = targets\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Reshape for loss computation\n",
    "    B, T = type_ids.shape\n",
    "    \n",
    "    type_loss = criterion(\n",
    "        type_logits.view(B * T, -1),\n",
    "        type_ids.view(B * T)\n",
    "    )\n",
    "    \n",
    "    command_loss = criterion(\n",
    "        command_logits.view(B * T, -1),\n",
    "        command_ids.view(B * T)\n",
    "    )\n",
    "    \n",
    "    param_type_loss = criterion(\n",
    "        param_type_logits.view(B * T, -1),\n",
    "        param_type_ids.view(B * T)\n",
    "    )\n",
    "    \n",
    "    param_value_loss = criterion(\n",
    "        param_value_logits.view(B * T, -1),\n",
    "        param_value_ids.view(B * T)\n",
    "    )\n",
    "    \n",
    "    # Weighted sum\n",
    "    total_loss = (\n",
    "        weights['type_weight'] * type_loss +\n",
    "        weights['command_weight'] * command_loss +\n",
    "        weights['param_type_weight'] * param_type_loss +\n",
    "        weights['param_value_weight'] * param_value_loss\n",
    "    )\n",
    "    \n",
    "    losses = {\n",
    "        'total': total_loss.item(),\n",
    "        'type': type_loss.item(),\n",
    "        'command': command_loss.item(),\n",
    "        'param_type': param_type_loss.item(),\n",
    "        'param_value': param_value_loss.item(),\n",
    "    }\n",
    "    \n",
    "    return total_loss, losses\n",
    "\n",
    "print(\"✓ Loss function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Loop\n",
    "\n",
    "Train the model for the specified number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'learning_rate': [],\n",
    "}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Loss weights\n",
    "loss_weights = {\n",
    "    'type_weight': config['type_weight'],\n",
    "    'command_weight': config['command_weight'],\n",
    "    'param_type_weight': config['param_type_weight'],\n",
    "    'param_value_weight': config['param_value_weight'],\n",
    "}\n",
    "\n",
    "print(\"Starting training...\\n\")\n",
    "\n",
    "for epoch in range(config['epochs']):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['epochs']} [Train]\")\n",
    "    \n",
    "    for batch in pbar:\n",
    "        continuous = batch['continuous'].to(device)\n",
    "        categorical = batch['categorical'].to(device)\n",
    "        \n",
    "        targets = (\n",
    "            batch['type_ids'].to(device),\n",
    "            batch['command_ids'].to(device),\n",
    "            batch['param_type_ids'].to(device),\n",
    "            batch['param_value_ids'].to(device),\n",
    "        )\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(continuous, categorical)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss, loss_dict = compute_loss(logits, targets, loss_weights)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{config['epochs']} [Val]  \"):\n",
    "            continuous = batch['continuous'].to(device)\n",
    "            categorical = batch['categorical'].to(device)\n",
    "            \n",
    "            targets = (\n",
    "                batch['type_ids'].to(device),\n",
    "                batch['command_ids'].to(device),\n",
    "                batch['param_type_ids'].to(device),\n",
    "                batch['param_value_ids'].to(device),\n",
    "            )\n",
    "            \n",
    "            logits = model(continuous, categorical)\n",
    "            loss, _ = compute_loss(logits, targets, loss_weights)\n",
    "            val_losses.append(loss.item())\n",
    "    \n",
    "    # Update learning rate\n",
    "    if epoch >= config['warmup_epochs']:\n",
    "        scheduler.step()\n",
    "    \n",
    "    # Compute epoch metrics\n",
    "    train_loss = np.mean(train_losses)\n",
    "    val_loss = np.mean(val_losses)\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['learning_rate'].append(lr)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\nEpoch {epoch+1}/{config['epochs']}:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Val Loss:   {val_loss:.4f}\")\n",
    "    print(f\"  LR:         {lr:.6f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model': model,\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'config': config,\n",
    "        }\n",
    "        torch.save(checkpoint, f\"{config['output_dir']}/checkpoint_best.pt\")\n",
    "        print(f\"  ✓ Saved best model (val_loss={val_loss:.4f})\")\n",
    "    \n",
    "    # Save periodic checkpoint\n",
    "    if (epoch + 1) % config['save_every'] == 0:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model': model,\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'config': config,\n",
    "        }\n",
    "        torch.save(checkpoint, f\"{config['output_dir']}/checkpoint_epoch{epoch+1}.pt\")\n",
    "\n",
    "print(\"\\n✓ Training complete!\")\n",
    "print(f\"  Best validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss curves\n",
    "epochs = range(1, len(history['train_loss']) + 1)\n",
    "axes[0].plot(epochs, history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(epochs, history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Learning rate\n",
    "axes[1].plot(epochs, history['learning_rate'], color='green', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Learning Rate')\n",
    "axes[1].set_title('Learning Rate Schedule')\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{config['output_dir']}/training_history.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Training curves saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best checkpoint\n",
    "checkpoint = torch.load(f\"{config['output_dir']}/checkpoint_best.pt\")\n",
    "model = checkpoint['model']\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loaded best model from epoch {checkpoint['epoch']+1}\")\n",
    "print(f\"Validation loss: {checkpoint['val_loss']:.4f}\\n\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_losses = []\n",
    "all_type_preds = []\n",
    "all_type_targets = []\n",
    "all_command_preds = []\n",
    "all_command_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Evaluating on test set\"):\n",
    "        continuous = batch['continuous'].to(device)\n",
    "        categorical = batch['categorical'].to(device)\n",
    "        \n",
    "        targets = (\n",
    "            batch['type_ids'].to(device),\n",
    "            batch['command_ids'].to(device),\n",
    "            batch['param_type_ids'].to(device),\n",
    "            batch['param_value_ids'].to(device),\n",
    "        )\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(continuous, categorical)\n",
    "        loss, _ = compute_loss(logits, targets, loss_weights)\n",
    "        test_losses.append(loss.item())\n",
    "        \n",
    "        # Collect predictions\n",
    "        type_preds = torch.argmax(logits[0], dim=-1)\n",
    "        command_preds = torch.argmax(logits[1], dim=-1)\n",
    "        \n",
    "        all_type_preds.append(type_preds.cpu())\n",
    "        all_type_targets.append(targets[0].cpu())\n",
    "        all_command_preds.append(command_preds.cpu())\n",
    "        all_command_targets.append(targets[1].cpu())\n",
    "\n",
    "# Compute test metrics\n",
    "test_loss = np.mean(test_losses)\n",
    "\n",
    "all_type_preds = torch.cat(all_type_preds).flatten()\n",
    "all_type_targets = torch.cat(all_type_targets).flatten()\n",
    "all_command_preds = torch.cat(all_command_preds).flatten()\n",
    "all_command_targets = torch.cat(all_command_targets).flatten()\n",
    "\n",
    "type_accuracy = (all_type_preds == all_type_targets).float().mean() * 100\n",
    "command_accuracy = (all_command_preds == all_command_targets).float().mean() * 100\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Test Loss:         {test_loss:.4f}\")\n",
    "print(f\"Type Accuracy:     {type_accuracy:.2f}%\")\n",
    "print(f\"Command Accuracy:  {command_accuracy:.2f}%\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary\n",
    "\n",
    "In this tutorial, you learned how to:\n",
    "\n",
    "✓ Load preprocessed sensor data and G-code tokens  \n",
    "✓ Configure a multi-head transformer model  \n",
    "✓ Train with weighted multi-task loss  \n",
    "✓ Monitor training progress and save checkpoints  \n",
    "✓ Evaluate model performance on test set  \n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Tutorial 2**: Create custom datasets\n",
    "- **Tutorial 3**: Advanced data augmentation\n",
    "- **Tutorial 4**: Export models to ONNX\n",
    "- **Tutorial 5**: Deploy with Docker\n",
    "\n",
    "### Improving Results\n",
    "\n",
    "To improve model performance:\n",
    "1. Increase model size (`d_model=256`, more layers)\n",
    "2. Train for more epochs (100+)\n",
    "3. Run hyperparameter sweeps (see HYPERPARAMETER_TUNING.md)\n",
    "4. Use better vocabulary (3-digit bucketing)\n",
    "5. Tune loss weights for parameter heads\n",
    "\n",
    "See the documentation for detailed guides!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
