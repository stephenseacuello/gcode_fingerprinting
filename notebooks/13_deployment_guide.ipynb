{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Deployment Guide\n",
    "\n",
    "Production deployment strategies including ONNX export, quantization, and containerization.\n",
    "\n",
    "## Contents\n",
    "1. [Setup](#1-setup)\n",
    "2. [ONNX Export](#2-onnx-export)\n",
    "3. [Model Quantization](#3-model-quantization)\n",
    "4. [TorchScript Compilation](#4-torchscript-compilation)\n",
    "5. [Docker Containerization](#5-docker-containerization)\n",
    "6. [Performance Benchmarking](#6-performance-benchmarking)\n",
    "7. [Deployment Configurations](#7-deployment-configurations)\n",
    "8. [Monitoring & Logging](#8-monitoring-and-logging)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root / 'src'))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "from typing import Dict, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Environment check\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Device: {'MPS' if torch.backends.mps.is_available() else 'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
    "\n",
    "# Check optional dependencies\n",
    "try:\n",
    "    import onnx\n",
    "    import onnxruntime\n",
    "    print(f\"ONNX: {onnx.__version__}\")\n",
    "    print(f\"ONNX Runtime: {onnxruntime.__version__}\")\n",
    "    ONNX_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"ONNX not installed. Run: pip install onnx onnxruntime\")\n",
    "    ONNX_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "from miracle.model.backbone import MMDTAELSTMBackbone\n",
    "from miracle.model.multihead_lm import MultiHeadGCodeLM\n",
    "\n",
    "# Paths\n",
    "VOCAB_PATH = project_root / 'data' / 'gcode_vocab_v2.json'\n",
    "CHECKPOINT_PATH = project_root / 'outputs' / 'final_model' / 'checkpoint_best.pt'\n",
    "EXPORT_DIR = project_root / 'exports'\n",
    "EXPORT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Load vocabulary\n",
    "with open(VOCAB_PATH) as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocab['token_to_id'])}\")\n",
    "\n",
    "# Load checkpoint\n",
    "device = torch.device('cpu')  # Use CPU for export\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    checkpoint = torch.load(CHECKPOINT_PATH, map_location=device, weights_only=False)\n",
    "    config = checkpoint.get('config', {})\n",
    "    print(f\"Loaded checkpoint from epoch {checkpoint.get('epoch', 'unknown')}\")\n",
    "else:\n",
    "    print(f\"Checkpoint not found, using default config\")\n",
    "    config = {'hidden_dim': 256, 'num_layers': 4, 'num_heads': 8, 'dropout': 0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "backbone = MMDTAELSTMBackbone(\n",
    "    continuous_dim=155,\n",
    "    categorical_dims=[10, 10, 50, 50],\n",
    "    d_model=config.get('hidden_dim', 256),\n",
    "    num_layers=config.get('num_layers', 4),\n",
    "    num_heads=config.get('num_heads', 8),\n",
    "    dropout=0.0  # Disable dropout for inference\n",
    ").to(device)\n",
    "\n",
    "lm = MultiHeadGCodeLM(\n",
    "    d_model=config.get('hidden_dim', 256),\n",
    "    vocab_sizes=vocab.get('head_vocab_sizes', {'type': 10, 'command': 50, 'param_type': 30, 'param_value': 100})\n",
    ").to(device)\n",
    "\n",
    "# Load weights if available\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    backbone.load_state_dict(checkpoint['backbone_state_dict'])\n",
    "    lm.load_state_dict(checkpoint['lm_state_dict'])\n",
    "    print(\"Model weights loaded\")\n",
    "\n",
    "backbone.eval()\n",
    "lm.eval()\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in backbone.parameters()) + sum(p.numel() for p in lm.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ONNX Export\n",
    "\n",
    "Export the model to ONNX format for cross-platform deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedModel(nn.Module):\n",
    "    \"\"\"Combined backbone + LM for single-model export.\"\"\"\n",
    "    \n",
    "    def __init__(self, backbone, lm):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.lm = lm\n",
    "        \n",
    "    def forward(self, continuous, categorical):\n",
    "        \"\"\"Forward pass returning logits for all heads.\"\"\"\n",
    "        hidden = self.backbone(continuous, categorical)\n",
    "        outputs = self.lm(hidden)\n",
    "        # Return as tuple for ONNX compatibility\n",
    "        return (\n",
    "            outputs['type'],\n",
    "            outputs['command'],\n",
    "            outputs['param_type'],\n",
    "            outputs['param_value']\n",
    "        )\n",
    "\n",
    "# Create combined model\n",
    "combined_model = CombinedModel(backbone, lm)\n",
    "combined_model.eval()\n",
    "\n",
    "print(\"Combined model created for export\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ONNX_AVAILABLE:\n",
    "    # Create dummy inputs\n",
    "    batch_size = 1\n",
    "    seq_length = 64\n",
    "    \n",
    "    dummy_continuous = torch.randn(batch_size, seq_length, 155)\n",
    "    dummy_categorical = torch.randint(0, 10, (batch_size, seq_length, 4))\n",
    "    \n",
    "    # Export to ONNX\n",
    "    onnx_path = EXPORT_DIR / 'gcode_model.onnx'\n",
    "    \n",
    "    torch.onnx.export(\n",
    "        combined_model,\n",
    "        (dummy_continuous, dummy_categorical),\n",
    "        str(onnx_path),\n",
    "        input_names=['continuous', 'categorical'],\n",
    "        output_names=['type_logits', 'command_logits', 'param_type_logits', 'param_value_logits'],\n",
    "        dynamic_axes={\n",
    "            'continuous': {0: 'batch_size', 1: 'seq_length'},\n",
    "            'categorical': {0: 'batch_size', 1: 'seq_length'},\n",
    "            'type_logits': {0: 'batch_size', 1: 'seq_length'},\n",
    "            'command_logits': {0: 'batch_size', 1: 'seq_length'},\n",
    "            'param_type_logits': {0: 'batch_size', 1: 'seq_length'},\n",
    "            'param_value_logits': {0: 'batch_size', 1: 'seq_length'},\n",
    "        },\n",
    "        opset_version=14,\n",
    "        do_constant_folding=True,\n",
    "    )\n",
    "    \n",
    "    print(f\"ONNX model exported to: {onnx_path}\")\n",
    "    print(f\"File size: {onnx_path.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "    \n",
    "    # Verify ONNX model\n",
    "    onnx_model = onnx.load(str(onnx_path))\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "    print(\"ONNX model verification passed!\")\n",
    "else:\n",
    "    print(\"ONNX export skipped - install onnx package\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ONNX inference\n",
    "if ONNX_AVAILABLE:\n",
    "    import onnxruntime as ort\n",
    "    \n",
    "    # Create ONNX Runtime session\n",
    "    ort_session = ort.InferenceSession(str(onnx_path))\n",
    "    \n",
    "    # Test inference\n",
    "    test_continuous = np.random.randn(1, 64, 155).astype(np.float32)\n",
    "    test_categorical = np.random.randint(0, 10, (1, 64, 4)).astype(np.int64)\n",
    "    \n",
    "    ort_inputs = {\n",
    "        'continuous': test_continuous,\n",
    "        'categorical': test_categorical\n",
    "    }\n",
    "    \n",
    "    # Run inference\n",
    "    start = time.time()\n",
    "    ort_outputs = ort_session.run(None, ort_inputs)\n",
    "    onnx_time = (time.time() - start) * 1000\n",
    "    \n",
    "    print(f\"ONNX inference time: {onnx_time:.2f} ms\")\n",
    "    print(f\"Output shapes:\")\n",
    "    for i, name in enumerate(['type', 'command', 'param_type', 'param_value']):\n",
    "        print(f\"  {name}: {ort_outputs[i].shape}\")\n",
    "    \n",
    "    # Compare with PyTorch\n",
    "    with torch.no_grad():\n",
    "        start = time.time()\n",
    "        pt_outputs = combined_model(\n",
    "            torch.tensor(test_continuous),\n",
    "            torch.tensor(test_categorical)\n",
    "        )\n",
    "        pt_time = (time.time() - start) * 1000\n",
    "    \n",
    "    print(f\"\\nPyTorch inference time: {pt_time:.2f} ms\")\n",
    "    \n",
    "    # Check output similarity\n",
    "    for i, name in enumerate(['type', 'command', 'param_type', 'param_value']):\n",
    "        diff = np.abs(ort_outputs[i] - pt_outputs[i].numpy()).max()\n",
    "        print(f\"  {name} max diff: {diff:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Quantization\n",
    "\n",
    "Reduce model size and improve inference speed with quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic quantization (post-training)\n",
    "def quantize_model_dynamic(model):\n",
    "    \"\"\"Apply dynamic quantization to reduce model size.\"\"\"\n",
    "    quantized = torch.quantization.quantize_dynamic(\n",
    "        model,\n",
    "        {nn.Linear, nn.LSTM},  # Layers to quantize\n",
    "        dtype=torch.qint8\n",
    "    )\n",
    "    return quantized\n",
    "\n",
    "# Quantize the combined model\n",
    "quantized_model = quantize_model_dynamic(combined_model)\n",
    "\n",
    "# Save quantized model\n",
    "quantized_path = EXPORT_DIR / 'gcode_model_quantized.pt'\n",
    "torch.save(quantized_model.state_dict(), quantized_path)\n",
    "\n",
    "# Compare sizes\n",
    "original_size = sum(p.numel() * p.element_size() for p in combined_model.parameters()) / 1024 / 1024\n",
    "quantized_size = quantized_path.stat().st_size / 1024 / 1024\n",
    "\n",
    "print(f\"Original model size (estimated): {original_size:.2f} MB\")\n",
    "print(f\"Quantized model size: {quantized_size:.2f} MB\")\n",
    "print(f\"Compression ratio: {original_size / quantized_size:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark quantized model\n",
    "def benchmark_model(model, continuous, categorical, num_runs=100, warmup=10):\n",
    "    \"\"\"Benchmark model inference speed.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Warmup\n",
    "    with torch.no_grad():\n",
    "        for _ in range(warmup):\n",
    "            _ = model(continuous, categorical)\n",
    "    \n",
    "    # Benchmark\n",
    "    times = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_runs):\n",
    "            start = time.time()\n",
    "            _ = model(continuous, categorical)\n",
    "            times.append((time.time() - start) * 1000)\n",
    "    \n",
    "    return {\n",
    "        'mean_ms': np.mean(times),\n",
    "        'std_ms': np.std(times),\n",
    "        'min_ms': np.min(times),\n",
    "        'max_ms': np.max(times),\n",
    "        'p95_ms': np.percentile(times, 95)\n",
    "    }\n",
    "\n",
    "# Test inputs\n",
    "test_continuous = torch.randn(1, 64, 155)\n",
    "test_categorical = torch.randint(0, 10, (1, 64, 4))\n",
    "\n",
    "# Benchmark both models\n",
    "print(\"Benchmarking inference speed...\")\n",
    "print(\"\\nOriginal Model:\")\n",
    "original_stats = benchmark_model(combined_model, test_continuous, test_categorical)\n",
    "for key, value in original_stats.items():\n",
    "    print(f\"  {key}: {value:.3f}\")\n",
    "\n",
    "print(\"\\nQuantized Model:\")\n",
    "quantized_stats = benchmark_model(quantized_model, test_continuous, test_categorical)\n",
    "for key, value in quantized_stats.items():\n",
    "    print(f\"  {key}: {value:.3f}\")\n",
    "\n",
    "speedup = original_stats['mean_ms'] / quantized_stats['mean_ms']\n",
    "print(f\"\\nSpeedup: {speedup:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. TorchScript Compilation\n",
    "\n",
    "Compile to TorchScript for optimized inference and C++ deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script the model (preferred over trace for models with control flow)\n",
    "class ScriptableCombinedModel(nn.Module):\n",
    "    \"\"\"TorchScript-compatible combined model.\"\"\"\n",
    "    \n",
    "    def __init__(self, backbone, lm):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.lm = lm\n",
    "        \n",
    "    def forward(self, continuous: torch.Tensor, categorical: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Forward pass with explicit type annotations.\"\"\"\n",
    "        hidden = self.backbone(continuous, categorical)\n",
    "        outputs = self.lm(hidden)\n",
    "        return (\n",
    "            outputs['type'],\n",
    "            outputs['command'],\n",
    "            outputs['param_type'],\n",
    "            outputs['param_value']\n",
    "        )\n",
    "\n",
    "# Create scriptable model\n",
    "scriptable_model = ScriptableCombinedModel(backbone, lm)\n",
    "scriptable_model.eval()\n",
    "\n",
    "# Trace the model (simpler, works for most cases)\n",
    "traced_model = torch.jit.trace(\n",
    "    scriptable_model,\n",
    "    (test_continuous, test_categorical)\n",
    ")\n",
    "\n",
    "# Save traced model\n",
    "traced_path = EXPORT_DIR / 'gcode_model_traced.pt'\n",
    "traced_model.save(str(traced_path))\n",
    "\n",
    "print(f\"TorchScript model saved to: {traced_path}\")\n",
    "print(f\"File size: {traced_path.stat().st_size / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and test TorchScript model\n",
    "loaded_traced = torch.jit.load(str(traced_path))\n",
    "loaded_traced.eval()\n",
    "\n",
    "# Verify outputs match\n",
    "with torch.no_grad():\n",
    "    original_out = scriptable_model(test_continuous, test_categorical)\n",
    "    traced_out = loaded_traced(test_continuous, test_categorical)\n",
    "\n",
    "print(\"Output comparison (original vs traced):\")\n",
    "for i, name in enumerate(['type', 'command', 'param_type', 'param_value']):\n",
    "    diff = (original_out[i] - traced_out[i]).abs().max().item()\n",
    "    print(f\"  {name}: max diff = {diff:.8f}\")\n",
    "\n",
    "# Benchmark TorchScript\n",
    "print(\"\\nTorchScript Model Benchmark:\")\n",
    "traced_stats = benchmark_model(loaded_traced, test_continuous, test_categorical)\n",
    "for key, value in traced_stats.items():\n",
    "    print(f\"  {key}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Docker Containerization\n",
    "\n",
    "Create Docker configuration for production deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Dockerfile\n",
    "dockerfile_content = '''# G-Code Fingerprinting Model - Production Dockerfile\n",
    "FROM python:3.9-slim\n",
    "\n",
    "# Set working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Install system dependencies\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends \\\\\n",
    "    build-essential \\\\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Copy requirements first for layer caching\n",
    "COPY requirements-prod.txt .\n",
    "RUN pip install --no-cache-dir -r requirements-prod.txt\n",
    "\n",
    "# Copy application code\n",
    "COPY src/ ./src/\n",
    "COPY exports/ ./exports/\n",
    "COPY data/gcode_vocab_v2.json ./data/\n",
    "\n",
    "# Set environment variables\n",
    "ENV PYTHONPATH=/app/src\n",
    "ENV MODEL_PATH=/app/exports/gcode_model_traced.pt\n",
    "ENV VOCAB_PATH=/app/data/gcode_vocab_v2.json\n",
    "ENV PORT=8000\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 8000\n",
    "\n",
    "# Health check\n",
    "HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \\\\\n",
    "    CMD curl -f http://localhost:8000/health || exit 1\n",
    "\n",
    "# Run server\n",
    "CMD [\"uvicorn\", \"miracle.api.server:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "'''\n",
    "\n",
    "# Save Dockerfile\n",
    "dockerfile_path = EXPORT_DIR / 'Dockerfile'\n",
    "dockerfile_path.write_text(dockerfile_content)\n",
    "print(f\"Dockerfile saved to: {dockerfile_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate production requirements\n",
    "requirements_prod = '''# Production dependencies for G-code fingerprinting model\n",
    "torch>=2.0.0\n",
    "numpy>=1.24.0\n",
    "fastapi>=0.100.0\n",
    "uvicorn[standard]>=0.23.0\n",
    "pydantic>=2.0.0\n",
    "python-multipart>=0.0.6\n",
    "'''\n",
    "\n",
    "requirements_path = EXPORT_DIR / 'requirements-prod.txt'\n",
    "requirements_path.write_text(requirements_prod)\n",
    "print(f\"Production requirements saved to: {requirements_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate docker-compose.yml\n",
    "docker_compose = '''version: \"3.8\"\n",
    "\n",
    "services:\n",
    "  gcode-api:\n",
    "    build:\n",
    "      context: ..\n",
    "      dockerfile: exports/Dockerfile\n",
    "    ports:\n",
    "      - \"8000:8000\"\n",
    "    environment:\n",
    "      - MODEL_PATH=/app/exports/gcode_model_traced.pt\n",
    "      - VOCAB_PATH=/app/data/gcode_vocab_v2.json\n",
    "      - LOG_LEVEL=info\n",
    "    volumes:\n",
    "      - ../logs:/app/logs\n",
    "    restart: unless-stopped\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health\"]\n",
    "      interval: 30s\n",
    "      timeout: 10s\n",
    "      retries: 3\n",
    "    deploy:\n",
    "      resources:\n",
    "        limits:\n",
    "          memory: 2G\n",
    "        reservations:\n",
    "          memory: 1G\n",
    "\n",
    "  # Optional: Redis for caching\n",
    "  redis:\n",
    "    image: redis:7-alpine\n",
    "    ports:\n",
    "      - \"6379:6379\"\n",
    "    volumes:\n",
    "      - redis_data:/data\n",
    "    restart: unless-stopped\n",
    "\n",
    "volumes:\n",
    "  redis_data:\n",
    "'''\n",
    "\n",
    "compose_path = EXPORT_DIR / 'docker-compose.yml'\n",
    "compose_path.write_text(docker_compose)\n",
    "print(f\"Docker Compose saved to: {compose_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print deployment instructions\n",
    "print(\"\"\"\n",
    "=== Docker Deployment Instructions ===\n",
    "\n",
    "1. Build the Docker image:\n",
    "   cd exports\n",
    "   docker build -t gcode-fingerprint:latest -f Dockerfile ..\n",
    "\n",
    "2. Run with Docker:\n",
    "   docker run -p 8000:8000 gcode-fingerprint:latest\n",
    "\n",
    "3. Run with Docker Compose:\n",
    "   docker-compose up -d\n",
    "\n",
    "4. Test the API:\n",
    "   curl http://localhost:8000/health\n",
    "\n",
    "5. View logs:\n",
    "   docker-compose logs -f gcode-api\n",
    "\n",
    "6. Stop services:\n",
    "   docker-compose down\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Benchmarking\n",
    "\n",
    "Comprehensive performance comparison across deployment options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def comprehensive_benchmark(models_dict, test_inputs, num_runs=100):\n",
    "    \"\"\"Benchmark multiple models.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models_dict.items():\n",
    "        print(f\"Benchmarking {name}...\")\n",
    "        results[name] = benchmark_model(model, *test_inputs, num_runs=num_runs)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Models to benchmark\n",
    "models_to_test = {\n",
    "    'PyTorch (float32)': combined_model,\n",
    "    'Quantized (int8)': quantized_model,\n",
    "    'TorchScript': loaded_traced,\n",
    "}\n",
    "\n",
    "# Run benchmarks\n",
    "benchmark_results = comprehensive_benchmark(\n",
    "    models_to_test,\n",
    "    (test_continuous, test_categorical),\n",
    "    num_runs=100\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BENCHMARK RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Model':<25} {'Mean (ms)':<12} {'Std (ms)':<12} {'P95 (ms)':<12}\")\n",
    "print(\"-\"*60)\n",
    "for name, stats in benchmark_results.items():\n",
    "    print(f\"{name:<25} {stats['mean_ms']:<12.3f} {stats['std_ms']:<12.3f} {stats['p95_ms']:<12.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize benchmark results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Inference time comparison\n",
    "names = list(benchmark_results.keys())\n",
    "means = [benchmark_results[n]['mean_ms'] for n in names]\n",
    "stds = [benchmark_results[n]['std_ms'] for n in names]\n",
    "\n",
    "bars = axes[0].bar(names, means, yerr=stds, capsize=5, color=['steelblue', 'coral', 'forestgreen'])\n",
    "axes[0].set_ylabel('Inference Time (ms)')\n",
    "axes[0].set_title('Inference Time by Model Type')\n",
    "axes[0].tick_params(axis='x', rotation=15)\n",
    "\n",
    "# Add value labels\n",
    "for bar, mean in zip(bars, means):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                 f'{mean:.2f}', ha='center', va='bottom')\n",
    "\n",
    "# Throughput comparison (samples per second)\n",
    "throughputs = [1000 / benchmark_results[n]['mean_ms'] for n in names]\n",
    "bars2 = axes[1].bar(names, throughputs, color=['steelblue', 'coral', 'forestgreen'])\n",
    "axes[1].set_ylabel('Throughput (samples/sec)')\n",
    "axes[1].set_title('Throughput by Model Type')\n",
    "axes[1].tick_params(axis='x', rotation=15)\n",
    "\n",
    "for bar, tp in zip(bars2, throughputs):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                 f'{tp:.1f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(EXPORT_DIR / 'benchmark_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Deployment Configurations\n",
    "\n",
    "Configuration templates for different deployment scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration templates\n",
    "deployment_configs = {\n",
    "    'development': {\n",
    "        'model_path': 'outputs/final_model/checkpoint_best.pt',\n",
    "        'device': 'cpu',\n",
    "        'batch_size': 1,\n",
    "        'num_workers': 0,\n",
    "        'log_level': 'DEBUG',\n",
    "        'enable_profiling': True,\n",
    "        'cache_predictions': False,\n",
    "    },\n",
    "    'production_cpu': {\n",
    "        'model_path': 'exports/gcode_model_traced.pt',\n",
    "        'device': 'cpu',\n",
    "        'batch_size': 8,\n",
    "        'num_workers': 4,\n",
    "        'log_level': 'INFO',\n",
    "        'enable_profiling': False,\n",
    "        'cache_predictions': True,\n",
    "        'cache_ttl_seconds': 300,\n",
    "    },\n",
    "    'production_gpu': {\n",
    "        'model_path': 'exports/gcode_model_traced.pt',\n",
    "        'device': 'cuda',\n",
    "        'batch_size': 32,\n",
    "        'num_workers': 4,\n",
    "        'log_level': 'INFO',\n",
    "        'enable_profiling': False,\n",
    "        'cache_predictions': True,\n",
    "        'cache_ttl_seconds': 300,\n",
    "        'fp16_inference': True,\n",
    "    },\n",
    "    'edge_device': {\n",
    "        'model_path': 'exports/gcode_model_quantized.pt',\n",
    "        'device': 'cpu',\n",
    "        'batch_size': 1,\n",
    "        'num_workers': 1,\n",
    "        'log_level': 'WARNING',\n",
    "        'enable_profiling': False,\n",
    "        'cache_predictions': False,\n",
    "        'max_memory_mb': 512,\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save configurations\n",
    "config_dir = EXPORT_DIR / 'configs'\n",
    "config_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for env_name, config in deployment_configs.items():\n",
    "    config_path = config_dir / f'{env_name}.json'\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    print(f\"Saved: {config_path}\")\n",
    "\n",
    "print(f\"\\nConfiguration files saved to: {config_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print configuration guide\n",
    "print(\"\"\"\n",
    "=== Deployment Configuration Guide ===\n",
    "\n",
    "development:\n",
    "  - Full debugging enabled\n",
    "  - Single-threaded for easier debugging\n",
    "  - Uses original PyTorch model\n",
    "\n",
    "production_cpu:\n",
    "  - TorchScript optimized model\n",
    "  - Multi-worker data loading\n",
    "  - Prediction caching enabled\n",
    "  - Suitable for cloud CPU instances\n",
    "\n",
    "production_gpu:\n",
    "  - GPU acceleration with FP16\n",
    "  - Large batch processing\n",
    "  - Maximum throughput\n",
    "  - Requires CUDA-capable GPU\n",
    "\n",
    "edge_device:\n",
    "  - Quantized model for minimal footprint\n",
    "  - Memory-constrained operation\n",
    "  - Suitable for embedded systems\n",
    "\n",
    "Usage:\n",
    "  python -m miracle.api.server --config exports/configs/production_cpu.json\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Monitoring & Logging\n",
    "\n",
    "Set up monitoring and logging for production deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prometheus metrics template\n",
    "prometheus_metrics = '''# Prometheus metrics for G-code fingerprinting service\n",
    "from prometheus_client import Counter, Histogram, Gauge, generate_latest\n",
    "import time\n",
    "\n",
    "# Request counters\n",
    "REQUESTS_TOTAL = Counter(\n",
    "    'gcode_requests_total',\n",
    "    'Total number of prediction requests',\n",
    "    ['endpoint', 'status']\n",
    ")\n",
    "\n",
    "# Latency histograms\n",
    "REQUEST_LATENCY = Histogram(\n",
    "    'gcode_request_latency_seconds',\n",
    "    'Request latency in seconds',\n",
    "    ['endpoint'],\n",
    "    buckets=[0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0]\n",
    ")\n",
    "\n",
    "INFERENCE_LATENCY = Histogram(\n",
    "    'gcode_inference_latency_seconds',\n",
    "    'Model inference latency in seconds',\n",
    "    buckets=[0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25]\n",
    ")\n",
    "\n",
    "# Model metrics\n",
    "MODEL_LOADED = Gauge(\n",
    "    'gcode_model_loaded',\n",
    "    'Whether the model is loaded (1) or not (0)'\n",
    ")\n",
    "\n",
    "BATCH_SIZE = Histogram(\n",
    "    'gcode_batch_size',\n",
    "    'Batch sizes of inference requests',\n",
    "    buckets=[1, 2, 4, 8, 16, 32, 64]\n",
    ")\n",
    "\n",
    "# Prediction confidence\n",
    "PREDICTION_CONFIDENCE = Histogram(\n",
    "    'gcode_prediction_confidence',\n",
    "    'Confidence scores of predictions',\n",
    "    ['head'],\n",
    "    buckets=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99]\n",
    ")\n",
    "\n",
    "\n",
    "class MetricsMiddleware:\n",
    "    \"\"\"Middleware to track request metrics.\"\"\"\n",
    "    \n",
    "    def __init__(self, app):\n",
    "        self.app = app\n",
    "    \n",
    "    async def __call__(self, scope, receive, send):\n",
    "        if scope[\"type\"] != \"http\":\n",
    "            await self.app(scope, receive, send)\n",
    "            return\n",
    "        \n",
    "        path = scope[\"path\"]\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Process request\n",
    "        await self.app(scope, receive, send)\n",
    "        \n",
    "        # Record metrics\n",
    "        duration = time.time() - start_time\n",
    "        REQUEST_LATENCY.labels(endpoint=path).observe(duration)\n",
    "        REQUESTS_TOTAL.labels(endpoint=path, status=\"success\").inc()\n",
    "'''\n",
    "\n",
    "# Save metrics module\n",
    "metrics_path = EXPORT_DIR / 'metrics.py'\n",
    "metrics_path.write_text(prometheus_metrics)\n",
    "print(f\"Prometheus metrics module saved to: {metrics_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging configuration\n",
    "logging_config = '''{\n",
    "  \"version\": 1,\n",
    "  \"disable_existing_loggers\": false,\n",
    "  \"formatters\": {\n",
    "    \"standard\": {\n",
    "      \"format\": \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    "    },\n",
    "    \"json\": {\n",
    "      \"()\": \"pythonjsonlogger.jsonlogger.JsonFormatter\",\n",
    "      \"format\": \"%(asctime)s %(name)s %(levelname)s %(message)s\"\n",
    "    }\n",
    "  },\n",
    "  \"handlers\": {\n",
    "    \"console\": {\n",
    "      \"class\": \"logging.StreamHandler\",\n",
    "      \"level\": \"INFO\",\n",
    "      \"formatter\": \"standard\",\n",
    "      \"stream\": \"ext://sys.stdout\"\n",
    "    },\n",
    "    \"file\": {\n",
    "      \"class\": \"logging.handlers.RotatingFileHandler\",\n",
    "      \"level\": \"DEBUG\",\n",
    "      \"formatter\": \"json\",\n",
    "      \"filename\": \"logs/gcode_api.log\",\n",
    "      \"maxBytes\": 10485760,\n",
    "      \"backupCount\": 5\n",
    "    }\n",
    "  },\n",
    "  \"loggers\": {\n",
    "    \"miracle\": {\n",
    "      \"level\": \"DEBUG\",\n",
    "      \"handlers\": [\"console\", \"file\"],\n",
    "      \"propagate\": false\n",
    "    },\n",
    "    \"uvicorn\": {\n",
    "      \"level\": \"INFO\",\n",
    "      \"handlers\": [\"console\"],\n",
    "      \"propagate\": false\n",
    "    }\n",
    "  },\n",
    "  \"root\": {\n",
    "    \"level\": \"INFO\",\n",
    "    \"handlers\": [\"console\"]\n",
    "  }\n",
    "}'''\n",
    "\n",
    "logging_config_path = EXPORT_DIR / 'logging_config.json'\n",
    "logging_config_path.write_text(logging_config)\n",
    "print(f\"Logging configuration saved to: {logging_config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of exported files\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPORT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for file_path in sorted(EXPORT_DIR.rglob('*')):\n",
    "    if file_path.is_file():\n",
    "        size_kb = file_path.stat().st_size / 1024\n",
    "        rel_path = file_path.relative_to(EXPORT_DIR)\n",
    "        print(f\"{rel_path:<40} {size_kb:>10.1f} KB\")\n",
    "\n",
    "total_size = sum(f.stat().st_size for f in EXPORT_DIR.rglob('*') if f.is_file())\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Total':<40} {total_size/1024/1024:>10.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook covers production deployment:\n",
    "\n",
    "1. **ONNX Export**: Cross-platform model format with runtime verification\n",
    "2. **Quantization**: INT8 quantization for reduced size and faster inference\n",
    "3. **TorchScript**: Optimized compilation for production\n",
    "4. **Docker**: Containerization with health checks and compose files\n",
    "5. **Benchmarking**: Performance comparison across deployment options\n",
    "6. **Configurations**: Environment-specific deployment configs\n",
    "7. **Monitoring**: Prometheus metrics and structured logging\n",
    "\n",
    "---\n",
    "\n",
    "**Navigation:**\n",
    "← [Previous: 12_error_analysis](12_error_analysis.ipynb) |\n",
    "[Next: 14_robustness_testing](14_robustness_testing.ipynb) →"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
