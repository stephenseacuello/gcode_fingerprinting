{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17. Uncertainty Quantification\n",
    "\n",
    "Quantify prediction uncertainty using Monte Carlo Dropout and calibration analysis.\n",
    "\n",
    "## Contents\n",
    "1. [Setup](#1-setup)\n",
    "2. [Monte Carlo Dropout](#2-monte-carlo-dropout)\n",
    "3. [Confidence Calibration](#3-confidence-calibration)\n",
    "4. [Uncertainty Metrics](#4-uncertainty-metrics)\n",
    "5. [Selective Prediction](#5-selective-prediction)\n",
    "6. [Temperature Scaling](#6-temperature-scaling)\n",
    "7. [Visualization](#7-visualization)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root / 'src'))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and data\n",
    "from miracle.model.backbone import MMDTAELSTMBackbone\n",
    "from miracle.model.multihead_lm import MultiHeadGCodeLM\n",
    "\n",
    "VOCAB_PATH = project_root / 'data' / 'gcode_vocab_v2.json'\n",
    "CHECKPOINT_PATH = project_root / 'outputs' / 'final_model' / 'checkpoint_best.pt'\n",
    "DATA_DIR = project_root / 'outputs' / 'processed_v2'\n",
    "\n",
    "with open(VOCAB_PATH) as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    checkpoint = torch.load(CHECKPOINT_PATH, map_location=device, weights_only=False)\n",
    "    config = checkpoint.get('config', {})\n",
    "else:\n",
    "    config = {'hidden_dim': 256, 'num_layers': 4, 'num_heads': 8, 'dropout': 0.1}\n",
    "\n",
    "backbone = MMDTAELSTMBackbone(\n",
    "    continuous_dim=155,\n",
    "    categorical_dims=[10, 10, 50, 50],\n",
    "    d_model=config.get('hidden_dim', 256),\n",
    "    num_layers=config.get('num_layers', 4),\n",
    "    num_heads=config.get('num_heads', 8),\n",
    "    dropout=config.get('dropout', 0.1)\n",
    ").to(device)\n",
    "\n",
    "lm = MultiHeadGCodeLM(\n",
    "    d_model=config.get('hidden_dim', 256),\n",
    "    vocab_sizes=vocab.get('head_vocab_sizes', {'type': 10, 'command': 50, 'param_type': 30, 'param_value': 100})\n",
    ").to(device)\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    backbone.load_state_dict(checkpoint['backbone_state_dict'])\n",
    "    lm.load_state_dict(checkpoint['lm_state_dict'])\n",
    "\n",
    "print(\"Models loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_path = DATA_DIR / 'test.pt'\n",
    "if test_path.exists():\n",
    "    test_data = torch.load(test_path, weights_only=False)\n",
    "    test_continuous = torch.tensor(test_data['continuous'][:50], dtype=torch.float32).to(device)\n",
    "    test_categorical = torch.tensor(test_data['categorical'][:50], dtype=torch.long).to(device)\n",
    "else:\n",
    "    test_continuous = torch.randn(50, 64, 155).to(device)\n",
    "    test_categorical = torch.randint(0, 10, (50, 64, 4)).to(device)\n",
    "\n",
    "print(f\"Test data: {test_continuous.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Monte Carlo Dropout\n",
    "\n",
    "Enable dropout at inference time for uncertainty estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enable_dropout(model):\n",
    "    \"\"\"Enable dropout layers for MC Dropout.\"\"\"\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Dropout):\n",
    "            module.train()\n",
    "\n",
    "def mc_dropout_predictions(backbone, lm, continuous, categorical, \n",
    "                           n_samples=30, head='command'):\n",
    "    \"\"\"Generate multiple predictions using MC Dropout.\"\"\"\n",
    "    backbone.eval()\n",
    "    lm.eval()\n",
    "    enable_dropout(backbone)\n",
    "    enable_dropout(lm)\n",
    "    \n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_samples):\n",
    "            hidden = backbone(continuous, categorical)\n",
    "            preds = lm(hidden)\n",
    "            probs = F.softmax(preds[head], dim=-1)\n",
    "            all_probs.append(probs.cpu())\n",
    "    \n",
    "    # Stack: [n_samples, B, T, V]\n",
    "    all_probs = torch.stack(all_probs)\n",
    "    \n",
    "    # Mean and variance\n",
    "    mean_probs = all_probs.mean(dim=0)\n",
    "    var_probs = all_probs.var(dim=0)\n",
    "    \n",
    "    return mean_probs, var_probs, all_probs\n",
    "\n",
    "# Run MC Dropout\n",
    "print(\"Running MC Dropout (30 samples)...\")\n",
    "mean_probs, var_probs, all_probs = mc_dropout_predictions(\n",
    "    backbone, lm, test_continuous[:10], test_categorical[:10], n_samples=30\n",
    ")\n",
    "\n",
    "print(f\"Mean probs shape: {mean_probs.shape}\")\n",
    "print(f\"Variance shape: {var_probs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize MC Dropout predictions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "sample_idx, pos_idx = 0, 32\n",
    "\n",
    "# Distribution of predictions\n",
    "sample_probs = all_probs[:, sample_idx, pos_idx, :]  # [n_samples, V]\n",
    "top_classes = mean_probs[sample_idx, pos_idx].argsort(descending=True)[:5]\n",
    "\n",
    "for i, cls in enumerate(top_classes):\n",
    "    class_probs = sample_probs[:, cls].numpy()\n",
    "    axes[0].boxplot(class_probs, positions=[i], widths=0.6)\n",
    "\n",
    "axes[0].set_xticks(range(len(top_classes)))\n",
    "axes[0].set_xticklabels([f'Class {c.item()}' for c in top_classes])\n",
    "axes[0].set_ylabel('Probability')\n",
    "axes[0].set_title('MC Dropout: Prediction Distribution')\n",
    "\n",
    "# Mean confidence over sequence\n",
    "mean_conf = mean_probs[sample_idx].max(dim=-1)[0].numpy()\n",
    "axes[1].plot(mean_conf)\n",
    "axes[1].set_xlabel('Position')\n",
    "axes[1].set_ylabel('Mean Confidence')\n",
    "axes[1].set_title('Confidence Across Sequence')\n",
    "\n",
    "# Uncertainty (variance) over sequence\n",
    "pred_classes = mean_probs[sample_idx].argmax(dim=-1)\n",
    "uncertainties = [var_probs[sample_idx, t, pred_classes[t]].item() for t in range(len(pred_classes))]\n",
    "axes[2].plot(uncertainties, color='coral')\n",
    "axes[2].set_xlabel('Position')\n",
    "axes[2].set_ylabel('Prediction Variance')\n",
    "axes[2].set_title('Uncertainty Across Sequence')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(project_root / 'reports' / 'mc_dropout_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Confidence Calibration\n",
    "\n",
    "Analyze how well confidence matches accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_calibration(predictions, targets, n_bins=10):\n",
    "    \"\"\"Compute calibration curve.\"\"\"\n",
    "    confidences = predictions.max(dim=-1)[0].flatten().cpu().numpy()\n",
    "    pred_classes = predictions.argmax(dim=-1).flatten().cpu().numpy()\n",
    "    true_classes = targets.flatten().cpu().numpy()\n",
    "    \n",
    "    accuracies = (pred_classes == true_classes).astype(float)\n",
    "    \n",
    "    # Bin by confidence\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_centers = (bin_boundaries[:-1] + bin_boundaries[1:]) / 2\n",
    "    \n",
    "    bin_accuracies = []\n",
    "    bin_confidences = []\n",
    "    bin_counts = []\n",
    "    \n",
    "    for i in range(n_bins):\n",
    "        mask = (confidences >= bin_boundaries[i]) & (confidences < bin_boundaries[i+1])\n",
    "        if mask.sum() > 0:\n",
    "            bin_accuracies.append(accuracies[mask].mean())\n",
    "            bin_confidences.append(confidences[mask].mean())\n",
    "            bin_counts.append(mask.sum())\n",
    "        else:\n",
    "            bin_accuracies.append(0)\n",
    "            bin_confidences.append(bin_centers[i])\n",
    "            bin_counts.append(0)\n",
    "    \n",
    "    # Expected Calibration Error\n",
    "    ece = sum(abs(a - c) * n for a, c, n in zip(bin_accuracies, bin_confidences, bin_counts)) / sum(bin_counts)\n",
    "    \n",
    "    return {\n",
    "        'bin_centers': bin_centers,\n",
    "        'bin_accuracies': np.array(bin_accuracies),\n",
    "        'bin_confidences': np.array(bin_confidences),\n",
    "        'bin_counts': np.array(bin_counts),\n",
    "        'ece': ece\n",
    "    }\n",
    "\n",
    "# Generate predictions with normal inference\n",
    "backbone.eval()\n",
    "lm.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    hidden = backbone(test_continuous, test_categorical)\n",
    "    predictions = lm(hidden)\n",
    "\n",
    "# Using predictions as pseudo-targets for demo\n",
    "# In practice, use actual targets\n",
    "pseudo_targets = predictions['command'].argmax(dim=-1)\n",
    "\n",
    "# Add some noise to simulate errors\n",
    "noise_mask = torch.rand_like(pseudo_targets.float()) < 0.2\n",
    "noisy_targets = pseudo_targets.clone()\n",
    "noisy_targets[noise_mask] = torch.randint(0, 50, noisy_targets[noise_mask].shape, device=device)\n",
    "\n",
    "# Compute calibration\n",
    "probs = F.softmax(predictions['command'], dim=-1)\n",
    "calibration = compute_calibration(probs, noisy_targets)\n",
    "\n",
    "print(f\"Expected Calibration Error (ECE): {calibration['ece']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize calibration\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Reliability diagram\n",
    "axes[0].bar(calibration['bin_centers'], calibration['bin_accuracies'], width=0.08, \n",
    "           alpha=0.7, label='Accuracy')\n",
    "axes[0].plot([0, 1], [0, 1], 'r--', label='Perfect calibration')\n",
    "axes[0].set_xlabel('Confidence')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title(f'Reliability Diagram (ECE={calibration[\"ece\"]:.4f})')\n",
    "axes[0].legend()\n",
    "axes[0].set_xlim(0, 1)\n",
    "axes[0].set_ylim(0, 1)\n",
    "\n",
    "# Confidence histogram\n",
    "all_confs = probs.max(dim=-1)[0].flatten().cpu().numpy()\n",
    "axes[1].hist(all_confs, bins=20, alpha=0.7, edgecolor='black')\n",
    "axes[1].set_xlabel('Confidence')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Confidence Distribution')\n",
    "axes[1].axvline(x=np.mean(all_confs), color='red', linestyle='--', label=f'Mean: {np.mean(all_confs):.3f}')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(project_root / 'reports' / 'calibration_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Uncertainty Metrics\n",
    "\n",
    "Different ways to measure prediction uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(probs):\n",
    "    \"\"\"Compute predictive entropy.\"\"\"\n",
    "    return -(probs * torch.log(probs + 1e-10)).sum(dim=-1)\n",
    "\n",
    "def compute_mutual_information(all_probs):\n",
    "    \"\"\"Compute mutual information (epistemic uncertainty) from MC samples.\"\"\"\n",
    "    mean_probs = all_probs.mean(dim=0)\n",
    "    total_entropy = compute_entropy(mean_probs)\n",
    "    expected_entropy = compute_entropy(all_probs).mean(dim=0)\n",
    "    return total_entropy - expected_entropy\n",
    "\n",
    "def compute_variation_ratio(all_probs):\n",
    "    \"\"\"Compute variation ratio from MC samples.\"\"\"\n",
    "    predictions = all_probs.argmax(dim=-1)  # [n_samples, B, T]\n",
    "    mode_freq = torch.zeros(predictions.shape[1:], device=predictions.device)\n",
    "    \n",
    "    for b in range(predictions.shape[1]):\n",
    "        for t in range(predictions.shape[2]):\n",
    "            counts = torch.bincount(predictions[:, b, t])\n",
    "            mode_freq[b, t] = counts.max().float() / len(predictions)\n",
    "    \n",
    "    return 1 - mode_freq\n",
    "\n",
    "# Compute all uncertainty metrics\n",
    "entropy = compute_entropy(mean_probs)\n",
    "mutual_info = compute_mutual_information(all_probs)\n",
    "variation_ratio = compute_variation_ratio(all_probs)\n",
    "\n",
    "print(f\"Entropy - mean: {entropy.mean():.4f}, std: {entropy.std():.4f}\")\n",
    "print(f\"Mutual Info - mean: {mutual_info.mean():.4f}, std: {mutual_info.std():.4f}\")\n",
    "print(f\"Variation Ratio - mean: {variation_ratio.mean():.4f}, std: {variation_ratio.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize uncertainty metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "sample_idx = 0\n",
    "\n",
    "# Entropy\n",
    "axes[0, 0].plot(entropy[sample_idx].numpy())\n",
    "axes[0, 0].set_xlabel('Position')\n",
    "axes[0, 0].set_ylabel('Entropy')\n",
    "axes[0, 0].set_title('Predictive Entropy (Total Uncertainty)')\n",
    "\n",
    "# Mutual Information\n",
    "axes[0, 1].plot(mutual_info[sample_idx].numpy(), color='coral')\n",
    "axes[0, 1].set_xlabel('Position')\n",
    "axes[0, 1].set_ylabel('Mutual Information')\n",
    "axes[0, 1].set_title('Epistemic Uncertainty (Model Uncertainty)')\n",
    "\n",
    "# Variation Ratio\n",
    "axes[1, 0].plot(variation_ratio[sample_idx].numpy(), color='forestgreen')\n",
    "axes[1, 0].set_xlabel('Position')\n",
    "axes[1, 0].set_ylabel('Variation Ratio')\n",
    "axes[1, 0].set_title('Prediction Disagreement')\n",
    "\n",
    "# All metrics together\n",
    "axes[1, 1].plot(entropy[sample_idx].numpy() / entropy[sample_idx].max(), label='Entropy (normalized)')\n",
    "axes[1, 1].plot(mutual_info[sample_idx].numpy() / (mutual_info[sample_idx].max() + 1e-6), label='Mutual Info (normalized)')\n",
    "axes[1, 1].plot(variation_ratio[sample_idx].numpy(), label='Variation Ratio')\n",
    "axes[1, 1].set_xlabel('Position')\n",
    "axes[1, 1].set_ylabel('Normalized Value')\n",
    "axes[1, 1].set_title('Uncertainty Metrics Comparison')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(project_root / 'reports' / 'uncertainty_metrics.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Selective Prediction\n",
    "\n",
    "Reject predictions with high uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selective_prediction_curve(probs, targets, uncertainty_metric):\n",
    "    \"\"\"Compute accuracy at different coverage levels.\"\"\"\n",
    "    predictions = probs.argmax(dim=-1).flatten().cpu().numpy()\n",
    "    targets_flat = targets.flatten().cpu().numpy()\n",
    "    uncertainty_flat = uncertainty_metric.flatten().cpu().numpy()\n",
    "    \n",
    "    correct = predictions == targets_flat\n",
    "    \n",
    "    # Sort by uncertainty (ascending = most confident first)\n",
    "    sorted_indices = np.argsort(uncertainty_flat)\n",
    "    correct_sorted = correct[sorted_indices]\n",
    "    \n",
    "    # Compute cumulative accuracy at each coverage\n",
    "    n = len(correct_sorted)\n",
    "    coverages = np.arange(1, n + 1) / n\n",
    "    cumulative_acc = np.cumsum(correct_sorted) / np.arange(1, n + 1)\n",
    "    \n",
    "    return coverages, cumulative_acc\n",
    "\n",
    "# Compute selective prediction curves\n",
    "confidence = mean_probs.max(dim=-1)[0]  # Higher = more confident\n",
    "\n",
    "# Use entropy as uncertainty (higher = more uncertain)\n",
    "coverages_ent, acc_ent = selective_prediction_curve(mean_probs, noisy_targets.cpu(), entropy)\n",
    "coverages_mi, acc_mi = selective_prediction_curve(mean_probs, noisy_targets.cpu(), mutual_info)\n",
    "coverages_conf, acc_conf = selective_prediction_curve(mean_probs, noisy_targets.cpu(), 1 - confidence)  # Negate confidence\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Coverage vs Accuracy\n",
    "axes[0].plot(coverages_ent, acc_ent, label='Entropy')\n",
    "axes[0].plot(coverages_mi, acc_mi, label='Mutual Information')\n",
    "axes[0].plot(coverages_conf, acc_conf, label='Confidence')\n",
    "axes[0].axhline(y=acc_conf[-1], color='gray', linestyle='--', alpha=0.5, label='Full coverage acc')\n",
    "axes[0].set_xlabel('Coverage')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Selective Prediction: Coverage vs Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].set_xlim(0, 1)\n",
    "\n",
    "# Risk-coverage curve\n",
    "risk_ent = 1 - acc_ent\n",
    "risk_mi = 1 - acc_mi\n",
    "risk_conf = 1 - acc_conf\n",
    "\n",
    "axes[1].plot(coverages_ent, risk_ent, label='Entropy')\n",
    "axes[1].plot(coverages_mi, risk_mi, label='Mutual Information')\n",
    "axes[1].plot(coverages_conf, risk_conf, label='Confidence')\n",
    "axes[1].set_xlabel('Coverage')\n",
    "axes[1].set_ylabel('Risk (Error Rate)')\n",
    "axes[1].set_title('Risk-Coverage Curve')\n",
    "axes[1].legend()\n",
    "axes[1].set_xlim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(project_root / 'reports' / 'selective_prediction.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Temperature Scaling\n",
    "\n",
    "Post-hoc calibration using temperature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemperatureScaling(nn.Module):\n",
    "    \"\"\"Temperature scaling for calibration.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.temperature = nn.Parameter(torch.ones(1))\n",
    "        \n",
    "    def forward(self, logits):\n",
    "        return logits / self.temperature\n",
    "\n",
    "def train_temperature(logits, targets, max_iter=100):\n",
    "    \"\"\"Optimize temperature on validation set.\"\"\"\n",
    "    temp_model = TemperatureScaling().to(device)\n",
    "    optimizer = torch.optim.LBFGS([temp_model.temperature], lr=0.01, max_iter=max_iter)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        scaled_logits = temp_model(logits)\n",
    "        B, T, V = scaled_logits.shape\n",
    "        loss = criterion(scaled_logits.view(-1, V), targets.view(-1))\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    \n",
    "    optimizer.step(closure)\n",
    "    \n",
    "    return temp_model.temperature.item()\n",
    "\n",
    "# Train temperature\n",
    "logits = predictions['command']\n",
    "optimal_temp = train_temperature(logits, noisy_targets)\n",
    "print(f\"Optimal temperature: {optimal_temp:.4f}\")\n",
    "\n",
    "# Apply temperature scaling\n",
    "scaled_logits = logits / optimal_temp\n",
    "scaled_probs = F.softmax(scaled_logits, dim=-1)\n",
    "\n",
    "# Compare calibration\n",
    "calibration_before = compute_calibration(F.softmax(logits, dim=-1), noisy_targets)\n",
    "calibration_after = compute_calibration(scaled_probs, noisy_targets)\n",
    "\n",
    "print(f\"ECE before: {calibration_before['ece']:.4f}\")\n",
    "print(f\"ECE after:  {calibration_after['ece']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize temperature scaling effect\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Before\n",
    "axes[0].bar(calibration_before['bin_centers'], calibration_before['bin_accuracies'], \n",
    "           width=0.08, alpha=0.7)\n",
    "axes[0].plot([0, 1], [0, 1], 'r--')\n",
    "axes[0].set_xlabel('Confidence')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title(f'Before Scaling (ECE={calibration_before[\"ece\"]:.4f})')\n",
    "axes[0].set_xlim(0, 1)\n",
    "axes[0].set_ylim(0, 1)\n",
    "\n",
    "# After\n",
    "axes[1].bar(calibration_after['bin_centers'], calibration_after['bin_accuracies'], \n",
    "           width=0.08, alpha=0.7, color='coral')\n",
    "axes[1].plot([0, 1], [0, 1], 'r--')\n",
    "axes[1].set_xlabel('Confidence')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title(f'After Scaling (ECE={calibration_after[\"ece\"]:.4f}, T={optimal_temp:.2f})')\n",
    "axes[1].set_xlim(0, 1)\n",
    "axes[1].set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(project_root / 'reports' / 'temperature_scaling.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization\n",
    "\n",
    "Comprehensive uncertainty visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of uncertainty across sequence\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Confidence heatmap\n",
    "conf_matrix = mean_probs.max(dim=-1)[0].cpu().numpy()\n",
    "im1 = axes[0, 0].imshow(conf_matrix, aspect='auto', cmap='RdYlGn')\n",
    "axes[0, 0].set_xlabel('Position')\n",
    "axes[0, 0].set_ylabel('Sample')\n",
    "axes[0, 0].set_title('Confidence Heatmap')\n",
    "plt.colorbar(im1, ax=axes[0, 0])\n",
    "\n",
    "# Entropy heatmap\n",
    "im2 = axes[0, 1].imshow(entropy.numpy(), aspect='auto', cmap='YlOrRd')\n",
    "axes[0, 1].set_xlabel('Position')\n",
    "axes[0, 1].set_ylabel('Sample')\n",
    "axes[0, 1].set_title('Entropy Heatmap')\n",
    "plt.colorbar(im2, ax=axes[0, 1])\n",
    "\n",
    "# Mutual Information heatmap\n",
    "im3 = axes[1, 0].imshow(mutual_info.numpy(), aspect='auto', cmap='YlOrRd')\n",
    "axes[1, 0].set_xlabel('Position')\n",
    "axes[1, 0].set_ylabel('Sample')\n",
    "axes[1, 0].set_title('Mutual Information Heatmap')\n",
    "plt.colorbar(im3, ax=axes[1, 0])\n",
    "\n",
    "# Variation Ratio heatmap\n",
    "im4 = axes[1, 1].imshow(variation_ratio.numpy(), aspect='auto', cmap='YlOrRd')\n",
    "axes[1, 1].set_xlabel('Position')\n",
    "axes[1, 1].set_ylabel('Sample')\n",
    "axes[1, 1].set_title('Variation Ratio Heatmap')\n",
    "plt.colorbar(im4, ax=axes[1, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(project_root / 'reports' / 'uncertainty_heatmaps.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save uncertainty report\n",
    "report = {\n",
    "    'mc_dropout_samples': 30,\n",
    "    'optimal_temperature': optimal_temp,\n",
    "    'calibration': {\n",
    "        'ece_before': calibration_before['ece'],\n",
    "        'ece_after': calibration_after['ece'],\n",
    "    },\n",
    "    'uncertainty_stats': {\n",
    "        'entropy_mean': float(entropy.mean()),\n",
    "        'entropy_std': float(entropy.std()),\n",
    "        'mutual_info_mean': float(mutual_info.mean()),\n",
    "        'mutual_info_std': float(mutual_info.std()),\n",
    "    }\n",
    "}\n",
    "\n",
    "report_path = project_root / 'reports' / 'uncertainty_report.json'\n",
    "with open(report_path, 'w') as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(f\"Report saved to: {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook covers uncertainty quantification:\n",
    "\n",
    "1. **MC Dropout**: Enable dropout at inference for Bayesian approximation\n",
    "2. **Calibration**: Analyze confidence vs accuracy relationship\n",
    "3. **Metrics**: Entropy, mutual information, variation ratio\n",
    "4. **Selective Prediction**: Reject uncertain predictions\n",
    "5. **Temperature Scaling**: Post-hoc calibration\n",
    "6. **Visualization**: Heatmaps and curves\n",
    "\n",
    "---\n",
    "\n",
    "**Navigation:**\n",
    "← [Previous: 16_architecture_comparison](16_architecture_comparison.ipynb) |\n",
    "[Next: 18_transfer_learning](18_transfer_learning.ipynb) →"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
