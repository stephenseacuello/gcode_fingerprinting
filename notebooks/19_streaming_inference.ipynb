{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19. Streaming Inference\n",
    "\n",
    "Real-time inference on streaming sensor data with sliding windows.\n",
    "\n",
    "## Contents\n",
    "1. [Setup](#1-setup)\n",
    "2. [Sliding Window Inference](#2-sliding-window-inference)\n",
    "3. [Incremental Processing](#3-incremental-processing)\n",
    "4. [Latency Optimization](#4-latency-optimization)\n",
    "5. [Buffer Management](#5-buffer-management)\n",
    "6. [Real-Time Simulation](#6-real-time-simulation)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root / 'src'))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Optional, Deque\n",
    "from collections import deque\n",
    "import json\n",
    "import time\n",
    "import threading\n",
    "import queue\n",
    "\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "device = torch.device('cpu')  # Use CPU for real-time to avoid GPU sync overhead\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "from miracle.model.backbone import MMDTAELSTMBackbone\n",
    "from miracle.model.multihead_lm import MultiHeadGCodeLM\n",
    "\n",
    "VOCAB_PATH = project_root / 'data' / 'gcode_vocab_v2.json'\n",
    "CHECKPOINT_PATH = project_root / 'outputs' / 'final_model' / 'checkpoint_best.pt'\n",
    "\n",
    "with open(VOCAB_PATH) as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    checkpoint = torch.load(CHECKPOINT_PATH, map_location=device, weights_only=False)\n",
    "    config = checkpoint.get('config', {})\n",
    "else:\n",
    "    config = {'hidden_dim': 256, 'num_layers': 4, 'num_heads': 8, 'dropout': 0.0}\n",
    "\n",
    "backbone = MMDTAELSTMBackbone(\n",
    "    continuous_dim=155,\n",
    "    categorical_dims=[10, 10, 50, 50],\n",
    "    d_model=config.get('hidden_dim', 256),\n",
    "    num_layers=config.get('num_layers', 4),\n",
    "    num_heads=config.get('num_heads', 8),\n",
    "    dropout=0.0  # No dropout for inference\n",
    ").to(device)\n",
    "\n",
    "lm = MultiHeadGCodeLM(\n",
    "    d_model=config.get('hidden_dim', 256),\n",
    "    vocab_sizes=vocab.get('head_vocab_sizes', {'type': 10, 'command': 50, 'param_type': 30, 'param_value': 100})\n",
    ").to(device)\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    backbone.load_state_dict(checkpoint['backbone_state_dict'])\n",
    "    lm.load_state_dict(checkpoint['lm_state_dict'])\n",
    "\n",
    "backbone.eval()\n",
    "lm.eval()\n",
    "print(\"Model loaded for streaming inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sliding Window Inference\n",
    "\n",
    "Process streaming data with overlapping windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlidingWindowInference:\n",
    "    \"\"\"Sliding window inference for streaming data.\"\"\"\n",
    "    \n",
    "    def __init__(self, backbone, lm, window_size=64, stride=16):\n",
    "        self.backbone = backbone\n",
    "        self.lm = lm\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "        \n",
    "        # Buffer for incoming data\n",
    "        self.continuous_buffer = deque(maxlen=window_size * 2)\n",
    "        self.categorical_buffer = deque(maxlen=window_size * 2)\n",
    "        \n",
    "        # Prediction cache for smoothing\n",
    "        self.prediction_cache = {}\n",
    "        \n",
    "    def add_sample(self, continuous, categorical):\n",
    "        \"\"\"Add a single timestep to the buffer.\"\"\"\n",
    "        self.continuous_buffer.append(continuous)\n",
    "        self.categorical_buffer.append(categorical)\n",
    "        \n",
    "    def can_predict(self):\n",
    "        \"\"\"Check if we have enough data for prediction.\"\"\"\n",
    "        return len(self.continuous_buffer) >= self.window_size\n",
    "    \n",
    "    def predict(self):\n",
    "        \"\"\"Run inference on current window.\"\"\"\n",
    "        if not self.can_predict():\n",
    "            return None\n",
    "        \n",
    "        # Get current window\n",
    "        cont_window = list(self.continuous_buffer)[-self.window_size:]\n",
    "        cat_window = list(self.categorical_buffer)[-self.window_size:]\n",
    "        \n",
    "        # Stack into tensors\n",
    "        continuous = torch.stack(cont_window).unsqueeze(0)  # [1, T, C]\n",
    "        categorical = torch.stack(cat_window).unsqueeze(0)  # [1, T, 4]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            hidden = self.backbone(continuous, categorical)\n",
    "            predictions = self.lm(hidden)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Clear buffers.\"\"\"\n",
    "        self.continuous_buffer.clear()\n",
    "        self.categorical_buffer.clear()\n",
    "\n",
    "\n",
    "# Create sliding window processor\n",
    "sliding_window = SlidingWindowInference(backbone, lm, window_size=64, stride=16)\n",
    "\n",
    "# Simulate streaming data\n",
    "print(\"Simulating streaming inference...\")\n",
    "\n",
    "latencies = []\n",
    "for t in range(100):\n",
    "    # Generate random sensor sample\n",
    "    continuous = torch.randn(155)\n",
    "    categorical = torch.randint(0, 10, (4,))\n",
    "    \n",
    "    # Add to buffer\n",
    "    sliding_window.add_sample(continuous, categorical)\n",
    "    \n",
    "    # Predict if we have enough data\n",
    "    if sliding_window.can_predict() and t % sliding_window.stride == 0:\n",
    "        start = time.time()\n",
    "        preds = sliding_window.predict()\n",
    "        latency = (time.time() - start) * 1000\n",
    "        latencies.append(latency)\n",
    "\n",
    "print(f\"Mean latency: {np.mean(latencies):.2f} ms\")\n",
    "print(f\"Max latency: {np.max(latencies):.2f} ms\")\n",
    "print(f\"P95 latency: {np.percentile(latencies, 95):.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sliding window operation\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 6))\n",
    "\n",
    "# Generate sample signal\n",
    "signal = np.sin(np.linspace(0, 8*np.pi, 200)) + 0.3 * np.random.randn(200)\n",
    "window_size = 64\n",
    "stride = 16\n",
    "\n",
    "# Show signal with windows\n",
    "axes[0].plot(signal, 'b-', alpha=0.7)\n",
    "\n",
    "colors = plt.cm.Set2(np.linspace(0, 1, 5))\n",
    "for i, start in enumerate([0, 16, 32, 48, 64]):\n",
    "    if start + window_size <= len(signal):\n",
    "        axes[0].axvspan(start, start + window_size, alpha=0.2, color=colors[i % len(colors)])\n",
    "        axes[0].axvline(x=start, color=colors[i % len(colors)], linestyle='--', alpha=0.5)\n",
    "\n",
    "axes[0].set_xlabel('Time Step')\n",
    "axes[0].set_ylabel('Signal')\n",
    "axes[0].set_title(f'Sliding Window (size={window_size}, stride={stride})')\n",
    "\n",
    "# Latency distribution\n",
    "axes[1].hist(latencies, bins=20, alpha=0.7, edgecolor='black')\n",
    "axes[1].axvline(x=np.mean(latencies), color='red', linestyle='--', label=f'Mean: {np.mean(latencies):.2f} ms')\n",
    "axes[1].axvline(x=np.percentile(latencies, 95), color='orange', linestyle='--', label=f'P95: {np.percentile(latencies, 95):.2f} ms')\n",
    "axes[1].set_xlabel('Latency (ms)')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Inference Latency Distribution')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(project_root / 'reports' / 'sliding_window_inference.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Incremental Processing\n",
    "\n",
    "Reuse computation from previous windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IncrementalInference:\n",
    "    \"\"\"Incremental inference reusing cached computations.\"\"\"\n",
    "    \n",
    "    def __init__(self, backbone, lm, window_size=64):\n",
    "        self.backbone = backbone\n",
    "        self.lm = lm\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        # State caching\n",
    "        self.cached_hidden = None\n",
    "        self.cache_position = 0\n",
    "        \n",
    "        # Data buffers\n",
    "        self.continuous_buffer = []\n",
    "        self.categorical_buffer = []\n",
    "        \n",
    "    def process_incremental(self, new_continuous, new_categorical):\n",
    "        \"\"\"\n",
    "        Process new samples incrementally.\n",
    "        Returns predictions for new positions.\n",
    "        \"\"\"\n",
    "        # Add new data\n",
    "        self.continuous_buffer.extend(new_continuous)\n",
    "        self.categorical_buffer.extend(new_categorical)\n",
    "        \n",
    "        # Keep only relevant history\n",
    "        if len(self.continuous_buffer) > self.window_size * 2:\n",
    "            excess = len(self.continuous_buffer) - self.window_size * 2\n",
    "            self.continuous_buffer = self.continuous_buffer[excess:]\n",
    "            self.categorical_buffer = self.categorical_buffer[excess:]\n",
    "            self.cache_position = max(0, self.cache_position - excess)\n",
    "        \n",
    "        # Full window inference for simplicity\n",
    "        # In practice, use cached LSTM states\n",
    "        if len(self.continuous_buffer) >= self.window_size:\n",
    "            continuous = torch.stack(self.continuous_buffer[-self.window_size:]).unsqueeze(0)\n",
    "            categorical = torch.stack(self.categorical_buffer[-self.window_size:]).unsqueeze(0)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                hidden = self.backbone(continuous, categorical)\n",
    "                predictions = self.lm(hidden)\n",
    "            \n",
    "            # Return only predictions for new positions\n",
    "            n_new = len(new_continuous)\n",
    "            new_preds = {k: v[:, -n_new:] for k, v in predictions.items()}\n",
    "            \n",
    "            return new_preds\n",
    "        \n",
    "        return None\n",
    "\n",
    "\n",
    "class CachedLSTMInference:\n",
    "    \"\"\"LSTM inference with hidden state caching.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_layers):\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_state = None\n",
    "        self.cell_state = None\n",
    "        \n",
    "    def reset_state(self):\n",
    "        \"\"\"Reset LSTM states.\"\"\"\n",
    "        self.hidden_state = None\n",
    "        self.cell_state = None\n",
    "        \n",
    "    def update_state(self, new_hidden, new_cell):\n",
    "        \"\"\"Update cached states.\"\"\"\n",
    "        self.hidden_state = new_hidden.detach()\n",
    "        self.cell_state = new_cell.detach()\n",
    "        \n",
    "    def get_initial_state(self, batch_size, device):\n",
    "        \"\"\"Get initial or cached state.\"\"\"\n",
    "        if self.hidden_state is None:\n",
    "            h0 = torch.zeros(self.num_layers, batch_size, self.d_model, device=device)\n",
    "            c0 = torch.zeros(self.num_layers, batch_size, self.d_model, device=device)\n",
    "            return (h0, c0)\n",
    "        return (self.hidden_state, self.cell_state)\n",
    "\n",
    "\n",
    "# Demo incremental inference\n",
    "incremental = IncrementalInference(backbone, lm, window_size=64)\n",
    "\n",
    "print(\"Incremental inference demo...\")\n",
    "for batch in range(5):\n",
    "    # Simulate receiving a batch of 16 new samples\n",
    "    new_cont = [torch.randn(155) for _ in range(16)]\n",
    "    new_cat = [torch.randint(0, 10, (4,)) for _ in range(16)]\n",
    "    \n",
    "    preds = incremental.process_incremental(new_cont, new_cat)\n",
    "    \n",
    "    if preds:\n",
    "        print(f\"Batch {batch}: Predictions for {preds['command'].shape[1]} positions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Latency Optimization\n",
    "\n",
    "Techniques to minimize inference latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_batch_sizes(backbone, lm, window_size=64, batch_sizes=[1, 2, 4, 8, 16]):\n",
    "    \"\"\"Find optimal batch size for throughput.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        continuous = torch.randn(batch_size, window_size, 155)\n",
    "        categorical = torch.randint(0, 10, (batch_size, window_size, 4))\n",
    "        \n",
    "        # Warmup\n",
    "        with torch.no_grad():\n",
    "            for _ in range(5):\n",
    "                _ = lm(backbone(continuous, categorical))\n",
    "        \n",
    "        # Benchmark\n",
    "        times = []\n",
    "        for _ in range(50):\n",
    "            start = time.time()\n",
    "            with torch.no_grad():\n",
    "                _ = lm(backbone(continuous, categorical))\n",
    "            times.append((time.time() - start) * 1000)\n",
    "        \n",
    "        mean_time = np.mean(times)\n",
    "        throughput = batch_size * 1000 / mean_time  # samples/second\n",
    "        \n",
    "        results.append({\n",
    "            'batch_size': batch_size,\n",
    "            'mean_latency_ms': mean_time,\n",
    "            'latency_per_sample_ms': mean_time / batch_size,\n",
    "            'throughput': throughput\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run batch size benchmark\n",
    "batch_results = benchmark_batch_sizes(backbone, lm)\n",
    "\n",
    "print(f\"{'Batch':<8} {'Latency':<12} {'Per Sample':<12} {'Throughput':<12}\")\n",
    "print(\"-\" * 44)\n",
    "for r in batch_results:\n",
    "    print(f\"{r['batch_size']:<8} {r['mean_latency_ms']:<12.2f} {r['latency_per_sample_ms']:<12.2f} {r['throughput']:<12.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JIT compilation for faster inference\n",
    "class JITOptimizedModel(nn.Module):\n",
    "    \"\"\"JIT-compiled model for faster inference.\"\"\"\n",
    "    \n",
    "    def __init__(self, backbone, lm):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.lm = lm\n",
    "        \n",
    "    def forward(self, continuous, categorical):\n",
    "        hidden = self.backbone(continuous, categorical)\n",
    "        return self.lm(hidden)\n",
    "\n",
    "# Create and trace model\n",
    "combined_model = JITOptimizedModel(backbone, lm)\n",
    "\n",
    "# Trace with sample input\n",
    "sample_cont = torch.randn(1, 64, 155)\n",
    "sample_cat = torch.randint(0, 10, (1, 64, 4))\n",
    "\n",
    "try:\n",
    "    traced_model = torch.jit.trace(combined_model, (sample_cont, sample_cat))\n",
    "    print(\"Model traced successfully\")\n",
    "    \n",
    "    # Benchmark traced vs non-traced\n",
    "    def benchmark_inference(model, continuous, categorical, n_runs=100):\n",
    "        times = []\n",
    "        with torch.no_grad():\n",
    "            for _ in range(n_runs):\n",
    "                start = time.time()\n",
    "                _ = model(continuous, categorical)\n",
    "                times.append((time.time() - start) * 1000)\n",
    "        return np.mean(times), np.std(times)\n",
    "    \n",
    "    original_mean, original_std = benchmark_inference(combined_model, sample_cont, sample_cat)\n",
    "    traced_mean, traced_std = benchmark_inference(traced_model, sample_cont, sample_cat)\n",
    "    \n",
    "    print(f\"\\nOriginal: {original_mean:.2f} ± {original_std:.2f} ms\")\n",
    "    print(f\"Traced:   {traced_mean:.2f} ± {traced_std:.2f} ms\")\n",
    "    print(f\"Speedup:  {original_mean / traced_mean:.2f}x\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Tracing failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize latency optimization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Batch size vs throughput\n",
    "batch_sizes = [r['batch_size'] for r in batch_results]\n",
    "throughputs = [r['throughput'] for r in batch_results]\n",
    "latencies = [r['mean_latency_ms'] for r in batch_results]\n",
    "\n",
    "ax1 = axes[0]\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "line1 = ax1.plot(batch_sizes, throughputs, 'b-o', label='Throughput', linewidth=2)\n",
    "line2 = ax2.plot(batch_sizes, latencies, 'r--s', label='Latency', linewidth=2)\n",
    "\n",
    "ax1.set_xlabel('Batch Size')\n",
    "ax1.set_ylabel('Throughput (samples/sec)', color='blue')\n",
    "ax2.set_ylabel('Latency (ms)', color='red')\n",
    "ax1.set_title('Batch Size Trade-off')\n",
    "\n",
    "lines = line1 + line2\n",
    "labels = [l.get_label() for l in lines]\n",
    "ax1.legend(lines, labels)\n",
    "\n",
    "# Latency breakdown (simulated)\n",
    "components = ['Input\\nProcessing', 'Backbone', 'LM Head', 'Output\\nProcessing']\n",
    "times = [1.2, 8.5, 2.3, 0.5]\n",
    "\n",
    "axes[1].barh(components, times, color=['steelblue', 'coral', 'forestgreen', 'purple'])\n",
    "axes[1].set_xlabel('Time (ms)')\n",
    "axes[1].set_title('Latency Breakdown')\n",
    "\n",
    "for i, (comp, t) in enumerate(zip(components, times)):\n",
    "    axes[1].text(t + 0.1, i, f'{t:.1f}ms', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(project_root / 'reports' / 'latency_optimization.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Buffer Management\n",
    "\n",
    "Efficient memory management for streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CircularBuffer:\n",
    "    \"\"\"Efficient circular buffer for streaming data.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_size, feature_dim):\n",
    "        self.max_size = max_size\n",
    "        self.feature_dim = feature_dim\n",
    "        self.buffer = torch.zeros(max_size, feature_dim)\n",
    "        self.write_idx = 0\n",
    "        self.count = 0\n",
    "        \n",
    "    def add(self, sample):\n",
    "        \"\"\"Add sample to buffer.\"\"\"\n",
    "        self.buffer[self.write_idx] = sample\n",
    "        self.write_idx = (self.write_idx + 1) % self.max_size\n",
    "        self.count = min(self.count + 1, self.max_size)\n",
    "        \n",
    "    def add_batch(self, samples):\n",
    "        \"\"\"Add multiple samples efficiently.\"\"\"\n",
    "        n = len(samples)\n",
    "        if n >= self.max_size:\n",
    "            # Just keep the last max_size samples\n",
    "            self.buffer[:] = samples[-self.max_size:]\n",
    "            self.write_idx = 0\n",
    "            self.count = self.max_size\n",
    "        else:\n",
    "            # Circular write\n",
    "            end_idx = self.write_idx + n\n",
    "            if end_idx <= self.max_size:\n",
    "                self.buffer[self.write_idx:end_idx] = samples\n",
    "            else:\n",
    "                first_part = self.max_size - self.write_idx\n",
    "                self.buffer[self.write_idx:] = samples[:first_part]\n",
    "                self.buffer[:n - first_part] = samples[first_part:]\n",
    "            \n",
    "            self.write_idx = end_idx % self.max_size\n",
    "            self.count = min(self.count + n, self.max_size)\n",
    "    \n",
    "    def get_window(self, size):\n",
    "        \"\"\"Get last 'size' samples in order.\"\"\"\n",
    "        if self.count < size:\n",
    "            return None\n",
    "        \n",
    "        start = (self.write_idx - size) % self.max_size\n",
    "        if start < self.write_idx:\n",
    "            return self.buffer[start:self.write_idx].clone()\n",
    "        else:\n",
    "            return torch.cat([self.buffer[start:], self.buffer[:self.write_idx]]).clone()\n",
    "    \n",
    "    def is_full(self):\n",
    "        return self.count >= self.max_size\n",
    "\n",
    "\n",
    "class StreamingBuffer:\n",
    "    \"\"\"Combined buffer for continuous and categorical data.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_size, continuous_dim, categorical_dim):\n",
    "        self.continuous = CircularBuffer(max_size, continuous_dim)\n",
    "        self.categorical = CircularBuffer(max_size, categorical_dim)\n",
    "        self.max_size = max_size\n",
    "        \n",
    "    def add(self, continuous, categorical):\n",
    "        self.continuous.add(continuous)\n",
    "        self.categorical.add(categorical)\n",
    "        \n",
    "    def get_window(self, size):\n",
    "        cont = self.continuous.get_window(size)\n",
    "        cat = self.categorical.get_window(size)\n",
    "        if cont is None or cat is None:\n",
    "            return None, None\n",
    "        return cont, cat.long()\n",
    "\n",
    "\n",
    "# Test circular buffer\n",
    "buffer = StreamingBuffer(max_size=128, continuous_dim=155, categorical_dim=4)\n",
    "\n",
    "# Simulate adding data\n",
    "for t in range(150):\n",
    "    buffer.add(torch.randn(155), torch.randint(0, 10, (4,)).float())\n",
    "\n",
    "cont, cat = buffer.get_window(64)\n",
    "print(f\"Window shape: continuous={cont.shape}, categorical={cat.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Real-Time Simulation\n",
    "\n",
    "Complete streaming inference simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamingInferenceEngine:\n",
    "    \"\"\"Complete streaming inference engine.\"\"\"\n",
    "    \n",
    "    def __init__(self, backbone, lm, window_size=64, stride=16, \n",
    "                 buffer_size=256, continuous_dim=155):\n",
    "        self.backbone = backbone\n",
    "        self.lm = lm\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.buffer = StreamingBuffer(buffer_size, continuous_dim, 4)\n",
    "        self.samples_since_prediction = 0\n",
    "        \n",
    "        # Metrics\n",
    "        self.latencies = []\n",
    "        self.predictions = []\n",
    "        \n",
    "    def process_sample(self, continuous, categorical):\n",
    "        \"\"\"Process single incoming sample.\"\"\"\n",
    "        self.buffer.add(continuous, categorical.float())\n",
    "        self.samples_since_prediction += 1\n",
    "        \n",
    "        # Check if we should run inference\n",
    "        if self.samples_since_prediction >= self.stride:\n",
    "            cont, cat = self.buffer.get_window(self.window_size)\n",
    "            \n",
    "            if cont is not None:\n",
    "                start = time.time()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    cont = cont.unsqueeze(0)\n",
    "                    cat = cat.unsqueeze(0)\n",
    "                    hidden = self.backbone(cont, cat)\n",
    "                    preds = self.lm(hidden)\n",
    "                \n",
    "                latency = (time.time() - start) * 1000\n",
    "                self.latencies.append(latency)\n",
    "                \n",
    "                # Get prediction for latest position\n",
    "                latest_pred = preds['command'][0, -1].argmax().item()\n",
    "                self.predictions.append(latest_pred)\n",
    "                \n",
    "                self.samples_since_prediction = 0\n",
    "                return latest_pred\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get performance statistics.\"\"\"\n",
    "        if not self.latencies:\n",
    "            return {}\n",
    "        \n",
    "        return {\n",
    "            'mean_latency_ms': np.mean(self.latencies),\n",
    "            'std_latency_ms': np.std(self.latencies),\n",
    "            'p95_latency_ms': np.percentile(self.latencies, 95),\n",
    "            'max_latency_ms': np.max(self.latencies),\n",
    "            'total_predictions': len(self.predictions),\n",
    "        }\n",
    "\n",
    "\n",
    "# Run simulation\n",
    "engine = StreamingInferenceEngine(backbone, lm, window_size=64, stride=8)\n",
    "\n",
    "print(\"Running real-time simulation (500 samples at 100Hz)...\")\n",
    "simulation_results = []\n",
    "\n",
    "for t in range(500):\n",
    "    # Simulate sensor reading\n",
    "    continuous = torch.randn(155)\n",
    "    categorical = torch.randint(0, 10, (4,))\n",
    "    \n",
    "    # Process\n",
    "    pred = engine.process_sample(continuous, categorical)\n",
    "    \n",
    "    if pred is not None:\n",
    "        simulation_results.append({\n",
    "            'time': t,\n",
    "            'prediction': pred,\n",
    "            'latency': engine.latencies[-1]\n",
    "        })\n",
    "\n",
    "# Print stats\n",
    "stats = engine.get_stats()\n",
    "print(\"\\nPerformance Statistics:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value:.2f}\" if isinstance(value, float) else f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize real-time simulation\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Latency over time\n",
    "times = [r['time'] for r in simulation_results]\n",
    "latencies = [r['latency'] for r in simulation_results]\n",
    "\n",
    "axes[0, 0].plot(times, latencies, 'b-', alpha=0.7)\n",
    "axes[0, 0].axhline(y=np.mean(latencies), color='red', linestyle='--', label=f'Mean: {np.mean(latencies):.2f}ms')\n",
    "axes[0, 0].axhline(y=10, color='orange', linestyle=':', label='10ms target')\n",
    "axes[0, 0].set_xlabel('Sample')\n",
    "axes[0, 0].set_ylabel('Latency (ms)')\n",
    "axes[0, 0].set_title('Inference Latency Over Time')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Predictions over time\n",
    "predictions = [r['prediction'] for r in simulation_results]\n",
    "axes[0, 1].plot(times, predictions, 'g-', alpha=0.7)\n",
    "axes[0, 1].set_xlabel('Sample')\n",
    "axes[0, 1].set_ylabel('Predicted Class')\n",
    "axes[0, 1].set_title('Predictions Over Time')\n",
    "\n",
    "# Latency histogram\n",
    "axes[1, 0].hist(latencies, bins=30, alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].axvline(x=np.percentile(latencies, 95), color='red', linestyle='--', \n",
    "                   label=f'P95: {np.percentile(latencies, 95):.2f}ms')\n",
    "axes[1, 0].set_xlabel('Latency (ms)')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "axes[1, 0].set_title('Latency Distribution')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# System metrics\n",
    "metrics = list(stats.keys())\n",
    "values = [stats[m] for m in metrics]\n",
    "\n",
    "# Normalize for visualization\n",
    "norm_values = np.array(values) / max(values)\n",
    "\n",
    "axes[1, 1].barh(metrics, norm_values, color='steelblue')\n",
    "for i, (m, v) in enumerate(zip(metrics, values)):\n",
    "    axes[1, 1].text(norm_values[i] + 0.02, i, f'{v:.2f}' if isinstance(v, float) else str(v), va='center')\n",
    "axes[1, 1].set_xlim(0, 1.3)\n",
    "axes[1, 1].set_title('Performance Metrics (normalized)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(project_root / 'reports' / 'streaming_simulation.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save streaming configuration\n",
    "streaming_config = {\n",
    "    'window_size': 64,\n",
    "    'stride': 8,\n",
    "    'buffer_size': 256,\n",
    "    'continuous_dim': 155,\n",
    "    'categorical_dim': 4,\n",
    "    'performance': stats\n",
    "}\n",
    "\n",
    "config_path = project_root / 'configs' / 'streaming_config.json'\n",
    "config_path.parent.mkdir(exist_ok=True)\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(streaming_config, f, indent=2)\n",
    "\n",
    "print(f\"Streaming configuration saved to: {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook covers streaming inference:\n",
    "\n",
    "1. **Sliding Window**: Process continuous stream with overlapping windows\n",
    "2. **Incremental Processing**: Reuse computation between windows\n",
    "3. **Latency Optimization**: JIT tracing, batch size tuning\n",
    "4. **Buffer Management**: Efficient circular buffers\n",
    "5. **Real-Time Simulation**: Complete streaming engine with metrics\n",
    "\n",
    "Key performance targets:\n",
    "- Latency: < 10ms per prediction\n",
    "- Throughput: > 100 samples/second\n",
    "- Memory: O(window_size) buffer\n",
    "\n",
    "---\n",
    "\n",
    "**Navigation:**\n",
    "← [Previous: 18_transfer_learning](18_transfer_learning.ipynb) |\n",
    "[Back to README](README.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
