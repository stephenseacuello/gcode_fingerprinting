{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00 - Raw Data Analysis\n",
    "\n",
    "This notebook explores the raw G-code and sensor data before any preprocessing.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand raw G-code file structure\n",
    "- Explore sensor data format and features\n",
    "- Perform statistical analysis on raw datasets\n",
    "- Visualize data distributions\n",
    "- Identify data quality issues\n",
    "\n",
    "## Prerequisites\n",
    "- Python 3.8+\n",
    "- Project virtual environment activated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root / 'src'))\n",
    "\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploring Raw G-code Files\n",
    "\n",
    "G-code is a numerical control programming language used in CNC machining. Each line contains commands and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available G-code files\n",
    "data_dir = project_root / 'data'\n",
    "gcode_files = list(data_dir.glob('*.gcode')) + list(data_dir.glob('*.nc'))\n",
    "\n",
    "print(f\"Found {len(gcode_files)} G-code files\")\n",
    "for f in gcode_files[:10]:  # Show first 10\n",
    "    print(f\"  - {f.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display a sample G-code file\n",
    "if gcode_files:\n",
    "    sample_file = gcode_files[0]\n",
    "    print(f\"\\nSample G-code file: {sample_file.name}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    with open(sample_file, 'r') as f:\n",
    "        lines = f.readlines()[:20]  # First 20 lines\n",
    "        for i, line in enumerate(lines, 1):\n",
    "            print(f\"{i:3d}: {line.rstrip()}\")\n",
    "else:\n",
    "    print(\"No G-code files found. Please add sample files to the data/ directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G-code Command Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_gcode_line(line):\n",
    "    \"\"\"Extract G-code commands from a line.\"\"\"\n",
    "    # Remove comments\n",
    "    line = re.sub(r';.*', '', line)\n",
    "    line = re.sub(r'\\(.*?\\)', '', line)\n",
    "    \n",
    "    # Extract tokens\n",
    "    tokens = line.strip().split()\n",
    "    commands = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        # Match command patterns (G0, M3, X10.5, etc.)\n",
    "        match = re.match(r'([A-Z])([\\d.\\-]+)', token)\n",
    "        if match:\n",
    "            commands.append(match.group(1))  # Just the letter\n",
    "    \n",
    "    return commands\n",
    "\n",
    "# Analyze all G-code files\n",
    "all_commands = []\n",
    "\n",
    "for gcode_file in gcode_files[:10]:  # Analyze first 10 files\n",
    "    with open(gcode_file, 'r') as f:\n",
    "        for line in f:\n",
    "            all_commands.extend(parse_gcode_line(line))\n",
    "\n",
    "# Count command frequencies\n",
    "command_counts = Counter(all_commands)\n",
    "\n",
    "print(f\"\\nTotal commands analyzed: {len(all_commands)}\")\n",
    "print(f\"Unique command types: {len(command_counts)}\")\n",
    "print(\"\\nTop 10 most common commands:\")\n",
    "for cmd, count in command_counts.most_common(10):\n",
    "    print(f\"  {cmd}: {count:,} ({count/len(all_commands)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize command distribution\n",
    "top_commands = dict(command_counts.most_common(15))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(top_commands.keys(), top_commands.values(), color='steelblue', alpha=0.7)\n",
    "plt.xlabel('Command Type', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Top 15 G-code Commands Distribution', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploring Sensor Data\n",
    "\n",
    "Sensor data captures machine states during G-code execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for sensor data files\n",
    "sensor_files = list(data_dir.glob('*sensor*.json')) + list(data_dir.glob('*sensor*.csv'))\n",
    "\n",
    "if sensor_files:\n",
    "    print(f\"Found {len(sensor_files)} sensor data files:\")\n",
    "    for f in sensor_files[:5]:\n",
    "        print(f\"  - {f.name}\")\nelse:\n",
    "    print(\"No sensor files found. Sensor data might be embedded with G-code.\")\n",
    "    print(\"Check the data/ directory structure.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load sensor data (adjust based on your data format)\n",
    "# This is a template - modify based on your actual data structure\n",
    "\n",
    "# Option 1: If sensor data is in JSON format\n",
    "# with open(sensor_files[0], 'r') as f:\n",
    "#     sensor_data = json.load(f)\n",
    "\n",
    "# Option 2: If sensor data is in CSV format\n",
    "# sensor_df = pd.read_csv(sensor_files[0])\n",
    "# print(sensor_df.head())\n",
    "\n",
    "print(\"Adjust this cell based on your actual sensor data format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vocabulary Analysis\n",
    "\n",
    "The vocabulary file maps G-code tokens to integer IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vocabulary\n",
    "vocab_path = project_root / 'data' / 'vocabulary.json'\n",
    "\n",
    "if vocab_path.exists():\n",
    "    with open(vocab_path, 'r') as f:\n",
    "        vocab = json.load(f)\n",
    "    \n",
    "    print(f\"Vocabulary size: {len(vocab)}\")\n",
    "    print(\"\\nFirst 20 tokens:\")\n",
    "    for token, idx in list(vocab.items())[:20]:\n",
    "        print(f\"  {token:15s} -> {idx}\")\n",
    "else:\n",
    "    print(\"Vocabulary file not found at\", vocab_path)\n",
    "    print(\"Run preprocessing to generate vocabulary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze vocabulary composition\n",
    "if vocab_path.exists():\n",
    "    # Categorize tokens\n",
    "    g_commands = [t for t in vocab.keys() if t.startswith('G')]\n",
    "    m_commands = [t for t in vocab.keys() if t.startswith('M')]\n",
    "    params = [t for t in vocab.keys() if t[0] in 'XYZFIJKR' and len(t) > 1]\n",
    "    special = ['<PAD>', '<UNK>', '<SOS>', '<EOS>']\n",
    "    \n",
    "    print(\"Token categories:\")\n",
    "    print(f\"  G-commands: {len(g_commands)}\")\n",
    "    print(f\"  M-commands: {len(m_commands)}\")\n",
    "    print(f\"  Parameters: {len(params)}\")\n",
    "    print(f\"  Special tokens: {sum(1 for t in vocab if t in special)}\")\n",
    "    print(f\"  Other: {len(vocab) - len(g_commands) - len(m_commands) - len(params) - 4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze G-code file sizes and line counts\n",
    "if gcode_files:\n",
    "    file_stats = []\n",
    "    \n",
    "    for gcode_file in gcode_files:\n",
    "        with open(gcode_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            non_empty = [l for l in lines if l.strip() and not l.strip().startswith((';', '('))]\n",
    "            \n",
    "            file_stats.append({\n",
    "                'file': gcode_file.name,\n",
    "                'total_lines': len(lines),\n",
    "                'code_lines': len(non_empty),\n",
    "                'size_kb': gcode_file.stat().st_size / 1024\n",
    "            })\n",
    "    \n",
    "    stats_df = pd.DataFrame(file_stats)\n",
    "    \n",
    "    print(\"G-code File Statistics:\")\n",
    "    print(f\"\\nTotal files: {len(stats_df)}\")\n",
    "    print(f\"\\nLines per file:\")\n",
    "    print(f\"  Mean: {stats_df['code_lines'].mean():.0f}\")\n",
    "    print(f\"  Median: {stats_df['code_lines'].median():.0f}\")\n",
    "    print(f\"  Min: {stats_df['code_lines'].min()}\")\n",
    "    print(f\"  Max: {stats_df['code_lines'].max()}\")\n",
    "    print(f\"\\nFile size (KB):\")\n",
    "    print(f\"  Mean: {stats_df['size_kb'].mean():.1f}\")\n",
    "    print(f\"  Total: {stats_df['size_kb'].sum():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize file statistics\n",
    "if gcode_files:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Lines distribution\n",
    "    axes[0].hist(stats_df['code_lines'], bins=20, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "    axes[0].set_xlabel('Number of Code Lines', fontsize=12)\n",
    "    axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "    axes[0].set_title('Distribution of G-code File Sizes', fontsize=13, fontweight='bold')\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Size distribution\n",
    "    axes[1].hist(stats_df['size_kb'], bins=20, color='coral', alpha=0.7, edgecolor='black')\n",
    "    axes[1].set_xlabel('File Size (KB)', fontsize=12)\n",
    "    axes[1].set_ylabel('Frequency', fontsize=12)\n",
    "    axes[1].set_title('Distribution of File Sizes', fontsize=13, fontweight='bold')\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hands-On Exercise\n",
    "\n",
    "**Task**: Analyze a G-code file and extract key statistics.\n",
    "\n",
    "For a G-code file of your choice:\n",
    "1. Count the number of different G-commands (G0, G1, G2, etc.)\n",
    "2. Count the number of different M-commands\n",
    "3. Find the most common parameter (X, Y, Z, F, etc.)\n",
    "4. Calculate the average line length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "\n",
    "# Example starter code:\n",
    "if gcode_files:\n",
    "    target_file = gcode_files[0]  # Pick any file\n",
    "    \n",
    "    # TODO: Implement the analysis\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "- How to load and parse raw G-code files\n",
    "- How to analyze command distributions\n",
    "- How to explore vocabulary structure\n",
    "- How to assess data quality\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue to **01_getting_started.ipynb** for an overview of the entire project.\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "- **No G-code files found**: Ensure your data is in the `data/` directory\n",
    "- **Vocabulary not found**: Run preprocessing first\n",
    "- **Import errors**: Make sure your virtual environment is activated"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
