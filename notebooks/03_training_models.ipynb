{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Training Models\n",
    "\n",
    "Learn how to train the G-code fingerprinting model.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand model architecture\n",
    "- Configure training hyperparameters\n",
    "- Train a model from scratch\n",
    "- Monitor training with W&B\n",
    "- Manage checkpoints\n",
    "\n",
    "## Model Components\n",
    "1. **MM-DTAE-LSTM Backbone**: Processes sensor data\n",
    "2. **Multi-Head Language Model**: Predicts G-code tokens\n",
    "   - Command head\n",
    "   - Parameter type head\n",
    "   - Parameter value head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root / 'src'))\n",
    "\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from miracle.model.model import MM_DTAE_LSTM, ModelConfig\n",
    "from miracle.model.multihead_lm import MultiHeadGCodeLM\n",
    "from miracle.dataset.target_utils import TokenDecomposer\n",
    "from miracle.utilities.device import get_device\n",
    "\n",
    "print(\"âœ“ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Architecture Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample model to inspect architecture\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load vocabulary\n",
    "vocab_path = project_root / 'data' / 'vocabulary.json'\n",
    "decomposer = TokenDecomposer(str(vocab_path))\n",
    "\n",
    "print(f\"\\nVocabulary stats:\")\n",
    "print(f\"  Total tokens: {decomposer.vocab_size}\")\n",
    "print(f\"  Commands: {decomposer.n_commands}\")\n",
    "print(f\"  Param types: {decomposer.n_param_types}\")\n",
    "print(f\"  Param values: {decomposer.n_param_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create backbone model\n",
    "backbone_config = ModelConfig(\n",
    "    sensor_dims=[155, 4],  # 155 continuous, 4 categorical\n",
    "    d_model=128,\n",
    "    lstm_layers=2,\n",
    "    gcode_vocab=decomposer.vocab_size,\n",
    "    n_heads=4\n",
    ")\n",
    "\n",
    "backbone = MM_DTAE_LSTM(backbone_config).to(device)\n",
    "\n",
    "print(\"Backbone Model:\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in backbone.parameters()):,}\")\n",
    "print(f\"  Hidden dim: {backbone_config.d_model}\")\n",
    "print(f\"  LSTM layers: {backbone_config.lstm_layers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multi-head language model\n",
    "multihead_lm = MultiHeadGCodeLM(\n",
    "    d_model=128,\n",
    "    n_commands=decomposer.n_commands,\n",
    "    n_param_types=decomposer.n_param_types,\n",
    "    n_param_values=decomposer.n_param_values,\n",
    "    nhead=4,\n",
    "    num_layers=2,\n",
    "    dropout=0.1,\n",
    "    vocab_size=decomposer.vocab_size\n",
    ").to(device)\n",
    "\n",
    "print(\"Multi-Head LM:\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in multihead_lm.parameters()):,}\")\n",
    "print(f\"  Transformer layers: 2\")\n",
    "print(f\"  Attention heads: 4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "config = {\n",
    "    'learning_rate': 0.001,\n",
    "    'batch_size': 16,\n",
    "    'max_epochs': 10,  # Small for demo\n",
    "    'hidden_dim': 128,\n",
    "    'num_layers': 2,\n",
    "    'num_heads': 4,\n",
    "    'dropout': 0.1,\n",
    "    'weight_decay': 1e-5,\n",
    "    'grad_clip': 1.0,\n",
    "    'command_weight': 3.0  # Higher weight for command prediction\n",
    "}\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "for k, v in config.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Command-Line Training\n",
    "\n",
    "The recommended way to train is using the training script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training command (for reference)\n",
    "print(\"To train from command line:\")\n",
    "print()\n",
    "print(\"PYTORCH_ENABLE_MPS_FALLBACK=1 PYTHONPATH=src .venv/bin/python scripts/train_multihead.py \\\\\")\n",
    "print(\"    --data-dir outputs/processed_current \\\\\")\n",
    "print(\"    --vocab-path data/vocabulary.json \\\\\")\n",
    "print(\"    --output-dir outputs/training_demo \\\\\")\n",
    "print(\"    --max-epochs 10 \\\\\")\n",
    "print(\"    --batch_size 16 \\\\\")\n",
    "print(\"    --learning_rate 0.001 \\\\\")\n",
    "print(\"    --hidden_dim 128 \\\\\")\n",
    "print(\"    --use-wandb\")\n",
    "print()\n",
    "print(\"This will train for 10 epochs with W&B logging.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Monitoring Training\n",
    "\n",
    "When using `--use-wandb`, metrics are logged to Weights & Biases:\n",
    "\n",
    "- **Training loss**: Overall loss, command loss, param loss\n",
    "- **Validation accuracy**: Command, param type, param value, overall\n",
    "- **Learning rate**: Current LR (if using scheduler)\n",
    "- **Gradient norm**: For monitoring stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Checkpoint Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find available checkpoints\n",
    "import glob\n",
    "\n",
    "checkpoints = glob.glob(str(project_root / 'outputs' / '*' / 'checkpoint_*.pt'))\n",
    "\n",
    "print(f\"Found {len(checkpoints)} checkpoint(s):\")\n",
    "for cp in checkpoints[:10]:\n",
    "    cp_path = Path(cp)\n",
    "    print(f\"  - {cp_path.parent.name}/{cp_path.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect checkpoint contents\n",
    "if checkpoints:\n",
    "    checkpoint = torch.load(checkpoints[0], map_location='cpu')\n",
    "    \n",
    "    print(\"Checkpoint contents:\")\n",
    "    for key in checkpoint.keys():\n",
    "        if 'state_dict' in key:\n",
    "            print(f\"  {key}: {len(checkpoint[key])} parameters\")\n",
    "        else:\n",
    "            print(f\"  {key}: {checkpoint[key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Tips\n",
    "\n",
    "### Hyperparameter Guidelines:\n",
    "- **Learning rate**: Start with 1e-3, reduce if loss plateaus\n",
    "- **Batch size**: 16-32 works well, adjust based on memory\n",
    "- **Hidden dim**: 128-256 for most datasets\n",
    "- **Layers**: 2-3 LSTM/Transformer layers\n",
    "- **Command weight**: 3.0-5.0 to emphasize command prediction\n",
    "\n",
    "### Common Issues:\n",
    "- **Loss not decreasing**: Try lower learning rate\n",
    "- **Overfitting**: Increase dropout, add weight decay\n",
    "- **OOM errors**: Reduce batch size or hidden dimension\n",
    "- **Nan loss**: Reduce learning rate, check gradient clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You learned:\n",
    "- Model architecture (backbone + multi-head LM)\n",
    "- Training configuration\n",
    "- How to run training\n",
    "- Checkpoint management\n",
    "- Hyperparameter tuning tips\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue to **04_inference_prediction.ipynb** to use trained models for prediction.\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "- **Import errors**: Set `PYTHONPATH=src`\n",
    "- **Device errors**: Use CPU with `device='cpu'` or enable MPS fallback\n",
    "- **W&B login**: Run `wandb login` in terminal"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
